{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Double ; 64-bit float number\n",
    "#Int    : 32-bit integer\n",
    "#Boolean: True or False\n",
    "def sum(x:Double, y:Int) = x + y\n",
    "sum(2.0, 1)  #Double = 3.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def test(x: Int, y:Double): Double = {var z=x*y; return z;}\n",
    "test(2,3) #Double = 6.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def test1(x: Int, y:Double): Double = {\n",
    "        if (x >5){\n",
    "            println(\"more than 5\");\n",
    "            return 0\n",
    "        }\n",
    "        var z=x*y; \n",
    "        return z;\n",
    "}\n",
    "test1(10,3)\n",
    "#more than 5\n",
    "#Double = 0.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def add(x:Int, y: Int): Int={\n",
    "        return x + y       \n",
    "    }\n",
    "add(2,3) #Int = 5\n",
    "###########################################################\n",
    "#define a class\n",
    "class Person(val fname: String, val lname: String, val anage: Int){\n",
    "    val firstname:String =fname;\n",
    "    val lastname:String = lname;\n",
    "    val age =anage;\n",
    "    def getfullname():String = { ## the return is a string type\n",
    "        return firstname + \" \" + lastname;\n",
    "    }\n",
    "}\n",
    "\n",
    "val obj = new Person(\"Mary\", \"Gary\", 42)  ## create an instance of the class\n",
    "obj.firstname     #String = Mary\n",
    "obj.getfullname() #String = Mary Gary\n",
    "###########################################################\n",
    "# singleton object\n",
    "object Hello{\n",
    "    def message(): String ={\n",
    "        return \"Hello\";\n",
    "    }\n",
    "}\n",
    "\n",
    "Hello.message()\n",
    "#String = Hello\n",
    "###########################################################\n",
    "var a = List(1,2)\n",
    "for (x <- a){\n",
    "    println(x);\n",
    "    \n",
    "}\n",
    "#1\n",
    "#2\n",
    "\n",
    "var x=8\n",
    "if (x >5){\n",
    "  println(\"more than 5\");\n",
    "}\n",
    "#more than 5\n",
    "\n",
    "if((Array(\"a\", \"b\", \"a\") contains \"a\")) println(\"Have this word\") #Have this word\n",
    "if(! (Array(\"a\", \"b\", \"a\") contains \"a\")) println(\"Have this word\")\n",
    "\n",
    "var x=3\n",
    "while (x >0){\n",
    "    println(x);\n",
    "    x=x-1\n",
    "}\n",
    "#3\n",
    "#2\n",
    "#1\n",
    "\n",
    "var colors = Map(\"A\" -> \"a\",\"B\" -> \"a\")\n",
    "for((key, value) <- colors){\n",
    "    printf(\"key: %s, value: %s\\n\", key, value);\n",
    "}\n",
    "#key: A, value: a\n",
    "#key: B, value: a\n",
    "\n",
    "for (i <- 1 to 5) yield i\n",
    "#scala.collection.immutable.IndexedSeq[Int] = Vector(1, 2, 3, 4, 5)\n",
    "for (i <- 1 to 5) yield i * 2\n",
    "#scala.collection.immutable.IndexedSeq[Int] = Vector(2, 4, 6, 8, 10)\n",
    "val a = Array(1, 2, 3, 4, 5)\n",
    "for (e <- a) yield e*2             #Array[Int] = Array(2, 4, 6, 8, 10)\n",
    "for (e <- a if e > 2) yield e      #Array[Int] = Array(3, 4, 5)\n",
    "########################################\n",
    "var x = 10 # create a variable\n",
    "var x: Int = 10 \n",
    "x=9 # x become 9\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#string   \n",
    "var x = \"hi\"\n",
    "var x : String = \"hi\" # we should always define the types of variables to make compile faster\n",
    "var a = \"   Hello world!\";\n",
    "a.trim()                               #String = Hello world!\n",
    "a.toLowerCase()                        #String = \"   hello world!\"\n",
    "a.toUpperCase()                        #String = \"   HELLO WORLD!\"\n",
    "a(4).toUpper                           #Char = E\n",
    "a.length()                             #Int = 15\n",
    "a.toLowerCase.replaceAll(\"[^a-z]\",\"x\") #String = xxxhelloxworldx\n",
    "a.toLowerCase.replaceAll(\"[^a-z]\",\"\")  #String = helloworld\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# array \n",
    "var language = Array('a', 'b')\n",
    "language(0) #Char = a\n",
    "language(0)='c' #Array[Char] = Array(c, b)\n",
    "\n",
    "var fruits = new Array[String](3)\n",
    "fruits(0) = \"Apple\"\n",
    "fruits(1) = \"Banana\"\n",
    "fruits(2) = \"Orange\"\n",
    "# Array[String] = Array(Apple, Banana, Orange)\n",
    "\n",
    "var output = Array[String]();\n",
    "output = output :+ \"a\"\n",
    "output = output :+ \"b\"\n",
    "#Array[String] = Array(a, b)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#list\n",
    "var num = List(3,6,9)\n",
    "num :+12 # List[Int] = List(3, 6, 9, 12)\n",
    "num(0) = 11 #Error !   can not be changed\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Set\n",
    "var num = Set(3,6,9)\n",
    "num = num + 12 #scala.collection.immutable.Set[Int] = Set(3, 6, 9, 12) # set does not gaurante ordering\n",
    "num(9) #if 9 is in the set\n",
    "#Boolean = true \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#tuple\n",
    "var t=(1,1.2,'a')\n",
    "var t=Tuple3(1,1.2,'a') \n",
    "#t: (Int, Double, Char) = (1,1.2,a)\n",
    "t._1 # access the first element\n",
    "#Int = 1\n",
    "var (my_int, my_double, my_string) = t\n",
    "#my_int: Int = 1\n",
    "#my_double: Double = 1.2\n",
    "#my_string: Char = a\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#dictionary\n",
    "var colors = Map(\"A\" -> \"a\",\"B\" -> \"a\")\n",
    "colors(\"A\") #String = a\n",
    "colors += \"C\" -> \"c\"\n",
    "#scala.collection.immutable.Map[String,String] = Map(A -> a, B -> a, C -> c)\n",
    "colors -= \"C\"\n",
    "#scala.collection.immutable.Map[String,String] = Map(A -> a, B -> a)\n",
    "\n",
    "var states = scala.collection.mutable.Map[String, String]()\n",
    "states(\"AK\") = \"Alaska\"\n",
    "tates += (\"AR\" -> \"Arkansas\", \"AZ\" -> \"Arizona\")\n",
    "#scala.collection.mutable.Map[String,String] = Map(AZ -> Arizona, AR -> Arkansas, AK -> Alaska)\n",
    "states -= (\"AL\", \"AZ\")\n",
    "#scala.collection.mutable.Map[String,String] = Map(AR -> Arkansas, AK -> Alaska\n",
    "states\n",
    "#scala.collection.mutable.Map[String,String] = Map(AR -> Arkansas, AK -> Alaska)\n",
    "states.clear\n",
    "########################################    \n",
    "val x = 8 # can not be changed\n",
    "val x: Int = 8    \n",
    "########################################   \n",
    "import math._ # math library\n",
    "sqrt(2) #Double = 1.4142135623730951\n",
    "\n",
    "1%2 #1\n",
    "2%2 #0\n",
    "3%2 #1\n",
    "########################################\n",
    "var a=List(2,4,6)\n",
    "a.map(x => x + 1)\n",
    "a.map(_+ 1)\n",
    "#List[Int] = List(3, 5, 7)\n",
    "\n",
    "var a=List(\"bb\",\"cc\")\n",
    "#a: List[String] = List(bb, cc)\n",
    "a.flatMap(x => x + \"#\") \n",
    "a.flatMap(_ + \"#\")\n",
    "#List(b, b, #, c, c, #)\n",
    "\n",
    "var a=List(\"bb\",\"cc\")\n",
    "#a: List[String] = List(bb, cc)\n",
    "a.filter(x => x.contains(\"b\"))\n",
    "#List[String] = List(bb)\n",
    "\n",
    "var a=List(\"bb\",\"cc\")\n",
    "a.foreach(println) # foreach don't return a new collection\n",
    "#bb\n",
    "#cc\n",
    "\n",
    "var a = List(3,7,13,16)\n",
    "a.reduce((x,y) => x + y)\n",
    "a.reduce(_ + _)\n",
    "#Int = 39\n",
    "# because 3+7 = 10\n",
    "# 10 + 13=23\n",
    "# 23 +16=39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### create RDD\n",
    "var rdd = sc.textFile(\"/FileStore/tables/c4pzqsme1501527259399/amazon_cells_labelled.txt\")\n",
    "\n",
    "var arr = Array('a', 'b', 'a', 'a','b')\n",
    "var rdd = sc.parallelize(arr)\n",
    "\n",
    "var rdd= sc.parallelize(Array(\"This is a dog.\", \"This is a cat\"))\n",
    "\n",
    "var rdd = sc.parallelize(Array(Array(1, 2), Array(3, 4)));\n",
    "\n",
    "val arr = 1 to 10 \n",
    "var rdd = sc.parallelize(arr) \n",
    "\n",
    "var rdd = sc.parallelize(1 to 100)\n",
    "rdd.getNumPartitions \n",
    "rdd.partitions.size \n",
    "rdd.partitions.length \n",
    "#8\n",
    "var rdd1 = rdd.coalesce(4)##The coalesce() method combines all the RDD partitions into 4 partitions\n",
    "rdd1.partitions.length #4\n",
    "\n",
    "#create an RDD having records from 1 to 6 with 3 partitions\n",
    "var a = sc.parallelize(1 to 6, 3);\n",
    "var a = sc.parallelize(Array(1,2,3,4,5,6),3);\n",
    "\n",
    "### create key-value pair RDD\n",
    "val a = sc.parallelize(List((\"maths\", 21),(\"english\", 22),(\"science\", 31)),3)\n",
    "a.partitions.size # 3 \n",
    "# show what is in partitions\n",
    "a.glom().collect()\n",
    "#Array(Array((maths,21)), \n",
    "#      Array((english,22)), \n",
    "#      Array((science,31)))\n",
    "\n",
    "val a = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "a.collect() # Array[(String, Int)] = Array((maths,50), (maths,60), (english,65))\n",
    "\n",
    "### collect action ################################################\n",
    "#Brings all the elements back to you. Data must fit into memory.\n",
    "#Converts the RDD into a Scala array \n",
    "\n",
    "### take action ################################################\n",
    "#Bring only few elements to the driver. This is more practical than collect.\n",
    "\n",
    "### count action ################################################\n",
    "# find out how many elements are there in an RDD\n",
    "a.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############# create your own partition ##############################\n",
    "import org.apache.spark.Partitioner\n",
    "class TwoPartsPartitioner(override val numPartitions: Int) extends Partitioner {\n",
    "    def getPartition(key: Any): Int = key match {\n",
    "        case s: String => {\n",
    "            if (s(0).toUpper > 'J') 1 else 0\n",
    "        }\n",
    "    }\n",
    "}   \n",
    "\n",
    "var x=sc.parallelize(Array((\"sandeep\",1),(\"giri\",1),(\"abhishek\",1),(\"sravani\",1),(\"jude\",1)), 3)\n",
    "x.glom().collect()\n",
    "#Array(Array((sandeep,1)), \n",
    "#      Array((giri,1), (abhishek,1)), \n",
    "#      Array((sravani,1), (jude,1)))\n",
    "var y = x.partitionBy(new TwoPartsPartitioner(2))\n",
    "y.glom().collect()\n",
    "#Array(Array((giri,1), (abhishek,1), (jude,1)), \n",
    "#Array((sandeep,1), (sravani,1)))\n",
    "############# create a accumulator ##############################\n",
    "val accum = sc.longAccumulator(\"My Accumulator\")\n",
    "sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))\n",
    "accum.value #Long = 10\n",
    "\n",
    "############ create a Broadcast variable ##############################\n",
    "# Sending a read-only value using Broadcast variable\n",
    "# Can be used to send large read-only values to all worker nodes efficiently\n",
    "val broadcastVar = sc.broadcast(10)\n",
    "val a = sc.parallelize(1 to 4)\n",
    "val b = a.map(x => broadcastVar.value + x)\n",
    "b.collect() #Array(11, 12, 13, 14)\n",
    "\n",
    "var commonWords = Array(\"a\", \"an\", \"the\", \"of\" )\n",
    "val commonWordsMap = collection.mutable.Map[String, Int]()\n",
    "for(word <- commonWords){\n",
    "    commonWordsMap(word) = 1\n",
    "}\n",
    "var commonWordsBC = sc.broadcast(commonWordsMap)\n",
    "commonWordsBC.value\n",
    "#scala.collection.mutable.Map[String,Int] = Map(of -> 1, a -> 1, an -> 1, the -> 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################# flatMap, map ############################\n",
    "var rdd1 = sc.textFile(\"/FileStore/tables/c4pzqsme1501527259399/amazon_cells_labelled.txt\")\n",
    "rdd1.take(2)\n",
    "#Array(So there is no way for me to plug it in here in the US unless I go by a converter. 0, \n",
    "#       Good case, Excellent value. 1)\n",
    "\n",
    "var words = rdd1.flatMap(x=>x.split(\" \"))\n",
    "words.take(20)\n",
    "#Array(So, there, is, no, way, for, me, to, plug, it, in, here, in, the, US, unless, I, go, by, a)\n",
    "words.map(x=>(x.toLowerCase(), 1)).take(10)\n",
    "#Array((so,1), (there,1), (is,1), (no,1), (way,1), (for,1), (me,1), (to,1), (plug,1), (it,1))\n",
    "\n",
    "var words = rdd1.flatMap(x=>x.split(\"\\t\"))\n",
    "words.take(4)\n",
    "#Array(So there is no way for me to plug it in here in the US unless I go by a converter., \n",
    "#       0, \n",
    "#       Good case,\n",
    "#       Excellent value., \n",
    "#       1)\n",
    "\n",
    "var words = rdd1.map(x=>x.split(\"\\t\"))\n",
    "words.take(2)\n",
    "#Array(Array(So there is no way for me to plug it in here in the US unless I go by a converter., 0), \n",
    "#      Array(Good case, Excellent value., 1))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "var arr = Array('a', 'b', 'a', 'a','b')\n",
    "var rdd = sc.parallelize(arr)\n",
    "#convert each word in a key value pair: key is the actual word, the value is one\n",
    "var rdd1 = rdd.map(x=>(x,1))\n",
    "rdd1.take(2)\n",
    "#Array((a,1), (b,1))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "val arr = 1 to 10 \n",
    "var nums = sc.parallelize(arr) \n",
    "nums.take(5) #Array(1, 2, 3, 4, 5)\n",
    "\n",
    "def test(x:Int):Int=x*2\n",
    "var result=nums.map(test)\n",
    "result.take(5)\n",
    " #Array(2, 4, 6, 8, 10)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "var rdd=sc.parallelize(Array(\"This is a dog.\", \"This is a cat\"))\n",
    "def test(line:String):Array[String]=line.split(\" \")\n",
    "var rdd1=rdd.flatMap(test)\n",
    "rdd1.collect()            # Array(This, is, a, dog., This, is, a, cat)\n",
    "rdd1.take(4)              # Array(This, is, a, dog.)\n",
    "var rdd2=rdd.map(test)\n",
    "rdd2.collect()            # Array(Array(This, is, a, dog.), Array(This, is, a, cat))\n",
    "rdd2.take(2)              # Array(Array(This, is, a, dog.), Array(This, is, a, cat))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "val rdd = sc.parallelize(List(\"10, NYC\",\"20, IL\"))\n",
    "def test(line:String)={\n",
    "    var arr = line.split(\",\");\n",
    "    (arr(1).trim,arr(0).toInt)\n",
    "}\n",
    "rdd.map(test).collect() #Array((NYC,10), (IL,20))\n",
    "# the 0 element of arr : convert string to integer\n",
    "# the 1 element of arr : remove white space around it\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "sc.parallelize(List(1, 2, 3), 2).flatMap(x => List(x, x, x)).collect()\n",
    "# Array(1, 1, 1, 2, 2, 2, 3, 3, 3)\n",
    "############################# mapValues ##########################################################\n",
    "val a = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "a.collect() # Array[(String, Int)] = Array((maths,50), (maths,60), (english,65))\n",
    "a.mapValues(x => (x, 1)).collect() #Array[(String, (Int, Int))] = Array((maths,(50,1)), (maths,(60,1)), (english,(65,1)))\n",
    "a.map(x => (x, 1)).collect()       #Array[((String, Int), Int)] = Array(((maths,50),1), ((maths,60),1), ((english,65),1))\n",
    "############################# filter, filterByRange ##########################################\n",
    "val arr = 1 to 10 \n",
    "var nums = sc.parallelize(arr) \n",
    "def test(x:Int):Boolean = x%2==0\n",
    "var result=nums.filter(test)\n",
    "result.take(5)\n",
    "#Array(2, 4, 6, 8, 10)\n",
    "nums.filter(_ % 2 == 0).collect()  # Array(2, 4, 6, 8, 10)\n",
    "nums.filter(_ < 4).collect()       # Array(1, 2, 3)\n",
    "nums.filter(x => x != 2).collect() # Array(1, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "\n",
    "var rdd= sc.parallelize(Array(\"This is a dog.\", \"This is a cat\")).toDF(\"line\") ## create a single column called as 'line'\n",
    "rdd.collect()\n",
    "rdd.filter(col(\"line\").like(\"%dog%\")).take(2) #find the 'line' column has 'dog' word\n",
    "#Array([This is a dog.])\n",
    "\n",
    "val a = sc.parallelize(List((2,\"cat\"), (1, \"mouse\"),(7, \"cup\")), 3)\n",
    "a.filterByRange(1, 3).collect()  #Array((2,cat), (1,mouse))\n",
    "############################# groupByKey, groupBy ###############################################\n",
    "var a=List(('a',1),('b',2),('a', 3),('b', 4))\n",
    "var b=sc.parallelize(a)\n",
    "b.groupByKey.mapValues(_.toList).collect() #Array((a,List(1, 3)), (b,List(2, 4)))\n",
    "b.groupByKey().collect()                   #Array((a,CompactBuffer(1, 3)), (b,CompactBuffer(2, 4)))\n",
    "b.reduceByKey(_+_).collect()               #Array((a,4), (b,6))\n",
    "\n",
    "val a = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "a.groupByKey().collect() #Array((english,CompactBuffer(65)), (maths,CompactBuffer(50, 60)))\n",
    "a.groupBy{ x =>\n",
    "  if((x._2 % 2) == 0) \n",
    "  {\"evennumbers\"} \n",
    "  else \n",
    "  {\"oddnumbers\"}}.collect()\n",
    "#Array((evennumbers,CompactBuffer((maths,50), (maths,60))), (oddnumbers,CompactBuffer((english,65))))\n",
    "#############################  union ##########################################################\n",
    "var a = sc.parallelize(Array(Array(1, 2), Array(3, 4)));\n",
    "var b = sc.parallelize(Array(Array(10, 20), Array(30, 40)));\n",
    "var c=a.union(b)\n",
    "c.collect() \n",
    "c.take(4)\n",
    "# Array(Array(1, 2), Array(3, 4), Array(10, 20), Array(30, 40))\n",
    "\n",
    "var a = sc.parallelize(Array('1','2','3'));\n",
    "var b = sc.parallelize(Array('A','B','C'));\n",
    "var c=a.union(b)\n",
    "c.collect()      # Array(1, 2, 3, A, B, C)\n",
    "############################# sample, takeSample ##################################################\n",
    "# take a fraction of a RDD with or without replacement\n",
    "var a = sc.parallelize(1 to 100);\n",
    "#spark uses Bernoulli sampling for taking the sample. \n",
    "#The fraction argument represent the probability of each element in the population getting selected for the sample\n",
    "a.sample(false,0.1).take(5)  # Array(23, 28, 36, 52, 55)\n",
    "a.takeSample(false, 5)       # Array(32, 17, 16, 85, 1)\n",
    "################################ mapPartitions, mapPartitionsWithIndex ############################\n",
    "#mapPartitions() is called once for each Partition \n",
    "var a = sc.parallelize(1 to 6, 3);\n",
    "#We get Iterator as an argument for mapPartition, through which we can iterate through all the elements in a Partition.\n",
    "def test(x:Iterator[Int]):Iterator[Int]={\n",
    "        var sum=0\n",
    "        while(x.hasNext){\n",
    "            sum = sum + x.next\n",
    "        }\n",
    "        return List(sum).iterator   \n",
    "    }\n",
    "def test(x:Iterator[Int]):Iterator[Int]={\n",
    "        return Array(x.sum).toIterator\n",
    "    }\n",
    "\n",
    "a.mapPartitions(test).collect() # sum of each partition # Array(3, 7, 11)\n",
    "\n",
    "a.mapPartitions( x => List(x.next).iterator).collect() #Array(1, 3, 5)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\n",
    "#mapPartitionsWithIndex provides an index to track the Partition No\n",
    "val a =  sc.parallelize(List(\"yellow\",\"red\",\"blue\",\"cyan\",\"black\"),3)\n",
    "val mapped =   a.mapPartitionsWithIndex{(index, iterator) => {println(\"Called in Partition -> \" + index)\n",
    "                                                              val myList = iterator.toList\n",
    "                                                              myList.map(x => x + \" -> \" + index).iterator}}\n",
    "mapped.collect() #Array(yellow -> 0, red -> 1, blue -> 1, cyan -> 2, black -> 2)\n",
    "################################ distinct #####################################################\n",
    "var a = sc.parallelize(Array('1','2','3','1'));\n",
    "a.distinct().collect() #Array(1, 2, 3)\n",
    "################################ sortBy, sortByKey ##################################################\n",
    "var a=List(('a',5),('c',2),('b', 3))\n",
    "var b=sc.parallelize(a)\n",
    "b.sortBy(x=>x._1).collect()        # Array((a,5), (b,3), (c,2))\n",
    "b.sortBy(x=>x._2).collect()        # Array((c,2), (b,3), (a,5))\n",
    "b.sortBy(x=>x._1, false).collect() # Array((c,2), (b,3), (a,5)) # ascending=false\n",
    "b.sortByKey().collect()            # Array((a,5), (b,3), (c,2))\n",
    "b.sortByKey(false).collect()       # Array((c,2), (b,3), (a,5))\n",
    "\n",
    "var a= sc.parallelize(Array(Array(1, 10, 3), Array(2, 3, 4), Array(3, 2, 1)))\n",
    "var c = a.sortBy(x => x(0), false).collect() #Array(Array(3, 2, 1), Array(2, 3, 4), Array(1, 10, 3))\n",
    "\n",
    "#sort the elements of this RDD\n",
    "var a=sc.parallelize(Array(2,5,1))\n",
    "a.sortBy(x=>x).collect() #Array(1, 2, 5)\n",
    "################################ subtract, intersection #######################################\n",
    "var a = sc.parallelize(Array('1','2','3'));\n",
    "var b = sc.parallelize(Array('1','2','5'));\n",
    "a.subtract(b).collect()     # Array(3)\n",
    "a.intersection(b).collect() # Array(1, 2)\n",
    "################################ cartesian #####################################################\n",
    "var a = sc.parallelize(Array('a','b'));\n",
    "var b = sc.parallelize(Array('1','2'));\n",
    "a.cartesian(b).collect() #Array((a,1), (a,2), (b,1), (b,2))\n",
    "################################ fold, foldByKey ###############################################\n",
    "#fold(initial value)(func)\n",
    "#Each element of each partition will be merged to the initial value by func\n",
    "var a = sc.parallelize(1 to 6, 2);\n",
    "var b = a.map(_.toString)\n",
    "def concat(x:String, y:String):String = x+y;\n",
    "var s=\"_\";\n",
    "b.fold(s)(concat)  # String = __123_456\n",
    "\n",
    "var a = sc.parallelize(1 to 6, 3);\n",
    "var b = a.map(_.toString)\n",
    "def concat(x:String, y:String):String = x+y;\n",
    "var s=\"!!\";\n",
    "b.fold(s)(concat) #!!!!56!!34!!12\n",
    "\n",
    "val a = sc.parallelize(List(1,2,3), 3)\n",
    "var b = a.map(_.toString)\n",
    "b.fold(\"_\")(_ + _) #String = __2_3_1\n",
    "a.fold(100)(_ + _) #Int = 406 # 100+100+2+100+3+100+1\n",
    "a.fold(0)(_ + _)   #Int = 6\n",
    "\n",
    "val a = sc.parallelize(Seq((3,\"dog\"), (3,\"cat\"), (2,\"owl\"), (2,\"gnu\")), 2)\n",
    "a.foldByKey(\"\")(_ + _).collect() //Array((2,owlgnu), (3,dogcat))\n",
    "################################ aggregate, aggregateByKey, reduce, reduceByKey ########################\n",
    "var a = sc.parallelize(1 to 4,2);\n",
    "var init=(0,0)\n",
    "a.aggregate(init) ((x, y) =>(x._1 + y, x._2 + 1), (x, y) => (x._1 + y._1, x._2 + y._2)) //(10,4)\n",
    "#The first partition : {1,2} ; The second partition : {3,4} ; \n",
    "#The intial x is init = (0,0), intial x._1=0, x._2=0\n",
    "#At the first partition: (x,y) =>(x._1 + y, x._2 + 1)        #(x,y)=(0+1, 0+1)=(1,1) for y=1 (the first element)\n",
    "#                                                             (x,y)=(1+2, 1+1)=(3,2) for y=2 (the second element)\n",
    "#At the second partition: (x,y) =>(x._1 + y, x._2 + 1)       #(x,y)=(0+3, 0+1)=(3,1) for y=3 (the first element)\n",
    "#                                                             (x,y)=(3+4, 1+1)=(7,2) for y=4 (the second element)\n",
    "# from (x, y) => (x._1 + y._1, x._2 + y._2) = (3+7, 2+2)=(10,4) merge two partition\n",
    "\n",
    "var a=sc.parallelize(1 to 4,2);\n",
    "var rdd_count=a.map((_,1));\n",
    "rdd_count.reduce((x,y)=>(x._1+y._1,x._2+y._2))  #(10,4)\n",
    "\n",
    "var a=sc.parallelize(1 to 10,5);\n",
    "def test(x:Int, y:Int):Int={return x+y}\n",
    "a.reduce(test) #Int = 55\n",
    "\n",
    "var arr = Array('a', 'b', 'a', 'a','b')\n",
    "var rdd = sc.parallelize(arr)\n",
    "var rdd1 = rdd.map(x=>(x,1))\n",
    "rdd1.take(2)\n",
    "#Array((a,1), (b,1))\n",
    "# groupby the key, for each key, compute sum of the values\n",
    "var rdd2 = rdd1.reduceByKey(_+_)\n",
    "var rdd2 = rdd1.reduceByKey((x, y)=>x+y)\n",
    "rdd2.take(2)\n",
    "#Array((a,3), (b,2))\n",
    "\n",
    "val rdd = sc.parallelize(List((15, 1),(12, 20),(25, 3)))\n",
    "def max(a:Int, b:Int)=if (b>a)b else a\n",
    "rdd.reduceByKey(max).take(3) # Array((25,3), (12,20), (15,1))\n",
    "\n",
    "val a = sc.parallelize(Array((\"a\", 3), (\"a\", 1), (\"b\", 7)))\n",
    "a.reduceByKey(_ + _).collect()             #Array((a,4), (b,7))\n",
    "a.aggregateByKey(0)(_+_,_+_).collect()     #Array((a,4), (b,7))\n",
    "a.aggregateByKey(100)(_+_,_+_).collect()   #Array((a,204), (b,107))\n",
    "import scala.collection.mutable.HashSet\n",
    "a.aggregateByKey(new HashSet[Int])(_+_, _++_).collect() #Array((a,Set(1, 3)), (b,Set(7)))\n",
    "################################ countByValue, top, takeOrdered  ################################\n",
    "var a = sc.parallelize(List(1,2,3,3))\n",
    "a.countByValue() #Map(1 -> 1, 2 -> 1, 3 -> 2)\n",
    "a.top(3)         #Array(3, 3, 2) #sorts and gets the maximum n values\n",
    "a.takeOrdered(2) #Array(1, 2)\n",
    "\n",
    "var a = List((2,\"ff\"), (4,\"aa\"), (1,\"cd\"))\n",
    "var b=sc.parallelize(a)\n",
    "b.takeOrdered(3)(Ordering[Int].reverse.on(x=>x._1))     #Array((4,aa), (2,ff), (1,cd))\n",
    "b.takeOrdered(3)(Ordering[String].reverse.on(x=>x._2))  #Array((2,ff), (1,cd), (4,aa))\n",
    "b.takeOrdered(3)(Ordering[String].on(x=>x._2))          #Array((4,aa), (1,cd), (2,ff))\n",
    "################################ countByKey, collectAsMap, lookup  ################################\n",
    "val a = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "a.countByKey()     #Map(english -> 1, maths -> 2)\n",
    "a.collectAsMap()   #Map(maths -> 60, english -> 65) # only return unique keys\n",
    "a.lookup(\"maths\")  #WrappedArray(50, 60)\n",
    "################################ combineByKey  ################################\n",
    "val a = sc.parallelize(Seq((\"maths\", 50), (\"maths\", 60), (\"english\", 65)))\n",
    "val reduced = a.combineByKey(\n",
    "  (v) => (v, 1),\n",
    "  (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),\n",
    "  (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  \n",
    "reduced.collect() //Array[(String, (Int, Int))] = Array((english,(65,1)), (maths,(110,2)))\n",
    "\n",
    "################################ foreach, foreachPartition #########################################\n",
    "# apply a functioon to all elements of this RDD\n",
    "def f(x:Int)=println(s\"Save $x to test\")\n",
    "sc.parallelize(1 to 5).foreach(f)\n",
    "#Save 2 to test\n",
    "#Save 4 to test\n",
    "#Save 5 to test\n",
    "#Save 3 to test\n",
    "#Save 1 to test\n",
    "\n",
    "# apply a function to each partition of this RDD\n",
    "def test(x:Iterator[Int])=println(\"The sum is \" + x.sum.toString)\n",
    "sc.parallelize(1 to 10,2).foreachPartition(test)\n",
    "#The sum is 15\n",
    "#The sum is 40\n",
    "################################ join, leftOuterJoin, rightOuterJoin ################################\n",
    "val rdd1 = sc.parallelize(List(\"abe\", \"abby\", \"apple\")).map(a => (a, 1))\n",
    "val rdd2 = sc.parallelize(List(\"apple\", \"beatty\", \"beatrice\")).map(a => (a, 1))\n",
    "rdd1.join(rdd2).collect()           # Array((apple,(1,1)))\n",
    "rdd1.leftOuterJoin(rdd2).collect()  # Array((abby,(1,None)), (apple,(1,Some(1))), (abe,(1,None)))\n",
    "rdd1.rightOuterJoin(rdd2).collect() # Array((apple,(Some(1),1)), (beatty,(None,1)), (beatrice,(None,1)))\n",
    "################################ saveAsTextFile #########################################\n",
    "# save the RDD result into HDFS\n",
    "rdd2.saveAsTextFile(\"my_result\")\n",
    "rdd2.coalesce(1).saveAsTextFile(\"my_result\")\n",
    "#combines all the RDD partitions into a single partition since we want a single output file,\n",
    "################################ contains #########################################\n",
    "var arr = Array(\"a b\", \"b d\", \"a c\")\n",
    "var rdd = sc.parallelize(arr)\n",
    "rdd.filter(x => x.contains(\"a\")).collect() //Array[String] = Array(a b, a c)\n",
    "rdd.filter(x => !x.contains(\"a\")).collect() // Array[String] = Array(b d)\n",
    "################################ first #########################################\n",
    "val c = sc.parallelize(List(\"Gnu\", \"Cat\", \"Rat\", \"Dog\"), 2)\n",
    "c.first #String = Gnu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################ persistence (caching) ##################################\n",
    "#when an action is exceuted, all RDD and its dependencies are recomputed.\n",
    "#That means if an action on RDD is called twice, the same computation would happen twice.\n",
    "#So to avoid re-computation on a RDD, we can cach or persist the RDD.\n",
    "#we need to unpersist an RDD before repersisting \n",
    "# which storage level to choose\n",
    "### if RDD fits comfortably in memory, use MEMORY_ONLY, this is most CPU efficient option\n",
    "### if not, try MEMORY_ONLY_SER\n",
    "### Do not persist to disk unless the computation is expensive.\n",
    "\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "rdd1.unpersist()\n",
    "###### persist the RDD  ######################################\n",
    "#method1\n",
    "rdd1.persist(StorageLevel.MEMORY_AND_DISK) #persist the RDD into memory and disk\n",
    "#(true, true, false, true, 1)\n",
    "#or \n",
    "rdd1.persist(StorageLevel.MEMORY_ONLY) \n",
    "#save RDD into memory, so (false, true, false, true, 1)\n",
    "#or\n",
    "rdd1.persist(StorageLevel.MEMORY_ONLY_SER)#(false, true, false, false, 1)\n",
    "#or \n",
    "rdd1.persist(StorageLevel.MEMORY_AND_DISK_SER)##(true, true, false, false, 1)\n",
    "#or \n",
    "rdd1.persist(StorageLevel.DISK_ONLY)#(true, false,, false, false, 1)\n",
    "##########################################################################\n",
    "#method2\n",
    "# create an instance of storage levle\n",
    "var mysl = StorageLevel(true, true, false, true, 1)\n",
    "#the first argument is whether to use discs or not\n",
    "#the second argument is whether to use memory or not\n",
    "#the third argument is whether to use the heap memory \n",
    "#(the object stored in heap is fast to retrieved)\n",
    "#the fourth argument is whether to persist deserialized\n",
    "#by setting False, it stores RDD as serialized Java object -- one byte array per partition\n",
    "#the last argument is to specify how many copies do we want\n",
    "rdd1.persist(mysl)\n",
    "#########################################################################\n",
    "rdd1.getStorageLevel\n",
    "StorageLevel.MEMORY_AND_DISK \n",
    "#StorageLevel(disk, memory, deserialized, 1 replicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.classification.NaiveBayes  \n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier \n",
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "\n",
    "import org.apache.spark.ml.feature.{Tokenizer, RegexTokenizer,StopWordsRemover,HashingTF, IDF, StringIndexer}  \n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}  \n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "import org.apache.spark.sql._ \n",
    "import org.apache.spark.sql.DataFrame   \n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.joda.time._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RDD from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a RDD from the file in HDFS\n",
    "var rdd1 = sc.textFile(\"/FileStore/tables/c4pzqsme1501527259399/amazon_cells_labelled.txt\")\n",
    "rdd1.take(2)\n",
    "#Array[String] = Array(So there is no way for me to plug it in here in the US unless I go by a converter.\t0,\n",
    "#Good case, Excellent value.\t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a review sentence and a corresponding label\n",
    "val rdd2 = rdd1.map(_.split(\"\\t\")).map(p => Row(p(1), p(0)))\n",
    "rdd2.take(2)\n",
    "#Array([0,So there is no way for me to plug it in here in the US unless I go by a converter.], \n",
    "#[1,Good case, Excellent value.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The schema is encoded in a string\n",
    "val schemaString = \"label review\"\n",
    "# Generate the schema based on the string of schema\n",
    "val schema = StructType(schemaString.split(\" \").map(fieldName => StructField(fieldName, StringType, true)))\n",
    "# Apply the schema to the RDD.\n",
    "val df = sqlContext.createDataFrame(rdd2, schema)\n",
    "# Register the DataFrames as a table.\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "# Convert String labels to Double type\n",
    "val df1 = df.selectExpr(\"cast(label as Double) as label\", \"review\")\n",
    "df1.show()\n",
    "#label|              review|\n",
    "#+-----+--------------------+\n",
    "#|  0.0|So there is no wa...|\n",
    "#|  1.0|Good case, Excell...|\n",
    "df1.printSchema()\n",
    "#root\n",
    "# |-- label: double (nullable = true)\n",
    "# |-- review: string (nullable = true)\n",
    "##################################################################\n",
    "val df = sqlContext\n",
    "  .read.format(\"com.databricks.spark.csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"dbfs:/databricks-datasets/data.gov/irs_zip_code_data/data-001/2013_soi_zipcode_agi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"label\").count().show()\n",
    "#+-----+-----+\n",
    "#|label|count|\n",
    "#+-----+-----+\n",
    "##|  1.0|  500|\n",
    "#|  0.0|  500|\n",
    "#+-----+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sampleData = List(10.2, 14.1,14.4,14.4,14.4,14.5,14.5,14.6,14.7,14.7, 14.7,14.9,15.1, 15.9,16.4)\n",
    "val rowRDD = sqlContext.sparkContext.makeRDD(sampleData.map(value => Row(value)))\n",
    "val schema = StructType(Array(StructField(\"value\",DoubleType)))\n",
    "val df = sqlContext.createDataFrame(rowRDD,schema)\n",
    "val quantiles = df.stat.approxQuantile(\"value\", Array(0.25,0.75),0.0) #Array[Double] = Array(14.4, 15.1)\n",
    "val Q1 = quantiles(0)  #Double = 14.4\n",
    "val Q3 = quantiles(1)  #Double = 15.1 \n",
    "val IQR = Q3 - Q1\n",
    "val lowerRange = Q1 - 1.5*IQR\n",
    "val upperRange = Q3+ 1.5*IQR\n",
    "# filter outliers\n",
    "val outliers = df.filter(s\"value < $lowerRange or value > $upperRange\")\n",
    "outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  convert the distinct labels in the input dataset to index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer : break a review sentence into individual words\n",
    "val tokenizer = new Tokenizer().setInputCol(\"review\").setOutputCol(\"words\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"review\").setOutputCol(\"words\").setPattern(\"\\\\W\") \n",
    "val df_tokenized = tokenizer.transform(df1)\n",
    "df_tokenized.select(\"review\", \"words\").take(1)\n",
    "#Array([Great for the jawbone.,WrappedArray(Great, for, the, jawbone)])\n",
    "##################################################################################################\n",
    "# remove stop words. Stop words are low-information words such as 'a', 'and', 'the'\n",
    "val remover = new StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\")\n",
    "val df_removed = remover.transform(df_tokenized)\n",
    "df_removed.select(\"words\",\"filtered\").take(1)\n",
    "#Array([WrappedArray(Great, for, the, jawbone),WrappedArray(Great, jawbone)])\n",
    "##################################################################################################\n",
    "# Convert to TF-IDF (Term frequency-inverse document frequency)\n",
    "val hashingTF = new HashingTF().setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "val df_TF = hashingTF.transform(df_removed)\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(df_TF)\n",
    "val df_idf = idfModel.transform(df_TF)\n",
    "df_idf.select(\"filtered\", \"rawFeatures\",\"features\").take(1)\n",
    "# [WrappedArray(great, jawbone),(262144,[138356,138642],[1.0,1.0]),(262144,[138356,138642],[2.3237873006446486,5.52246041819533]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  I split data into train (80%) and test (20%) sets. \n",
    "val Array(train1, test1) = df1.randomSplit(Array(0.8,0.2),seed=0L)\n",
    "# Cache the train and test data in-memory\n",
    "val train = train1.cache()\n",
    "val test = test1.cache()\n",
    "println(s\"Sample number of the train data = $numtrain, Sample number of the test data = $numtest\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val d1 = new DateTime()\n",
    "var d2 = new DateTime()\n",
    "println(\"Classifier trained in \" +(d2.getMillis()/ 1000L - d1.getMillis()/ 1000L)+\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(p1: Int, p2: Double, p3: Double, p4: Int): Unit = {\n",
    "val lr = new LogisticRegression()\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, remover, hashingTF, idfModel, lr))\n",
    "#Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(hashingTF.numFeatures, Array(p1)).\n",
    "addGrid(lr.regParam, Array(p2)).\n",
    "addGrid(lr.elasticNetParam, Array(p3)).\n",
    "addGrid(lr.maxIter, Array(p4)).build()\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").\n",
    "setMetricName(\"precision\")\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).\n",
    "setEstimatorParamMaps(paramGrid).setNumFolds(4) \n",
    "val cvModel = cv.fit(train)\n",
    "# average cross-validation accuracy metric/s on all folds                \n",
    "val average_score = cvModel.avgMetrics\n",
    "println(\"average cross-validation accuracy =\" + average_score(0))\n",
    "println(\"Accuracy score in the test data\" + \" = \" + evaluator.evaluate(cvModel.transform(test))*100 + \" %\")\n",
    "}\n",
    "#################################################################\n",
    "for( p1 <- Array(8000, 9000,9500, 10000)){\n",
    "for( p2 <- Array(0.01,0.02,0.03,0.04)){\n",
    "for( p3 <- Array(0.08,0.09,0.1, 0.11)){\n",
    "for( p4 <- Array(3,4,5,6,7)){\n",
    "println(s\"(numFeatures,regParam,elasticNetParam,maxIter)=($p1, $p2, $p3, $p4)\") \n",
    "grid_search(p1,p2,p3,p4)\n",
    "}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var best_score=0.0 \n",
    "for( p1 <- Array(5000,7000,8000,9000,10000)){\n",
    "for( p2 <- Array(0.1,0.02,0.03,0.04,0.05)){\n",
    "for( p3 <- Array(0.04,0.05,0.06,0.07,0.08)){\n",
    "for( p4 <- Array(5,6,7,9,11,15)){\n",
    "println(s\"(numFeatures,regParam,elasticNetParam,maxIter)=($p1, $p2, $p3, $p4)\")\n",
    "val lr = new LogisticRegression()\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, remover, hashingTF, idfModel, lr))\n",
    "# Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(hashingTF.numFeatures, Array(p1)).\n",
    "addGrid(lr.regParam, Array(p2)).\n",
    "addGrid(lr.elasticNetParam, Array(p3)).\n",
    "addGrid(lr.maxIter, Array(p4)).build()\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").\n",
    "setMetricName(\"precision\")\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).\n",
    "setEstimatorParamMaps(paramGrid).setNumFolds(4) \n",
    "val cvModel = cv.fit(train)\n",
    "# average cross-validation accuracy metric/s on all folds\n",
    "val average_score = cvModel.avgMetrics \n",
    "println(\"average cross-validation accuracy =\" + average_score(0))\n",
    "println(\"Accuracy score in the test data\" + \" = \" + evaluator.evaluate(cvModel.transform(test))*100 + \" %\")\n",
    "if(average_score(0)>best_score) {\n",
    "println(\"~~~~~~~~~~~Best score~~~~~~~~~~~~~\")\n",
    "best_score=average_score(0)\n",
    "}\n",
    "}}}}\n",
    "println(\"Best score is \" + best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trained by a logistic regression\n",
    "val lr = new LogisticRegression()\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "println(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# Build a pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(labelIndexer,tokenizer, remover, hashingTF, idfModel, lr))\n",
    "#Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(hashingTF.numFeatures, Array(50000)).\n",
    "addGrid(lr.regParam, Array(0.10)).\n",
    "addGrid(lr.elasticNetParam, Array(0.10)).\n",
    "addGrid(lr.maxIter, Array(10)).\n",
    "build()\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(4) \n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "val cvModel = cv.fit(train)\n",
    "#/average cross-validation accuracy on all folds\n",
    "val average_score = cvModel.avgMetrics \n",
    "println(\"average cross-validation accuracy =\" + average_score(0))\n",
    "\n",
    "# Make predictions on on the test data\n",
    "val prediction = cvModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trained by a logistic regression \n",
    "val lr = new LogisticRegression()\n",
    "#Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(lr.regParam, Array(0.10)).\n",
    "addGrid(lr.elasticNetParam, Array(0.10)).\n",
    "addGrid(lr.maxIter, Array(10)).\n",
    "build()\n",
    "#regParam (regularization parameter), \n",
    "#elasticNetParam (the ElasticNet mixing parameter), \n",
    "#maxIter (max number of iterations).\n",
    "\n",
    "##############################################\n",
    "# trained by a Na√Øve Bayes\n",
    "val nb = new NaiveBayes()\n",
    "#Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(nb.smoothing, Array(1.0)).build()\n",
    "#smoothing (smoothing parameter) 0 ~ 1 \n",
    "\n",
    "##############################################\n",
    "# trained by a Decision Tree\n",
    "val dt = new DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setImpurity(\"entropy\")\n",
    "#Create ParamGrid for Cross Validation \n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(dt.maxDepth, Array(25)).\n",
    "addGrid(dt.minInstancesPerNode, Array(4)).\n",
    "build()\n",
    "#maxDepth (maximum depth of the tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Decision Tree classifier). \n",
    "\n",
    "##############################################\n",
    "# trained by a Random Forest\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"indexedLabel\").setImpurity(\"entropy\").setSeed(5043)\n",
    "# Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(rf.numTrees, Array(31)).\n",
    "addGrid(rf.maxDepth, Array(29)).\n",
    "addGrid(rf.minInstancesPerNode, Array(1)).\n",
    "build()\n",
    "#numTrees (number of trees to train), \n",
    "#maxDepth (maximum depth of the tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Random Forest classifier). \n",
    "\n",
    "##############################################\n",
    "# trained by a Gradient Boosted Tree \n",
    "val gbt = new GBTClassifier().setLabelCol(\"indexedLabel\")\n",
    "# Create ParamGrid for Cross Validation\n",
    "val paramGrid = new ParamGridBuilder().\n",
    "addGrid(gbt.maxIter, Array(25)).\n",
    "addGrid(gbt.maxDepth, Array(19)).\n",
    "addGrid(gbt.minInstancesPerNode, Array(2)).\n",
    "build()\n",
    "#maxIter (max number of iterations), \n",
    "#maxDepth (maximum depth of the tree for a Gradient Boosted Tree classifier), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Gradient Boosted Tree classifier). \n",
    "\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########  Calculate accuracy of the prediction of the test data\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").\n",
    "setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "println(\"Accuracy score in the test data\" + \" = \" + evaluator.evaluate(prediction)*100 + \" %\")\n",
    "\n",
    "########  calculate area under ROC\n",
    "val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n",
    "println(\"areaUnderROC in the train data\" + \" = \" + evaluator.evaluate(cvModel.transform(train)))\n",
    "println(\"areaUnderROC in the test data\" + \" = \" + evaluator.evaluate(prediction))\n",
    "\n",
    "########  calculate F1 score of the prediction of the test data\n",
    "val evaluator_F1 = new MulticlassClassificationEvaluator().setLabelCol(\"label\").\n",
    "setPredictionCol(\"prediction\").setMetricName(\"f1\")\n",
    "println(\"F1 metric in the test data\" + \" = \" + evaluator_F1.evaluate(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
