{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from __future__ import division # show flaoting number\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lable encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer \n",
    "df=DataFrame([[1, 2], [np.nan, 3], [7, 6]])\n",
    "#    0   1\n",
    "#0  1.0  2\n",
    "#1  NaN  3\n",
    "#2  7.0  6\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0) #strategy='mean'('median','most_frequent')\n",
    "imp.fit(df)\n",
    "imp.transform(df)\n",
    "#    0   1\n",
    "#0  1.0  2\n",
    "#1    4  3\n",
    "#2  7.0  6\n",
    "#######################################################\n",
    "df=DataFrame({'column1':[1,4,2,1], 'column2':[1,2,2,1]})\n",
    "#    column1  column2\n",
    "#0      1        1\n",
    "#1      4        2    \n",
    "#2      2        2  \n",
    "#2      1        1     \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)df['column1'].values\n",
    "poly.fit_transform(df.values) # create a new feature matrix of polynomial degree = 2 combinations \n",
    "#if a sample is [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "#array([[  1.,   1.,   1.,   1.,   1.,   1.],\n",
    "#       [  1.,   4.,   2.,  16.,   8.,   4.],\n",
    "#       [  1.,   2.,   2.,   4.,   4.,   4.],\n",
    "#       [  1.,   1.,   1.,   1.,   1.,   1.]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import LabelEncoder#fit_transform(df['A'].values), inverse_transform(y)\n",
    "le =LabelEncoder()\n",
    "X_new=le.fit_transform(df['column1'].values) #array([0, 2, 1, 0], dtype=int64) \n",
    "le.classes_  #array([1, 2, 4], dtype=int64)\n",
    "le.inverse_transform(X_new) #array([1, 4, 2, 1], dtype=int64)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train=pd.DataFrame({'column1':['A', 'C', 'B']})\n",
    "test=pd.DataFrame({'column1':['A', 'B', 'D']})\n",
    "df_obj = train.select_dtypes(include=['object'])\n",
    "for i in df_obj: #i shows each 'object' column name\n",
    "    total=list(train[i].values.astype('str')) + list(test[i].values.astype('str')) #['A', 'C', 'B', 'A', 'B', 'D']\n",
    "    encoder=LabelEncoder()\n",
    "    encoder.fit(total)\n",
    "    train[i]=encoder.transform(list(train[i].values.astype('str')))\n",
    "    test[i] =encoder.transform(list(test[i].values.astype('str')))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from pandas import factorize\n",
    "factorize(df['column1'])[0] #array([0, 1, 2, 0], dtype=int64)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df=DataFrame({'column1':['A','B','A'], 'column2':['C','C','D']})\n",
    "df_obj = df.select_dtypes(include=['object'])\n",
    "for c in df_obj:\n",
    "    df_obj[c] = pd.factorize(df_obj[c])[0]\n",
    "#df_obj\n",
    "#      column1  column2\n",
    "#0        0        0\n",
    "#1        1        0\n",
    "#2        0        1\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(categorical_features=[0])#[0] : only encode the first column'; 'all' : encode all columns\n",
    "enc.fit_transform(df).toarray() \n",
    "#array([[ 1.,  0.,  0.,  1.],\n",
    "#       [ 0.,  0.,  1.,  2.],\n",
    "#       [ 0.,  1.,  0.,  2.],\n",
    "#       [ 1.,  0.,  0.,  1.]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import Binarizer\n",
    "B=Binarizer(threshold=1) # if a value > threshold, set to 1. Otherwise, set to 0. \n",
    "B.fit_transform(df['column1'].values) #array([[0, 1, 1, 0]], dtype=int64)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer(neg_label=0, pos_label=1)\n",
    "lb.fit_transform(df['column1'])\n",
    "#array([[1, 0, 0],\n",
    "#       [0, 0, 1],\n",
    "#       [0, 1, 0],\n",
    "#       [1, 0, 0]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=pd.DataFrame({'column1':['A', 'B', 'C']})\n",
    "df = pd.concat([df, pd.get_dummies(df['column1'], prefix='column1')],axis=1)\n",
    "df.drop(['column1'],axis=1, inplace=True)  \n",
    "df\n",
    "#     column1_A   column1_B   column1_C\n",
    "#0        1           0          0\n",
    "#1        0           1          0\n",
    "#2        0           0          1\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df=pd.DataFrame({'column1':['A', 'B', 'A'],'column2':['C', 'D', 'C'],\n",
    "                 'column3':[2.0, 3.0, 1.0],'column4':[4.0, 3.0, 2.0] })\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = []\n",
    "for f in df.select_dtypes(include=['object']):\n",
    "    #print f\n",
    "    dummy = pd.get_dummies(df[f].astype('category'))\n",
    "    dummy = csr_matrix(dummy)#\n",
    "    #print dummy.toarray()\n",
    "    features.append(dummy)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "float_columns_scaled=scaler.fit_transform(df[df.select_dtypes(exclude=['object']).columns])\n",
    "float_columns_scaled = csr_matrix(float_columns_scaled)\n",
    "features.append(float_columns_scaled)\n",
    "features=hstack(features, format = 'csr').toarray()\n",
    "#array([[ 1.        ,  0.        ,  1.        ,  0.        ,  0.        ,  1.22474487],\n",
    "#       [ 0.        ,  1.        ,  0.        ,  1.        ,  1.22474487,    0.      ],\n",
    "#       [ 1.        ,  0.        ,  1.        ,  0.        , -1.22474487, -1.22474487]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import theano\n",
    "from keras.utils.np_utils import to_categorical #like onehotencoder\n",
    "to_categorical(df['column1'].values,5) #number of classes =5\n",
    "#array([[ 0.,  1.,  0.,  0.,  0.],\n",
    "#       [ 0.,  0.,  0.,  0.,  1.],\n",
    "#       [ 0.,  0.,  1.,  0.,  0.],\n",
    "#       [ 0.,  1.,  0.,  0.,  0.]]\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "train=pd.DataFrame({'column1':[1, 3, 4]})\n",
    "test=pd.DataFrame({'column1':[1, 2, 3]})\n",
    "categories=list(train['column1'].unique()) + [f for f in test['column1'].unique() if f not in train['column1'].unique()]\n",
    "train=pd.concat([train,DataFrame(to_categorical(train['column1'], len(categories)+1))],axis=1)\n",
    "#     column1    0    1     2    3     4\n",
    "#0      1       0.0  1.0   0.0  0.0   0.0\n",
    "#1      3       0.0  0.0   0.0  1.0   0.0\n",
    "#2      2       0.0  0.0   0.0  0.0   1.0\n",
    "test=pd.concat([test,DataFrame(to_categorical(test['column1'], len(categories)+1))],axis=1)\n",
    "#     column1    0    1     2    3     4\n",
    "#0      1       0.0  1.0   0.0  0.0   0.0\n",
    "#1      3       0.0  0.0   1.0  0.0   0.0\n",
    "#2      2       0.0  0.0   0.0  1.0   0.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "df = pd.DataFrame({'A' : ['a', 'b', 'a','c'], 'B' : ['e', 'x', 'x','e']})\n",
    "cols=['A','B']\n",
    "vec = DictVectorizer()\n",
    "mkdict = lambda row: dict((col, row[col]) for col in cols)\n",
    "X=df[cols].apply(mkdict, axis=1)\n",
    "#0    {u'A': u'a', u'B': u'e'}\n",
    "#1    {u'A': u'b', u'B': u'x'}\n",
    "#2    {u'A': u'a', u'B': u'x'}\n",
    "#3    {u'A': u'c', u'B': u'e'\n",
    "vec.fit_transform(X).toarray()\n",
    "#array([[ 1.,  0.,  0.,  1.,  0.],\n",
    "#       [ 0.,  1.,  0.,  0.,  1.],\n",
    "#       [ 1.,  0.,  0.,  0.,  1.],\n",
    "#       [ 0.,  0.,  1.,  1.,  0.]])\n",
    "vecData = pd.DataFrame(vec.fit_transform(X).toarray())\n",
    "vecData.columns = vec.get_feature_names()\n",
    "vecData.index = df.index\n",
    "df.join(vecData).drop(cols, axis=1)\n",
    "#     A=a   A=b   A=c   B=e    B=x\n",
    "#0    1.0   0.0   0.0   1.0    0.0\n",
    "#1    0.0   1.0   0.0   0.0    1.0\n",
    "#2    1.0   0.0   0.0   0.0    1.0\n",
    "#3    0.0   0.0   1.0   1.0    0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression\n",
    "X = [[10, 0, 5], \n",
    "     [20, 1, 10], \n",
    "     [30, 0, 12], \n",
    "     [35, 1, 20]]\n",
    "Y = [1,2,3,4]\n",
    "from sklearn.feature_selection import VarianceThreshold#remove some features with low variance\n",
    "\n",
    "np.var(X, axis=0) #array([ 92.1875,   0.25  ,  29.1875])\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit_transform(X)\n",
    "#array([[10,  5],\n",
    "#       [20, 10],\n",
    "#       [30, 12],\n",
    "#       [35, 20]])\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import f_regression #each linear model fit one of the features\n",
    "fvalue, pvalue = f_regression(X, Y)\n",
    "zip(fvalue, pvalue) ##choose pvalue <=0.05, for those features are highly correlated with the target Y\n",
    "#[(96.333333333332732, 0.010221733442710707),\n",
    "# (0.49999999999999994, 0.55278640450004202),  # delete this feature\n",
    "# (35.063492063492014, 0.027354872578278844)]\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "selector = SelectKBest(f_regression, k=2) #k : Number of top features to select\n",
    "selector.fit_transform(X, Y)\n",
    "#array([[10,  5],\n",
    "#       [20, 10],\n",
    "#       [30, 12],\n",
    "#       [35, 20]])\n",
    "zip(selector.scores_ ,selector.pvalues_) \n",
    "#[(96.333333333332732, 0.010221733442710707),\n",
    "# (0.49999999999999994, 0.55278640450004202),\n",
    "# (35.063492063492014, 0.027354872578278844)]\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X, Y)\n",
    "model = SelectFromModel(clf, threshold=0.1, prefit=True)\n",
    "#Features whose importance is greater or equal are kept while the others are discarded.\n",
    "X_new = model.transform(X)\n",
    "#array([[ 0,  5],\n",
    "#       [ 1, 10],\n",
    "#       [ 0, 12],\n",
    "#       [ 1, 20]])\n",
    "model.get_support() #array([False,  True,  True], dtype=bool)\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import RFE #recursive feature elimination\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()#use lr to calculate each feature's weight\n",
    "#rank all features, i.e continue the elimination until the last ones\n",
    "rfe = RFE(lr, n_features_to_select=2)\n",
    "X_new = rfe.fit_transform(X,Y)\n",
    "#array([[ 0,  5],\n",
    "#       [ 1, 10],\n",
    "#       [ 0, 12],\n",
    "#       [ 1, 20]])\n",
    "rfe.ranking_ #array([2, 1, 1]) #1 if selected\n",
    "rfe.support_ #array([False,  True,  True], dtype=bool) (show each feature is selected or not (True or False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification\n",
    "Y = [1,0,0,1]\n",
    "X = [[10,0, 5], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [10,0, 20]]\n",
    "from sklearn.feature_selection import f_classif #Compute the ANOVA F-value for the provided sample\n",
    "fvalue, pvalue = f_classif(X, Y)\n",
    "zip(fvalue, pvalue) ##choose pvalue <=0.05, for those features are highly correlated with the target Y\n",
    "#[(225.0, 0.0044150325), (inf, 0.0), (0.03930131, 0.86117655)]\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import chi2 #Compute chi-squared stats between each non-negative feature and class\n",
    "chi2statistics, pvalue = chi2(X, Y)\n",
    "zip(chi2statistics, pvalue) #choose pvalue <=0.05, for those features are highly correlated with the target Y\n",
    "#[(9.0, 0.0026997960632601883),\n",
    "# (2.0, 0.15729920705028505),\n",
    "# (0.19148936170212766, 0.66167991486599997)]\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "selector = SelectKBest(f_classif, k=2) #k : Number of top features to select\n",
    "selector.fit_transform(X, Y)\n",
    "#array([[10,  0],\n",
    "#       [ 2,  1],\n",
    "#       [ 3,  1],\n",
    "#       [10,  0]])\n",
    "zip(selector.scores_ ,selector.pvalues_) #[(225.0, 0.0044150325), (inf, 0.0), (0.03930131, 0.86117655)]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "selector = SelectKBest(chi2, k=2) #k : Number of top features to select\n",
    "X_new=selector.fit_transform(X, Y)\n",
    "zip(selector.scores_ ,selector.pvalues_)\n",
    "#[(9.0, 0.0026997960632601883),\n",
    "# (2.0, 0.15729920705028505),\n",
    "# (0.19148936170212766, 0.66167991486599997)]\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "selector = SelectPercentile(f_classif, percentile=60) \n",
    "#selector = SelectPercentile(chi2, percentile=60) \n",
    "selector.fit_transform(X, Y)\n",
    "#array([[10,  0],\n",
    "#       [ 2,  1],\n",
    "#       [ 3,  1],\n",
    "#       [10,  0]])\n",
    "selector.get_support() #array([ True,  True, False], dtype=bool) show each feature is selected or not \n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(X, Y)\n",
    "model = SelectFromModel(clf, threshold=0.3, prefit=True)\n",
    "#Features whose importance is greater or equal are kept while the others are discarded.\n",
    "X_new = model.transform(X)\n",
    "X_new\n",
    "#array([[10,  0],\n",
    "#       [ 2,  1],\n",
    "#       [ 3,  1],\n",
    "#       [10,  0]])\n",
    "model.get_support() #array([ True,  True, False], dtype=bool)\n",
    "#############################################################################################\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mutual_info_classif(X,Y) #Return estimated mutual information between each feature and the target. \n",
    "#array([ 0.83333333,  0.33333333,  0.08333333])\n",
    "# MI value=0 if two variables are independent\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "selector = SelectKBest(mutual_info_classif, k=2) #k : Number of top features to select\n",
    "selector.fit_transform(X, Y)\n",
    "#array([[10,  0],\n",
    "#       [ 2,  1],\n",
    "#       [ 3,  1],\n",
    "#       [10,  0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = [1,0,0,1]\n",
    "X = [[10, 1, 0], \n",
    "     [-1, 0, 1], \n",
    "     [-1, 0, 1], \n",
    "     [10, 1, 0]]\n",
    "from sklearn.decomposition import DictionaryLearning \n",
    "#Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code\n",
    "dic = DictionaryLearning(n_components=2, max_iter=500) #n_components : number of classes\n",
    "dic.fit_transform(X)\n",
    "#array([[-10.04987562,   0.        ],\n",
    "#       [  0.        ,   1.41421356],\n",
    "#       [  0.        ,   1.41421356],\n",
    "#       [-10.04987562,   0.        ]])\n",
    "#for each sample, all values=0 except a class column which it belongs to\n",
    "#############################################################################################\n",
    "Y = [0,1,1,2,2]\n",
    "X =[[  5.1,  3.5,  1.4,  0.2],\n",
    "    [ 7. ,  3.2,  4.7,  1.4],\n",
    "    [ 6.4,  3.2,  4.5,  1.5],\n",
    "    [ 6.2,  3.4,  5.4,  2.3],\n",
    "    [ 5.9,  3. ,  5.1,  1.8]]\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2) \n",
    "#Number of components (< n_classes - 1) for dimensionality reduction.\n",
    "lda.fit(X, Y).transform(X)\n",
    "#array([[-7.92849875,  0.03078069],\n",
    "#       [ 2.70626992, -1.73129046],\n",
    "#       [ 1.12104434, -0.51183983],\n",
    "#       [ 2.66031756,  1.89878759],\n",
    "#       [ 1.44086692,  0.31356201]])\n",
    "lda.explained_variance_ratio_  #array([ 0.94059285,  0.05940715])\n",
    "#Percentage of variance explained by each of the selected components\n",
    "#############################################################################################\n",
    "X = [[10, 0, 5], \n",
    "     [20, 1, 10], \n",
    "     [30, 0, 12], \n",
    "     [35, 1, 20]]\n",
    "from sklearn.decomposition import FactorAnalysis \n",
    "fa = FactorAnalysis(n_components=2)\n",
    "fa.fit_transform(X)\n",
    "#array([[ 1.39947678, -0.06007879],\n",
    "#       [ 0.36108626, -0.63879181],\n",
    "#       [-0.47340936,  1.49222757],\n",
    "#       [-1.28715367, -0.79335696]])\n",
    "#############################################################################################\n",
    "from sklearn.decomposition import TruncatedSVD#truncated singular value decomposition#can operate on sparse x data\n",
    "svd = TruncatedSVD(n_components=2, n_iter=10, random_state=42)\n",
    "svd.fit(X) \n",
    "X_reduced=svd.transform(X) \n",
    "X_reduced\n",
    "#array([[ 11.17829772,   0.05080662],\n",
    "#       [ 22.37567529,  -0.07911717],\n",
    "#       [ 32.19050963,   2.78538999],\n",
    "#       [ 40.26344149,  -2.19704902]])\n",
    "#############################################################################################\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit_transform(X)\n",
    "pca.fit(X)\n",
    "pca.transform(X)\n",
    "#array([[ 15.31173002,  -0.57962934],\n",
    "#       [  4.11954044,  -0.35523   ],\n",
    "#       [ -5.60110411,   2.82751203],\n",
    "#       [-13.83016635,  -1.89265269]])\n",
    "pca.explained_variance_ratio_ #array([ 0.97444102,  0.02474639])\n",
    "plt.semilogy(pca.explained_variance_ratio_, '--o')\n",
    "plt.semilogy(pca.explained_variance_ratio_.cumsum(), '--o')\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "fig, ax=plt.subplots(nrows=2, ncols=1, figsize=(8, 12))\n",
    "ax[0].semilogy(pca.explained_variance_ratio_, '--o',label='explained variance ratio');\n",
    "ax[0].set_title('explained variance ratio')\n",
    "ax[0].set_ylim([10**-7, 1])\n",
    "ax[1].plot(pca.explained_variance_ratio_.cumsum(), '--o', label='cumulative explained variance ratio');\n",
    "ax[1].set_title('cumulative explained variance ratio')\n",
    "ax[1].set_xlabel('Principle components ')\n",
    "ax[1].set_ylim([0.2, 1])\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,5), facecolor='white')\n",
    "ax.plot(pca_trafo.explained_variance_ratio_.cumsum(),'--o',linewidth=3)\n",
    "ax.hlines(y=0.99, xmin=0, xmax=14, lw=2,color='r')\n",
    "ax.set_xlabel('Number of PCA components')\n",
    "ax.set_ylabel('Cumulative explained variance')\n",
    "ax.grid(color='black', linestyle='--', linewidth=1)\n",
    "#############################################################################################\n",
    "from sklearn.decomposition import KernelPCA\n",
    "pca = KernelPCA(kernel=\"rbf\", n_components=2, gamma=1)\n",
    "#kernel=“linear” (default)| “poly” | “rbf” | “sigmoid” | “cosine” \n",
    "pca.fit_transform(X)\n",
    "pca.fit(X)\n",
    "pca.transform(X)\n",
    "#array([[ 0.        ,  0.        ],\n",
    "#       [-0.57735027, -0.57735027],\n",
    "#       [-0.21132487,  0.78867513],\n",
    "#       [ 0.78867513, -0.21132487]])\n",
    "#############################################################################################\n",
    "from sklearn.manifold import MDS #multidimentional scaling \n",
    "mds = MDS(n_components=2)\n",
    "mds.fit_transform(X)\n",
    "#array([[ -4.91628672,  14.46430834],\n",
    "#       [ -2.49385278,   3.46170037],\n",
    "#       [ -0.45664302,  -6.50418256],\n",
    "#       [  7.86678252, -11.42182615]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if the regularization is applied, all features need to be scaled\n",
    "#Decision trees and random forests are one of the very few machine learning algorithms that don't need to scale features \n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaler.transform(X)\n",
    "scaler.fit_transform(X)\n",
    "#array([[-1.34164079, -1.34164079],\n",
    "#       [-0.4472136 , -0.4472136 ],\n",
    "#       [ 0.4472136 ,  0.4472136 ],\n",
    "#       [ 1.34164079,  1.34164079]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import scale \n",
    "scale(X, axis=0, with_mean=True, with_std=True)#enter=0, std=1\n",
    "#array([[-1.34164079, -1.34164079],\n",
    "#       [-0.4472136 , -0.4472136 ],\n",
    "#       [ 0.4472136 ,  0.4472136 ],\n",
    "#       [ 1.34164079,  1.34164079]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "scaler.transform(X)\n",
    "scaler.fit_transform(X)\n",
    "#array([[ 0.        ,  0.        ],\n",
    "#       [ 0.33333333,  0.33333333],\n",
    "#       [ 0.66666667,  0.66666667],\n",
    "#       [ 1.        ,  1.        ]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.preprocessing import normalize\n",
    "normalize(X) #scale each samle to a length of 1\n",
    "#array([[ 0.4472136 ,  0.89442719],\n",
    "#       [ 0.6       ,  0.8       ],\n",
    "#       [ 0.6401844 ,  0.76822128],\n",
    "#       [ 0.65850461,  0.75257669]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Normalize each feature column by the median of the feature values\n",
    "df=DataFrame(X)\n",
    "df.apply(lambda x: x / np.median(x), axis=0) \n",
    "#     0      1\n",
    "#0   0.25   0.4\n",
    "#1   0.75   0.8\n",
    "#2   1.25   1.2\n",
    "#3   1.75   1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download() #various corpus will be saved in your PC\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "W = WordNetLemmatizer()\n",
    "W.lemmatize('dogs') #u'dog'\n",
    "W.lemmatize('gathering','v') # u'gather'\n",
    "W.lemmatize('gathering','n') #'gathering'\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "words = [\"gaming\",\"gamed\",\"games\"]\n",
    "ps = PorterStemmer()\n",
    "[ps.stem(i) for i in words] #[u'game', u'game', u'game']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "rom nltk.stem import SnowballStemmer \n",
    "words = [\"gaming\",\"gamed\",\"games\"]\n",
    "s = SnowballStemmer('english')\n",
    "[s.stem(i) for i in words] #[u'game', u'game', u'game']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk.stem import LancasterStemmer \n",
    "words = [\"gaming\",\"gamed\",\"games\"]\n",
    "lan = LancasterStemmer()\n",
    "[lan.stem(i) for i in words] #['gam', 'gam', 'gam']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk.corpus import stopwords\n",
    "words = [\"how\",\"are\",\"dogs\"]\n",
    "[w for w in words if not w in stopwords.words('english')]  #['dogs']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk.tokenize import word_tokenize#A=word_tokenize('a book')\n",
    "sentence = 'Please buy me a book.\\n\\nThanks.'\n",
    "s=word_tokenize(sentence) #['Please', 'buy', 'me', 'a', 'book', '.', 'Thanks', '.']\n",
    "\n",
    "from nltk import FreqDist\n",
    "FreqDist(s) #show the appearing times for each token\n",
    "#FreqDist({'.': 2,\n",
    "#          'Please': 1,\n",
    "#          'Thanks': 1,\n",
    "#          'a': 1,\n",
    "#          'book': 1,\n",
    "#          'buy': 1,\n",
    "#          'me': 1})\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk import pos_tag, word_tokenize\n",
    "pos_tag(word_tokenize('a book'))\n",
    "#[('a',DT), ('book',NN)]       #NN:noun, JJ:adjective, DT:determiner, VB:verbs, RB:adverbs\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from nltk import bigrams\n",
    "b=list(bigrams('This is book'))\n",
    "#[('T', 'h'),\n",
    "# ('h', 'i'),\n",
    "# ('i', 's'),\n",
    "# ('s', ' '),\n",
    "# (' ', 'i'),\n",
    "# ('i', 's'),\n",
    "# ('s', ' '),\n",
    "# (' ', 'b'),\n",
    "# ('b', 'o'),\n",
    "# ('o', 'o'),\n",
    "# ('o', 'k')]\n",
    "from collections import Counter\n",
    "Counter(b)\n",
    "#Counter({(' ', 'b'): 1,\n",
    "#         (' ', 'i'): 1,\n",
    "#         ('T', 'h'): 1,\n",
    "#         ('b', 'o'): 1,\n",
    "#         ('h', 'i'): 1,\n",
    "#         ('i', 's'): 2,\n",
    "#         ('o', 'k'): 1,\n",
    "#         ('o', 'o'): 1,\n",
    "#         ('s', ' '): 2})\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "rom string import punctuation\n",
    "from string import lower\n",
    "from string import split\n",
    "text= 'This is a book, and a car.'\n",
    "exclude= [\"is\", \"a\"]\n",
    "' '.join(x for x in (word.strip(punctuation) for word in text.split()) if x.lower() not in exclude) #'This book and car'\n",
    "\n",
    "text.split() #['This', 'is', 'a', 'book,', 'and', 'a', 'car.']\n",
    "[word.strip(punctuation) for word in text.split()] #['This', 'is', 'a', 'book', 'and', 'a', 'car']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from string import punctuation\n",
    "from string import lower\n",
    "from string import split\n",
    "text= 'This is a book, and a car.'\n",
    "exclude= [\"is\", \"a\"]\n",
    "text.split() #['This', 'is', 'a', 'book,', 'and', 'a', 'car.']\n",
    "[word.strip(punctuation) for word in text.split()] #['This', 'is', 'a', 'book', 'and', 'a', 'car']\n",
    "\n",
    "' '.join(x for x in (word.strip(punctuation) for word in text.split()) if x.lower() not in exclude) #'This book and car'\n",
    "' '.join(x for x in (word.strip(punctuation) for word in text.split())) #'This is a book and a car'\n",
    "\n",
    "from string import maketrans\n",
    "from string import translate \n",
    "text.translate(maketrans(\"\",\"\"), punctuation) #'This is a book and a car'\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "text= 'This is a! book@, and a car.'\n",
    "from re import sub\n",
    "print re.split('\\W+', text) \n",
    "#['This', 'is', 'a', 'book', 'and', 'a', 'car', '']\n",
    "print sub('[\\w]+',' ', text.lower()) \n",
    "#     !  @,      .    .\n",
    "#re.sub with a special pattern as the first argument. Matches are replaced with an empty string (removed). \n",
    "\n",
    "v = \"\"\"<p id=1>Sometimes, <b>simpler</b> is better,but <i>not</i> always.</p>\"\"\"\n",
    "# Replace HTML tags with an empty string.\n",
    "re.sub(\"<.*?>\", \"\", v)#'Sometimes, simpler is better,but not always.'\n",
    "#<      Less-than sign (matches HTML bracket).\n",
    "#.*?    Match zero or more chars.\n",
    "#>      Greater-than (matches HTML bracket).\n",
    "\n",
    "text = \"He was carefully disguised but captured quickly by police.\"\n",
    "re.findall(r\"\\w+ly\", text) #['carefully', 'quickly']\n",
    "' '.join(\"a in_book\")                         #'a   i n _ b o o k'\n",
    "' '.join([\"a\", \"in_book\"])                    #'a in_book'\n",
    "' '.join([\"a\", \"in_book\"]).replace('_', ' ')  #'a in book'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts=[\"a dog cat fish\",\"the dog cat cat\",\"any fish bird\"]\n",
    "cv = CountVectorizer(stop_words='english',lowercase=True)\n",
    "#index items have no stop words, Convert all characters to lowercase before tokenizing.\n",
    "X_train_counts = cv.fit_transform(texts)\n",
    "cv.fit_transform(texts).toarray()\n",
    "cv.fit_transform(texts).todense()\n",
    "#array([[0, 1, 1, 1],\n",
    "#       [0, 2, 1, 0],\n",
    "#       [1, 0, 0, 1]], dtype=int64)\n",
    "cv.get_feature_names()  #[u'bird', u'cat', u'dog', u'fish']\n",
    "cv.vocabulary_ #{u'bird': 0, u'cat': 1, u'dog': 2, u'fish': 3}\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf=TfidfTransformer(norm=\"l2\",smooth_idf=True)\n",
    "tf.fit_transform(X_train_counts).toarray()\n",
    "#array([[ 0.        ,  0.57735027,  0.57735027,  0.57735027],\n",
    "#       [ 0.        ,  0.89442719,  0.4472136 ,  0.        ],\n",
    "#       [ 0.79596054,  0.        ,  0.        ,  0.60534851]])\n",
    "##################################################################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #(CountVectorizer  + TfidfTransformer)\n",
    "texts=[\"a dog cat fish\",\"the dog cat cat\",\"any fish bird\"]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(texts).toarray()\n",
    "#array([[ 0.        ,  0.57735027,  0.57735027,  0.57735027],\n",
    "#       [ 0.        ,  0.89442719,  0.4472136 ,  0.        ],\n",
    "#       [ 0.79596054,  0.        ,  0.        ,  0.60534851]])\n",
    "texts=[\"a dog cat fish\",\"the dog cat cat\",\"any fish bird\"]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit_transform(texts).toarray()\n",
    "#array([[ 0.        ,  0.57735027,  0.57735027,  0.57735027],\n",
    "#       [ 0.        ,  0.89442719,  0.4472136 ,  0.        ],\n",
    "#       [ 0.79596054,  0.        ,  0.        ,  0.60534851]])\n",
    "vectorizer.get_feature_names() #u'bird', u'cat', u'dog', u'fish']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "vectorizer = TfidfVectorizer(stop_words='english',ngram_range=(1,3)) #ngram_range=(min words, max words)\n",
    "vectorizer.fit_transform(texts).toarray()\n",
    "#array([[ 0.        ,  0.36617957,  0.        ,  0.48148213,  0.36617957,\n",
    "#         0.36617957,  0.        ,  0.48148213,  0.36617957,  0.        ],\n",
    "#       [ 0.        ,  0.6503311 ,  0.42755362,  0.        ,  0.32516555,\n",
    "#         0.32516555,  0.42755362,  0.        ,  0.        ,  0.        ],\n",
    "#       [ 0.62276601,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "#         0.        ,  0.        ,  0.        ,  0.4736296 ,  0.62276601]])\n",
    "vectorizer.get_feature_names()\n",
    "#[u'bird',\n",
    "# u'cat',\n",
    "# u'cat cat',\n",
    "# u'cat fish',\n",
    "# u'dog',\n",
    "# u'dog cat',\n",
    "# u'dog cat cat',\n",
    "# u'dog cat fish',\n",
    "# u'fish',\n",
    "# u'fish bird']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "texts=[\"a dog cat fish :d\",\"the dog cat cat \\br\\b\",\"any fish bird\"]\n",
    "def P1(text):\n",
    "    text=text.lower()\n",
    "    emoticons={':d':'good',':)':'good',':(':'good',}\n",
    "    E=[k for k in list(emoticons.keys())]\n",
    "    for i in E:\n",
    "        text=text.replace(i, emoticons[i])\n",
    "    abbreviations={r\"\\br\\b\": \"are\", r\"\\bu\\b\": \"you\"}\n",
    "    for i, j in abbreviations.items():\n",
    "        text=re.sub(i,j, text)#replace i with j# ex: i=\\br\\b, j=are\n",
    "    return text\n",
    "vectorizer = TfidfVectorizer(preprocessor=P1 )\n",
    "vectorizer.fit_transform(texts).toarray()\n",
    "vectorizer.get_feature_names() #[u'any', u'are', u'bird', u'cat', u'dog', u'fish', u'good', u'the']\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "#turns a collection of text documents into a sparse matrix of token occurrence counts, no IDF weighting \n",
    "texts=[\"a dog cat fish :d\",\"the dog cat cat \\br\\b\",\"any fish bird\"]\n",
    "hv = HashingVectorizer(n_features=2**2,ngram_range=(1,1),stop_words='english',lowercase =True,preprocessor=P1)\n",
    "hv.fit_transform(texts).toarray()\n",
    "#array([[ 0.        ,  0.        ,  0.        ,  1.        ],\n",
    "#       [ 0.        , -0.4472136 ,  0.        ,  0.89442719],\n",
    "#       [ 0.        , -0.70710678,  0.        ,  0.70710678]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Image preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mahotas import imread\n",
    "imread('test.jpg') # show each pixel with RGB values # shape is (4128L, 3096L, 3L)\n",
    "img=imread('test.jpg',as_grey=True) # shape is (4128L, 3096L) ##each pixel value is 0~255 float values\n",
    "\n",
    "from mahotas import imsave\n",
    "mh.imsave('copy.png', img)\n",
    "\n",
    "from mahotas import stretch\n",
    "im_stretch=stretch(img, 0,100) #stretch the image to the range of min:0 and max:100\n",
    "\n",
    "from mahotas.colors import rgb2grey #Compute luminance of an RGB image\n",
    "im=imread('test.jpg')\n",
    "rgb2grey(im, dtype=np.uint8) # shape is (4128L, 3096L)  #each pixel value is 0~255 int values\n",
    "\n",
    "from mahotas import gaussian_filter #can remove very noisy edges\n",
    "image=imread('test.jpg',as_grey=True) #shape is (4128L, 3096L)\n",
    "image_filter=gaussian_filter(image, sigma=16) #sigma : standard deviation for Gaussian kernel\n",
    "plt.imshow(image_filter)\n",
    "#####################################################################################\n",
    "from skimage.color import rgb2hsv \n",
    "#RGB to HSV (Hue, saturation, Value/Brightness) color space conversion\n",
    "im=imread('test.jpg')\n",
    "rgb2hsv(im) # shape is (4128L, 3096L, 3L)\n",
    "#####################################################################################\n",
    "from skimage.measure import find_contours \n",
    "x, y = np.ogrid[-np.pi:np.pi:100j, -np.pi:np.pi:100j]\n",
    "image = np.sin(np.exp((np.sin(x)**3 + np.cos(y)**2)))\n",
    "\n",
    "# Find contours at a constant value of 0.8\n",
    "contours =find_contours(r, 0.8)\n",
    "\n",
    "# Display the image and plot all contours found\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(r,interpolation='nearest', cmap=plt.cm.gray)\n",
    "# plot a list of found contours at a constant value of 0.8\n",
    "for n, contour in enumerate(contours):\n",
    "    ax.plot(contour[:, 1], contour[:, 0], linewidth=2)\n",
    "# plot the outer edge\n",
    "contour_max=max(contours, key=len)\n",
    "ax.plot(contour_max[:, 1], contour_max[:, 0], linewidth=10)\n",
    "ax.set_xticks([])\n",
    "ax.axis('image')\n",
    "ax.set_yticks([])\n",
    "plt.show()\n",
    "#####################################################################################\n",
    "from scipy.misc import imread\n",
    "im=imread('test.jpg') # show each pixel with RGB values # shape is (4128L, 3096L, 3L)\n",
    "plt.imshow(im)\n",
    "from scipy.ndimage import center_of_mass #Calculate the center of mass of an image\n",
    "center_of_mass(im) #(17409.064992314547, 12272.500769857432, 7.4503332729033245\n",
    "\n",
    "x = np.array([[0,0,0,0],\n",
    "              [0,1,1,0],\n",
    "              [0,1,1,0],\n",
    "              [0,1,1,0],\n",
    "              [0,0,0,0]])\n",
    "center_of_mass(x) #(2.0, 1.5) #Coordinates of centers-of-mass\n",
    "#####################################################################################\n",
    "import cv2\n",
    "from cv2 import imread, imwrite\n",
    "from cv2 import imshow\n",
    "from cv2 import waitKey\n",
    "from cv2 import destroyAllWindows\n",
    "from cv2 import resize\n",
    "from cv2 import INTER_AREA, INTER_LINEAR\n",
    "\n",
    "im=imread('test.jpg',1) # show each pixel with RGB values # shape is (4128L, 3096L, 3L) \n",
    "cv2.imwrite('copy.png',im)# save an image\n",
    "imshow('image',im)\n",
    "waitKey(0)\n",
    "destroyAllWindows()\n",
    "\n",
    "im=imread('test.jpg',0) # show each pixel with 0~255 int values # shape is (4128L, 3096L)\n",
    "height, width = im.shape\n",
    "im_resize = resize(im, (int(width/10), int(height/10)), interpolation = INTER_AREA)\n",
    "#interpolation = INTER_LINEAR (default)\n",
    "#INTER_AREA - resampling using pixel area relation. It is similar to the INTER_NEAREST method.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "if im.shape[0] > im.shape[1]:\n",
    "    scaled_size = (256, int(im.shape[0]*256/im.shape[1]))\n",
    "else:\n",
    "    scaled_size = (int(im.shape[1]*256/im.shape[0]),256)\n",
    "im_resize = cv2.resize(im, dsize=scaled_size) \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "plt.imshow(im_resize)\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.show()\n",
    "\n",
    "# Convert color to HSV\n",
    "hsvimage = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
    "# Convert color to gray\n",
    "grayimage= cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "#check of an image is blury or not\n",
    "blurness=cv2.Laplacian(grayimage, cv2.CV_64F).var()\n",
    "\n",
    "#convert a color image to a bi-level image by using an optimal threshold. \n",
    "a=2\n",
    "thresh = (grayimage.max())/a\n",
    "maxValue = 255\n",
    "_, outputimage = cv2.threshold(grayimage, thresh, maxValue, cv2.THRESH_BINARY);\n",
    "#the outputimage show 0 or maxValue. \n",
    "#The gray value of grayimage above thresh set to the value of maxValue (ex: 255), less than thresh set to 0\n",
    "contours, hierarchy = cv2.findContours(outputimage,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "main_contour = sorted(contours, key = cv2.contourArea, reverse = True)[0] #choose the outest contour\n",
    "# set the blank image\n",
    "ff = np.zeros((gray.shape[0],gray.shape[1]), 'uint8') \n",
    "# Draw the outest contour in the blank image\n",
    "cv2.drawContours(ff, main_contour,-1, 1, 20) #(destination_image, contours,contourIdx, contour_color, contour_thickness)\n",
    "#if contours is negative, all the contours are drawn.\n",
    "ff_mask = np.zeros((gray.shape[0]+2,gray.shape[1]+2), 'uint8')\n",
    "# fill red color inside the outest contour\n",
    "cv2.floodFill(ff, ff_mask, (int(gray.shape[1]/2), int(gray.shape[0]/2)),1)\n",
    "#(image, mask, start_pont, newVal) # Floodfill started from point (int(gray.shape[1]/2), int(gray.shape[0]/2))\n",
    "# newVal– New value of the repainted domain pixels.\n",
    "plt.imshow(ff)\n",
    "#####################################################################################\n",
    "from PIL import Image\n",
    "im = Image.open(\"test.jpg\") # show a color image\n",
    "#im = Image.open(\"test.jpg\").convert('L') # show a gray image\n",
    "im=im.resize((100,100),Image.ANTIALIAS) #ANTIALIAS is a high-quality downsampling filter\n",
    "im=im.crop((0,0,80,80))# define the left, upper, right, and lower pixel coordinate\n",
    "#(0, 0) in the upper left corner\n",
    "im1=np.asarray(im.getdata())#Returns pixel values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mahotas import imread\n",
    "from mahotas.features import surf \n",
    "image=imread('test.jpg',as_grey=True)\n",
    "features=surf.surf(image, descriptor_only=True) #shape is (1013L, 64L), s has (num of interesting points, 64 descriptors)\n",
    "\n",
    "s=surf.surf(image, 4, 6, 1)##draws coloured polygons around the interest points\n",
    "f=surf.show_surf(image, s)\n",
    "plt.imshow(f)\n",
    "\n",
    "surf.dense(image, spacing=64) #Descriptors at dense points in a distance of 64 pixels from each other\n",
    "# shape is (2684L, 64L)\n",
    "#####################################################################################\n",
    "from mahotas import imread\n",
    "from mahotas.features import lbp #local binary pattern\n",
    "image=imread('test.jpg',as_grey=True)\n",
    "features=lbp(image, radius=30, points=12)# shape is (352L,), histograms of the pixel code counts\n",
    "#####################################################################################\n",
    "from mahotas import imread\n",
    "from mahotas.features import haralick\n",
    "#calculate the probability that a pixel with the gray level i is adjacent to a pixel with the gray level j \n",
    "image=imread('test.jpg',as_grey=True)\n",
    "features=haralick(image.astype('int32')).ravel() #shape is (52L,) 4 directions and 14 statistics for each direction\n",
    "#####################################################################################\n",
    "from mahotas import imread\n",
    "from skimage.filters import sobel# perform edge detection using the Sobel transform\n",
    "image=imread('test.jpg',as_grey=True) #shape is (4128L, 3096L)\n",
    "features=sobel(image)  #shape is (4128L, 3096L)\n",
    "#####################################################################################\n",
    "#color histogram features (put pixel color values into histogram) \n",
    "from mahotas import imread\n",
    "image=imread('test.jpg')\n",
    "image = image // 64\n",
    "r, g, b=image.transpose((2,0,1))# each pixel value of r (or g or b) is 0,1,2 or 3# each image has 4*4*4=64 different colors\n",
    "pixelvalue=r+4*b+16*g\n",
    "H=np.bincount(pixelvalue.ravel(), minlength=64).astype(float) #shape : (64L,)\n",
    "features=np.log1p(H)#shape : (64L,)\n",
    "\n",
    "from mahotas import imread\n",
    "from cv2 import calcHist\n",
    "image=imread('test.jpg')\n",
    "hist=calcHist([image], [1,2,0], None, [4, 4, 4], [0, 256, 0, 256, 0, 256]) #shape : (4,4,4)\n",
    "hist=hist.ravel() #shape : (64L,)\n",
    "features= np.log1p(hist)#shape : (64L,)\n",
    "#####################################################################################\n",
    "image=np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "from skimage.feature import corner_harris, corner_peak\n",
    "image_corner=corner_peaks(corner_harris(image))# show the coordinates of the corners\n",
    "#array([[2, 2],\n",
    "#       [2, 7],\n",
    "#       [7, 2],\n",
    "#       [7, 7]], dtype=int64\n",
    "plt.imshow(image, cmap='Set3')\n",
    "plt.scatter(image_corner[:,1],image_corner[:,0]) #show corner marks\n",
    "#####################################################################################\n",
    "from skimage.transform import downscale_local_mean \n",
    "image=np.array([[ 1,2,0,1],\n",
    "            [ 1,0,1,8],\n",
    "            [ 1,0,1,1],\n",
    "            [0,0,2,2]])\n",
    "downscale_local_mean(image, (2, 2))\n",
    "#array([[ 1.  ,  2.5 ],\n",
    "#       [ 0.25,  1.5 ]])\n",
    "#####################################################################################\n",
    "from scipy.ndimage import binary_erosion, binary_dilation, binary_opening, binary_closing\n",
    "image=np.array([[0, 0, 0, 0, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 0]])\n",
    "binary_erosion(image).astype(a.dtype)# shrinking the shapes in an image\n",
    "#array([[0, 0, 0, 0, 0],\n",
    "#       [0, 0, 0, 0, 0],\n",
    "#       [0, 0, 1, 0, 0],\n",
    "#       [0, 0, 0, 0, 0],\n",
    "#       [0, 0, 0, 0, 0]])\n",
    "binary_dilation(image).astype(a.dtype)# expanding the shapes in an image.\n",
    "#array([[0, 1, 1, 1, 0],\n",
    "#       [1, 1, 1, 1, 1],\n",
    "#       [1, 1, 1, 1, 1],\n",
    "#       [1, 1, 1, 1, 1],\n",
    "#       [0, 1, 1, 1, 0]])\n",
    "binary_opening(image).astype(a.dtype)#the succession of an erosion and a dilation \n",
    "#array([[0, 0, 0, 0, 0],\n",
    "#       [0, 0, 1, 0, 0],\n",
    "#       [0, 1, 1, 1, 0],\n",
    "#       [0, 0, 1, 0, 0],\n",
    "#       [0, 0, 0, 0, 0]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "image=np.array([[0, 0, 0, 0, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 1, 0, 1, 0],\n",
    "                [0, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 0]])\n",
    "\n",
    "binary_closing(image).astype(a.dtype)#the succession of a dilation and an erosion \n",
    "#array([[0, 0, 0, 0, 0],\n",
    "#       [0, 1, 1, 1, 0],\n",
    "#       [0, 1, 1, 1, 0],\n",
    "#       [0, 1, 1, 1, 0],\n",
    "#       [0, 0, 0, 0, 0]])\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset, pipeline, cross validation and params search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = [1,0,0,1]\n",
    "X = [[10,0, 5], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [10,0, 20]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "############################ Pipeline   ##################################################\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "pca = PCA(random_state=0,n_components=2)\n",
    "classifier = SVC(random_state=0,C=1)\n",
    "\n",
    "#~~~~~~~~~~~~~\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline(steps=[('pca', pca), ('clf', classifier)])\n",
    "#~~~~~~~~~~~~~\n",
    "from sklearn.pipeline import make_pipeline \n",
    "pipe = make_pipeline(pca, classifier)\n",
    "#~~~~~~~~~~~~~\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "pipe.score(X_train, y_train)\n",
    "pipe.named_steps['pca'].n_components #2 , show the value of pca_n_components\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "X = [[10,0, 5], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [10,0, 20]]\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "pca = PCA(random_state=0,n_components=3)\n",
    "Kernelpca = KernelPCA(kernel=\"rbf\", n_components=2, gamma=1)\n",
    "combined = FeatureUnion([('linear_pca', pca), ('kernel_pca',Kernelpca)])\n",
    "combined.fit_transform(X)\n",
    "#array([[ -5.87353048e+00,   5.03752193e+00,   3.53522640e-03, 5.00841535e-01,   7.07106781e-01],\n",
    "#       [ -2.55786711e+00,  -3.85080151e+00,  -6.03620876e-02, -5.00841535e-01,  -5.06429716e-14],\n",
    "#       [ -4.01757915e-01,  -3.27243849e+00,   6.88251651e-02, -5.00841535e-01,  -3.40261343e-15],\n",
    "#       [  8.83315551e+00,   2.08571806e+00,  -1.19983039e-02,  5.00841535e-01,  -7.07106781e-01]])\n",
    "\n",
    "###########################   ParameterGrid ###############################################\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "list(ParameterGrid(param_grid))\n",
    "#[{'a': 1, 'b': True},\n",
    "# {'a': 1, 'b': False},\n",
    "# {'a': 2, 'b': True},\n",
    "# {'a': 2, 'b': False}]\n",
    "\n",
    "##############################  GridSearchCV  ##############################################\n",
    "from sklearn.pipeline import Pipeline\n",
    "pca = PCA(random_state=0)\n",
    "n_components = range(1,3,1)\n",
    "classifier = SVC(random_state=0)\n",
    "C = np.logspace(-3,3,7)\n",
    "pipe = Pipeline(steps=[('pca', pca), ('clf', classifier)])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipe,dict(pca__n_components=n_components, clf__C=C), scoring='accuracy', n_jobs=-1,cv=3)\n",
    "#scoring='log_loss'\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "my_scorer=make_scorer(score_func=f1_score, greater_is_better=True)\n",
    "grid = GridSearchCV(pipe,dict(pca__n_components=n_components, clf__C=C), scoring=my_scorer, n_jobs=-1,cv=3)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "grid = RandomizedSearchCV(pipe,param_distributions=dict(pca__n_components=n_components, clf__C=C), scoring='accuracy', \n",
    "                          n_jobs=-1,cv=3,n_iter=4)\n",
    "# randomly pick up n_iter parameter combibation, For example: n_iter=4\n",
    "#{'pca__n_components': 1, 'clf__C': 1000}\n",
    "#{'pca__n_components': 1, 'clf__C': 100}\n",
    "#{'pca__n_components': 3, 'clf__C': 100}\n",
    "#{'pca__n_components': 2, 'clf__C': 0.001}\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "for params, mean_score, scores in grid.grid_scores_: \n",
    "    print '%.5f +/- %.5f %r' %(mean_score, scores.std(),params)\n",
    "print 'Best parameters set found on development set:%r' % (grid.best_params_) \n",
    "print 'Best average accuracy in validation set: %.5f' %(grid.best_score_)\n",
    "print 'Accuracy in test set: %.5f' %(grid.score(X_test,y_test))\n",
    "y_pred=grid.predict(X_test)\n",
    "############################## manually grid search ###############################\n",
    "best_score, best_param = 0, None\n",
    "for p1 in [1,5,3]:\n",
    "    params=(p1)\n",
    "    try:\n",
    "        accuracy = ......\n",
    "        if accuracy > best_score:\n",
    "            best_score, best_param = accuracy, params\n",
    "    except:\n",
    "        continue\n",
    "print('Best parameters =%s accuracy=%.3f' % (best_param, best_score)) \n",
    "######################   cross validation  #########################################\n",
    "X = np.array([[1, 10], \n",
    "              [2, 20], \n",
    "              [3, 30], \n",
    "              [4, 40], \n",
    "              [5, 50], \n",
    "              [6, 60]])\n",
    "y = np.array([1,1,1,0,0,0])\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=3, random_state=0, shuffle=True)#shuffle the data before splitting into batches\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#('TRAIN:', array([0, 1, 3, 4], dtype=int64), 'TEST:', array([2, 5], dtype=int64))\n",
    "#('TRAIN:', array([0, 2, 4, 5], dtype=int64), 'TEST:', array([1, 3], dtype=int64))\n",
    "#('TRAIN:', array([1, 2, 3, 5], dtype=int64), 'TEST:', array([0, 4], dtype=int64))\n",
    "#************************************************************\n",
    "kfolds=KFold(n_splits=10, shuffle=True,random_state=0)\n",
    "trainindex=[]\n",
    "validindex=[]\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    trainindex.append(train_index)\n",
    "    validindex.append(valid_index)\n",
    "trainindex=np.array(trainindex)\n",
    "validindex=np.array(validindex)\n",
    "###########################\n",
    "foldnumber=0 #1, 2....9\n",
    "X_train, X_valid = X[trainindex[foldnumber]], X[validindex[foldnumber]]\n",
    "y_train, y_valid = y[trainindex[foldnumber]], y[validindex[foldnumber]]          \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import StratifiedKFold#preserving the percentage of samples for each class.\n",
    "kf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)#shuffle the data before splitting into batches\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#('TRAIN:', array([0, 1, 3, 4], dtype=int64), 'TEST:', array([2, 5], dtype=int64))\n",
    "#('TRAIN:', array([1, 2, 3, 5], dtype=int64), 'TEST:', array([0, 4], dtype=int64))\n",
    "#('TRAIN:', array([0, 2, 4, 5], dtype=int64), 'TEST:', array([1, 3], dtype=int64))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#('TRAIN:', array([1, 2, 3, 4, 5], dtype=int64), 'TEST:', array([0], dtype=int64))\n",
    "#('TRAIN:', array([0, 2, 3, 4, 5], dtype=int64), 'TEST:', array([1], dtype=int64))\n",
    "#('TRAIN:', array([0, 1, 3, 4, 5], dtype=int64), 'TEST:', array([2], dtype=int64))\n",
    "#('TRAIN:', array([0, 1, 2, 4, 5], dtype=int64), 'TEST:', array([3], dtype=int64))\n",
    "#('TRAIN:', array([0, 1, 2, 3, 5], dtype=int64), 'TEST:', array([4], dtype=int64))\n",
    "#('TRAIN:', array([0, 1, 2, 3, 4], dtype=int64), 'TEST:', array([5], dtype=int64))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "rs = ShuffleSplit(n_splits=3, test_size=.33333, random_state=0)\n",
    "for train_index, test_index in rs.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#('TRAIN:', array([1, 3, 0, 4], dtype=int64), 'TEST:', array([5, 2], dtype=int64))\n",
    "#('TRAIN:', array([4, 0, 2, 5], dtype=int64), 'TEST:', array([1, 3], dtype=int64))\n",
    "#('TRAIN:', array([1, 2, 4, 0], dtype=int64), 'TEST:', array([3, 5], dtype=int64))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=3, test_size=0.3333, random_state=0)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "#('TRAIN:', array([5, 2, 4, 0], dtype=int64), 'TEST:', array([3, 1], dtype=int64))\n",
    "#('TRAIN:', array([3, 1, 0, 4], dtype=int64), 'TEST:', array([5, 2], dtype=int64))\n",
    "#('TRAIN:', array([3, 1, 0, 4], dtype=int64), 'TEST:', array([5, 2], dtype=int64))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(clf, X, y, cv=3,scoring='accuracy',n_jobs=1)\n",
    "print scores #[ 0.5  1.   1. ]\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#Accuracy: 0.83 (+/- 0.47)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~ walk-forward validation  ~~~~~~~~~~~~~~~~~~~~\n",
    "#the training set is divided into the train set and validation set. \n",
    "#Each model is evaluated using walk-forward validation. \n",
    "#I use the last 50% of training set as the validation set.\n",
    "#The validation set is iterated to test and evaluate each model. \n",
    "#In the beginning, the first 50% of training set is used as the train set to train a model. \n",
    "#For each iteration in the validation set: \n",
    "#(1) A model is trained by the train set. \n",
    "#(2) A prediction made by the model is performed on the i-th data in the validation set. \n",
    "#A one-step prediction for one data will be stored for later evaluation. \n",
    "#(3) The feature variables and the actual target value from the i-th data in the validation set will be \n",
    "#added to the train set for the next iteration. \n",
    "#In the end, the predictions made during the iteration of the validation set will be evaluated in terms of classification accuracy. \n",
    "for c in [0.01,0.1,1]:\n",
    "    strain_size = int(len(x_train) * 0.50)\n",
    "    xtrain, xvalidation = x_train[0:-train_size], x_train[-train_size:]\n",
    "    ytrain, yvalidation = y_train.values[0:-train_size], y_train.values[-train_size:]\n",
    "    xhistory=xtrain.copy()\n",
    "    yhistory=ytrain.copy()\n",
    "    predictions = []\n",
    "    end=int(len(x_train) * 0.50)\n",
    "    for i in range(0,end,1): \n",
    "        estimator = SVC(C=1, random_state=0)\n",
    "        estimator.fit(xhistory, yhistory)       \n",
    "        y_pred=estimator.predict(xvalidation[i])\n",
    "        predictions.append(y_pred)\n",
    "        xhistory=np.vstack((xhistory,xvalidation[i]))\n",
    "        yhistory=np.r_[yhistory, yvalidation[i]]\n",
    "    accuracy=accuracy_score(y_true=yvalidation, y_pred=predictions) \n",
    "    print '{}: {}'.format('Accuracy ', accuracy) \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!    \n",
    "window=20 # 20 days\n",
    "\n",
    "train_size = (int(len(x_train) * 0.50)/window)*window\n",
    "xtrain, xvalidation = x_train[0:-train_size], x_train[-train_size:]\n",
    "train, yvalidation = y_train.values[0:-train_size], y_train.values[-train_size:]\n",
    "xhistory=xtrain.copy()\n",
    "yhistory=ytrain.copy()\n",
    "predictions = []\n",
    "end=((int(len(x_train) * 0.50)/window)-1)*window+1\n",
    "for i in range(0,end,window): \n",
    "    estimator = SVR(C=1, epsilon=epsi)\n",
    "    estimator.fit(xhistory, yhistory)       \n",
    "    y_pred=estimator.predict(xvalidation[i:window+i])\n",
    "    predictions.append(y_pred)\n",
    "    xhistory=np.vstack((xhistory,xvalidation[i:window+i]))\n",
    "    yhistory=np.r_[yhistory, yvalidation[i:window+i]]\n",
    "    #calculate the metrics for each validation fold\n",
    "    submae=mean_absolute_error(y_true=yvalidation[i:window+i], y_pred=y_pred)\n",
    "\n",
    "y_pred=np.ravel(predictions)\n",
    "mae=mean_absolute_error(y_true=yvalidation, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression and classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron#(n_iter, eta0, random_state)\n",
    "estimator = Perceptron(n_iter=100, random_state=0, eta0=0.002)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) #array([1, 0, 0])\n",
    "estimator.coef_ #array([[ 0.016, -0.01 , -0.028]]) Weights assigned to the features\n",
    "estimator.intercept_ #array([-0.01]) Constants in decision function\n",
    "accuracy=estimator.score(X_train, y_train) #0.66666666666666663\n",
    "########################################################################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "estimator = LogisticRegression(C=1.0,penalty='l2', random_state=0)#penalty : 'l1' or 'l2'\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "#array([[ 0.24547008,  0.75452992],\n",
    "#       [ 0.64501769,  0.35498231],\n",
    "#       [ 0.51242289,  0.48757711]])\n",
    "y_pred=estimator.predict(X_test) #array([1, 0, 0])\n",
    "estimator.coef_ #Weights assigned to the features\n",
    "estimator.intercept_ # Constants in decision function\n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "########################################################################################\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "estimator = SGDClassifier(penalty='l2', random_state=0,loss='log',l1_ratio=0.5,alpha=0.001)\n",
    "#penalty : 'l1', 'l2', or 'elasticnet' #alpha: Constant that multiplies the regularization term\n",
    "estimator.fit(X_train, y_train)\n",
    "estimator.partial_fit(X_train, y_train, classes=np.array([0,1])) #Fit linear model with Stochastic Gradient Descent.\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "estimator.coef_ #Weights assigned to the features\n",
    "estimator.intercept_ # Constants in decision function\n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "########################################################################################\n",
    "from sklearn.svm import SVC #support vector classification\n",
    "estimator = SVC(kernel='rbf', gamma=0.1,C=10, probability=True, random_state=0)\n",
    "estimator = SVC(kernel='linear', C=10, probability=True, random_state=0)\n",
    "estimator = SVC(kernel='poly', gamma=0.1,degree=3,C=10, probability=True, random_state=0)\n",
    "#C: Penalty parameter C of the error term\n",
    "#gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.svm import NuSVC #Nu-Support Vector Classification\n",
    "estimator = NuSVC(nu=0.5, kernel='rbf', gamma=0.1, probability=True, random_state=0)\n",
    "estimator = NuSVC(nu=0.5, kernel='linear', probability=True, random_state=0)\n",
    "estimator = NuSVC(nu=0.5, kernel='poly', gamma=0.1,degree=3, probability=True, random_state=0)\n",
    "#gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "#nu:(0, 1], An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "#estimator.coef_ #Weights assigned to the features #coef_ is only available when using a linear kernel\n",
    "estimator.intercept_ # Constants in decision function\n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "########################################################################################\n",
    "from sklearn.svm import LinearSVC#Linear Support Vector Classification.\n",
    "estimator = LinearSVC(penalty='l2', C=1.0, random_state=0) \n",
    "#C: Penalty parameter C of the error term # penalty : 'l1', 'l2'\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "########################################################################################\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis #All the classes have the same covariance matrix\n",
    "estimator = LinearDiscriminantAnalysis()\n",
    "estimator.fit(X_train, y_train)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#All the classes don't the same covariance matrix, fit better than LDA\n",
    "estimator = QuadraticDiscriminantAnalysis()\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_ #array([0, 1]) Unique class labels\n",
    "########################################################################################\n",
    "from sklearn.naive_bayes import GaussianNB #Gaussian Naive Bayes \n",
    "#suppose the probability of a feature belong to a class has a normal distribution\n",
    "estimator = GaussianNB()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.naive_bayes import BernoulliNB #for each feature has a binary value\n",
    "estimator = BernoulliNB(alpha =1.0)\n",
    "#alpha : smoothing parameter-->laplace:1, Lidstone:0.01,0.05,0.1,0.5, nonmmothing:0\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.naive_bayes import MultinomialNB #for text classification\n",
    "#ex:TfidfVectorizer + MultinomialNB\n",
    "estimator = MultinomialNB(alpha =1.0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_ #array([0, 1]) Unique class labels\n",
    "########################################################################################\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "estimator = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0) , n_restarts_optimizer=10, n_jobs =-1, random_state=0)\n",
    "#n_restarts_optimizer:The number of restarts of the optimizer for finding the kernel’s parameters which maximize \n",
    "#the log-marginal likelihood.\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_\n",
    "\n",
    "########################################################################################\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "estimator = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski', n_jobs=-1)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_\n",
    "\n",
    "########################################################################################\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = GaussianNB()\n",
    "estimator = VotingClassifier(estimators=[('lr', clf1), ('gnb', clf2)], voting='hard')\n",
    "estimator = VotingClassifier(estimators=[('lr', clf1), ('gnb', clf2)], voting='soft', weights=[2,1])\n",
    "#voting='hard', uses predicted class labels for majority rule voting. \n",
    "#voting='soft', predicts the class label based on the argmax of the sums of the predicted probabilities\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "########################################################################################\n",
    "X_train=np.array([[ 5.1,  3.5,  1.4,  0.2],\n",
    "            [ 4.9,  3. ,  1.4,  0.2],\n",
    "            [ 4.7,  3.2,  1.3,  0.2],\n",
    "            [ 5.4,  3.7,  1.5,  0.2],\n",
    "            [ 6.4,  3.2,  4.5,  1.5],\n",
    "            [ 6.9,  3.1,  4.9,  1.5]])\n",
    "y_train=np.array([ 0,  0,  0, -1, 1, 1])\n",
    "from sklearn.semi_supervised import LabelPropagation #Label Propagation classifier\n",
    "estimator = LabelPropagation(kernel='rbf',gamma=20)\n",
    "estimator = LabelPropagation(kernel='knn',n_neighbors=3)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.semi_supervised import LabelSpreading #has regulation\n",
    "estimator = LabelSpreading(kernel='rbf',gamma=20)\n",
    "estimator = LabelSpreading(kernel='knn',n_neighbors=3)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_train)\n",
    "y_pred=estimator.predict(X_train) #array([0, 0, 0, 0, 1, 1])\n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_ #array([0, 1]) Unique class labels\n",
    "########################################################################################\n",
    "## plot decision region\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "# Training a classifier\n",
    "svm = SVC(C=0.5, kernel='linear')\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "plot_decision_regions(X, y, clf=svm, res=0.02, legend=2)\n",
    "# Adding axes annotations\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.title('SVM on Iris')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "estimator = LinearRegression(n_jobs=-1)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "########################################################################################\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "estimator = SGDRegressor(penalty='l2', random_state=0,loss='squared_loss',l1_ratio=0.5,alpha=0.001)\n",
    "#penalty : 'l1', 'l2', or 'elasticnet' #alpha: Constant that multiplies the regularization term\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared=estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features\n",
    "estimator.intercept_ # Constants in decision function\n",
    "########################################################################################\n",
    "from sklearn.linear_model import RANSACRegressor #RANSAC (RANdom SAmple Consensus) algorithm.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "base_estimator = LinearRegression()\n",
    "estimator = RANSACRegressor(base_estimator, min_samples=2,residual_threshold=2, random_state=0,\n",
    "                                   max_trials=100,loss='absolute_loss')\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.inlier_mask_ #array([ True,  True,  True,  True,  True,  True,  True], dtype=bool) # inliers classified as True.\n",
    "estimator.estimator_.coef_ #Best fitted model Weights assigned to the features  \n",
    "estimator.estimator_.intercept_\n",
    "########################################################################################\n",
    "from sklearn.linear_model import Ridge #Linear least squares with l2 regularization\n",
    "estimator = Ridge(alpha=1.0, random_state=0) #alpha: Regularization strength\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.linear_model import Lasso #Linear Model trained with L1 prior as regularizer\n",
    "estimator = Lasso(alpha=0.1, random_state=0) #alpha: Regularization strength\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.linear_model import ElasticNet #Linear regression with combined L1 and L2 priors as regularizer\n",
    "estimator = ElasticNet(alpha=1, l1_ratio=0.5, random_state=0) #alpha: Regularization strength\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Lasso Path : plot various features' weights along the alpha values\n",
    "from sklearn.linear_model import lasso_path\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train, y_train, eps = 5e-3)\n",
    "#eps = Length of the path. eps=5e-3 means that alpha_min / alpha_max = 5e-3\n",
    "neg_log_alphas_lasso = -np.log10(alphas_lasso)\n",
    "for coef_l in coefs_lasso:\n",
    "    plt.plot(neg_log_alphas_lasso, coef_l)\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('coefficients')\n",
    "plt.axis('tight')\n",
    "########################################################################################\n",
    "from sklearn.linear_model import RidgeCV #Ridge linear model with iterative fitting along a regularization path\n",
    "estimator = RidgeCV(alphas=(0.1, 1.0, 10.0), store_cv_values=True, cv=None) \n",
    "#cv =None Leave-One-Out cross-validation, cv=integer, to specify the number of folds.\n",
    "\n",
    "estimator.cv_values_ #Cross-validation values for each alpha \n",
    "estimator.cv_values_.mean(axis=0).argmin() #show the best alpha index\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.linear_model import LassoCV #Lasso linear model with iterative fitting along a regularization path \n",
    "estimator = LassoCV(alphas=(0.1, 1.0, 10.0), cv=None, n_jobs=-1,random_state=0) \n",
    "#cv =None Leave-One-Out cross-validation, cv=integer, to specify the number of folds.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# for feature number > sample number,  ElasticNetCV can fit better than LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV #Elastic Net model with iterative fitting along a regularization path\n",
    "estimator = ElasticNetCV(alphas=(0.1, 1.0, 10.0), l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=None, n_jobs=-1, random_state=0) \n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "estimator.alpha_  #Estimated regularization parameter.\n",
    "########################################################################################\n",
    "from sklearn.linear_model import BayesianRidge #Bayesian ridge regression\n",
    "estimator = BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06) \n",
    "#lambda_1, lambda_2 are the shape parameters and inverse scale parameter for feature wight#alpha_1, alpha_2 are for noise\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.lambda_ # estimated precision of the weights.\n",
    "estimator.alpha_  #estimated precision of the noise\n",
    "########################################################################################\n",
    "from sklearn.linear_model import Lars #Least Angle Regression model # well suitable for feature number > sanple number\n",
    "estimator = Lars(n_nonzero_coefs=2) #n_nonzero_coefs :the number of non_zero weights for features\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.linear_model import LarsCV #Cross-validated Least Angle Regression model\n",
    "estimator = LarsCV(cv=None, n_jobs=-1)\n",
    "#cv: None, the default 3-fold cross-validation / cv:integer, to specify the number of folds.\n",
    "estimator.alpha_ #the estimated regularization parameter alpha\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features \n",
    "estimator.intercept_ # Constants in decision function\n",
    "########################################################################################\n",
    "from sklearn.svm import SVR #Support Vector Regression\n",
    "estimator = SVR(kernel='rbf', gamma=0.1,C=10,epsilon=0.2)\n",
    "estimator = SVR(kernel='linear', C=10,epsilon=0.2)\n",
    "estimator = SVR(kernel='poly', gamma=0.1,degree=3,C=10,epsilon=0.2)\n",
    "#C: Penalty parameter C of the error term\n",
    "#gamma: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "#epsilon: It specifies the epsilon-tube within which no penalty is associated in the training loss function \n",
    "#with points predicted within a distance epsilon from the actual value.\n",
    "\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.coef_ #Weights assigned to the features  #coef_ is only available when using a linear kernel\n",
    "estimator.intercept_ # Constants in decision function\n",
    "########################################################################################\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "estimator = GaussianProcessRegressor(kernel=1.0 * RBF(length_scale=1.0), alpha =1e-2 , n_restarts_optimizer=10, random_state=0)\n",
    "estimator = GaussianProcessRegressor(alpha =1e-2 , n_restarts_optimizer=10, random_state=0)\n",
    "#alpha : Value added to the diagonal of the kernel matrix during fitting. \n",
    "#Larger values correspond to increased noise level in the observations\n",
    "#n_restarts_optimizer:The number of restarts of the optimizer for finding the kernel’s parameters which maximize \n",
    "#the log-marginal likelihood.\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "########################################################################################\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "estimator = KNeighborsRegressor(n_neighbors=5, p=2, metric='minkowski', n_jobs=-1)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "########################################################################################\n",
    "## plot residual plot\n",
    "plt.scatter(y_pred, y_pred-y_test)\n",
    "plt.hlines(y=0, xmin=X_test.min(), xmax=X_test.max(), lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#can support multi-class classification without one versus all techniques, # can be used for nonlinear data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "estimator = DecisionTreeClassifier(criterion='entropy',max_depth=10,min_samples_split=2, \n",
    "                                   min_samples_leaf=1, max_features='auto', random_state=0)\n",
    "#max_features : The number of features to consider when looking for the best \n",
    "#If int, then consider max_features features at each split.\n",
    "#If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "#If “auto”, then max_features=sqrt(n_features).\n",
    "#If “sqrt”, then max_features=sqrt(n_features).\n",
    "#If “log2”, then max_features=log2(n_features).\n",
    "#If None, then max_features=n_features.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier(n_estimators=10, criterion='entropy',max_depth=10,min_samples_split=2, \n",
    "                                   min_samples_leaf=1, max_features='auto',random_state=0, n_jobs=-1)\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#randomly select some features and threshold for each split, grow all dataset\n",
    "estimator = ExtraTreesClassifier(criterion='entropy',max_depth=10,min_samples_split=2, \n",
    "                                   min_samples_leaf=1, max_features='auto',random_state=0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_estimator = DecisionTreeClassifier(criterion='entropy',max_depth=None, random_state=0)\n",
    "estimator = BaggingClassifier(base_estimator=base_estimator, n_estimators=100, max_samples=0.8, max_features=0.8, \n",
    "                              bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=0)\n",
    "#max_samples : int, then draw max_samples samples; float, then draw max_samples * X.shape[0] samples.\n",
    "#max_features : int, then draw max_features features; float, then draw max_features * X.shape[1] features.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_estimator = DecisionTreeClassifier(criterion='entropy',max_depth=1, random_state=0)\n",
    "estimator = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "estimator = GradientBoostingClassifier(n_estimators=10, max_depth=3, subsample=1.0, min_samples_split=2, \n",
    "                                       min_samples_leaf=1,learning_rate=0.1, max_features='auto', random_state=0)\n",
    "#subsample : The fraction of samples to be used for fitting the individual base learners. \n",
    "#max_features : int, float, 'auto', 'sqrt', 'log2', None\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.classes_\n",
    "estimator.feature_importances_ #The higher, the more important the feature #BaggingClassifier doesn't have\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "estimator = DecisionTreeRegressor(criterion='mse',max_depth=10,min_samples_split=2, \n",
    "                                  min_samples_leaf=1, max_features='auto', random_state=0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "estimator = RandomForestRegressor(n_estimators=10, criterion='mse',max_depth=10,min_samples_split=2, \n",
    "                                  min_samples_leaf=1, max_features='auto', random_state=0, n_jobs=-1)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "#randomly select some features and threshold for each split, grow all dataset\n",
    "estimator = ExtraTreesRegressor(criterion='mse',max_depth=10,min_samples_split=2, \n",
    "                                  min_samples_leaf=1, max_features='auto', random_state=0)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_estimator = DecisionTreeRegressor(criterion='mse',max_depth=None, random_state=0)\n",
    "estimator = BaggingRegressor(base_estimator=base_estimator, n_estimators=100, max_samples=0.8, max_features=0.8, \n",
    "                              bootstrap=True, bootstrap_features=False, n_jobs=-1, random_state=0)\n",
    "#max_samples : int, then draw max_samples samples; float, then draw max_samples * X.shape[0] samples.\n",
    "#max_features : int, then draw max_features features; float, then draw max_features * X.shape[1] features.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "base_estimator = DecisionTreeRegressor(criterion='mse',max_depth=1, random_state=0)\n",
    "estimator = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=10, learning_rate=0.1, random_state=0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "estimator = GradientBoostingRegressor(n_estimators=10, max_depth=3, subsample=1.0, min_samples_split=2, \n",
    "                                       min_samples_leaf=1,learning_rate=0.1, max_features='auto', random_state=0)\n",
    "#subsample : The fraction of samples to be used for fitting the individual base learners. \n",
    "#max_features : int, float, 'auto', 'sqrt', 'log2', None\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.feature_importances_ #The higher, the more important the feature #BaggingRegressor doesn't have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################xgboost  parameters ###############################################\n",
    "silent [default=0]: #print running messages #1 silent\n",
    "eta [default=0.3] #Analogous to learning rate , Makes the model more robust by shrinking the weights on each step\n",
    "#Typical final values to be used: 0.01-0.2\n",
    "min_child_weight [default=1] #the minimum sum of weights of all observations required in a child\n",
    "#Used to control over-fitting. Higher values prevent a model from learning relations which might be \n",
    "#highly specific to the particular sample selected for a tree.\n",
    "#Too high values can lead to under-fitting \n",
    "max_depth [default=6] #maximum depth of a tree #Typical values: 3-10\n",
    "max_leaf_nodes #maximum number of terminal nodes or leaves in a tree\n",
    "gamma [default=0] #A node is split only when the resulting split gives a positive reduction \n",
    "#in the loss function. Gamma specifies the minimum loss reduction required to make a split\n",
    "subsample [default=1] #the fraction of observations to be randomly samples for each tree.\n",
    "colsample_bytree [default=1]#the fraction of columns to be randomly samples for each tree.#Typical values: 0.5-1\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "from xgboost import XGBClassifier\n",
    "estimator = XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, subsample=1,max_depth=3,\n",
    "                         min_child_weight=1, colsample_bytree=0.8, base_score=0.005, seed =0)\n",
    "#subsample : Subsample ratio of the training instance.\n",
    "#colsample_bytree : Subsample ratio of columns when constructing each tree\n",
    "#base_score: The initial prediction score of all instances, global bias\n",
    "estimator.fit(X_train, y_train)\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "y_pred=estimator.predict(X_test) \n",
    "accuracy=estimator.score(X_train, y_train)\n",
    "estimator.feature_importances_ #The higher, the more important the feature #BaggingRegressor doesn't have\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from xgboost import XGBRegressor\n",
    "estimator = XGBRegressor(objective='reg:linear', n_estimators=100, learning_rate=0.1, subsample=1,max_depth=3,\n",
    "                         min_child_weight=1, colsample_bytree=0.8, base_score=0.005, seed =0)\n",
    "estimator.fit(X_train, y_train)\n",
    "y_pred=estimator.predict(X_test) \n",
    "R_squared =estimator.score(X_test, y_test)\n",
    "estimator.feature_importances_ #The higher, the more important the feature #BaggingRegressor doesn't have\n",
    "sorted(zip(X_train.columns, estimator.feature_importances_ ), key=lambda x:x[1])[::-1]\n",
    "########################################################################################\n",
    "from xgboost import DMatrix, cv, train, plot_importance\n",
    "dtrain = DMatrix(X_train, label=y_train, feature_names=['a','b','c'])\n",
    "dtest = DMatrix(X_test, feature_names=['a','b','c'])\n",
    "### parameter tuning\n",
    "param_grid = [{\n",
    "        'eta': [0.03],\n",
    "        'max_depth': [5],\n",
    "        'min_child_weight': [1],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [0.7],\n",
    "        'objective': ['reg:linear'], #'binary:logistic' for classifier\n",
    "        'eval_metric': ['rmse'],     # ['error'] for classifier\n",
    "        'gamma': [0],\n",
    "        'silent':[1]}]\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "best_valid_score=10000000\n",
    "for params in ParameterGrid(param_grid):\n",
    "    #print 'max_depth={}, min_child_weight={}'.format(params['max_depth'],params['min_child_weight'])\n",
    "    #print 'subsample={}, colsample_bytree={}'.format(params['subsample'],params['colsample_bytree'])\n",
    "    #print 'Gamma={}'.format(params['gamma'])\n",
    "    #print 'eta={}'.format(params['eta'])\n",
    "    model = cv(params, dtrain, num_boost_round=2000, nfold=4, early_stopping_rounds=100)\n",
    "    #num_boost_round : Number of boosting iterations\n",
    "    #stop, when the score didn't improve for more than early_stopping_rounds\n",
    "    #print 'The best rmse in the train set :{}'.format(model['train-rmse-mean'].values[-1])\n",
    "    #print 'The best rmse in the test set :{}'.format(model['test-rmse-mean'].values[-1])\n",
    "    diff=-model['train-rmse-mean'].values[-1]+model['test-rmse-mean'].values[-1]\n",
    "    valid_score=model['test-rmse-mean'].values[-1]\n",
    "    print 'diff = {} | valid_rmse= {}'.format(diff,valid_score)\n",
    "    if valid_score <= best_valid_score:\n",
    "        best_valid_score=valid_score\n",
    "        best_params=params\n",
    "        print 'Best valid_rmse= {}'.format(valid_score)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \n",
    "# After finding the best parameter\n",
    "params = {\n",
    "        'eta': 0.03,\n",
    "        'max_depth': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 1,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'objective': 'reg:linear', #'binary:logistic' for classifier\n",
    "        'eval_metric': 'rmse',     # ['error'] for classifier\n",
    "        'silent': 1}\n",
    "\n",
    "model = cv(params, dtrain, num_boost_round=2000, early_stopping_rounds=100,verbose_eval=100, show_stdv=False)\n",
    "num_boost_rounds = len(model)\n",
    "print 'Best iteration : {}'.format(num_boost_rounds)\n",
    "### plot \"test-rmse-mean\", \"train-rmse-mean\" vs. iterations\n",
    "model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n",
    "\n",
    "model = train(dict(params, silent=0), dtrain, num_boost_round= num_boost_rounds)\n",
    "### plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()\n",
    "### Prediction\n",
    "y_pred = model.predict(dtest)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "dtrain = DMatrix(X_train, y_train, feature_names=['a','b','c'])\n",
    "dvalid = DMatrix(X_valid, y_valid, feature_names=['a','b','c'])\n",
    "dtest = DMatrix(X_test, feature_names=['a','b','c'])\n",
    "params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "model = train(params, dtrain, num_boost_round=1000,\n",
    "                                     evals=[(dtrain, 'train'), (dvalid, 'valid')],\n",
    "                                     early_stopping_rounds=100,\n",
    "                                     verbose_eval=100)\n",
    "\n",
    "num_boost_round =model.best_iteration\n",
    "print 'Best iteration = {}'.format(model.best_iteration)\n",
    "print 'validation rmse = {}'.format(model.best_score)\n",
    "### calculate F scores for all features\n",
    "features=model.get_score(importance_type='weight') \n",
    "A=sorted([(k, features[k]) for k in features], key=lambda x: x[1], reverse=True)\n",
    "#[('b', 395),\n",
    "# ('c', 168),\n",
    "# ('a', 164)]\n",
    "df_features=DataFrame({'features':zip(*A)[0], 'score': zip(*A)[1]}) \n",
    "### Plot feature number vs. importance ratio\n",
    "import_sum=[]\n",
    "import_counts=[]\n",
    "for i in range(2544):#2544 is the largest F score\n",
    "    importance_sum=np.sum([f[1] for f in A if f[1]>i])\n",
    "    importance_counts=len([f for f in A if f[1]>i])\n",
    "    import_sum.append(importance_sum)\n",
    "    import_counts.append(importance_counts)\n",
    "import_sum=np.array(import_sum,dtype='float')/import_sum[0]\n",
    "plt.plot(import_counts, import_sum)\n",
    "### Prediction\n",
    "model = train(params, dtrain, num_boost_round=num_boost_round, verbose_eval=100)\n",
    "y_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning models for classification and regression (useful for image recognization or non_linear data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feed forward neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import h5py\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# need to Convert labels(Y) to categorical one-hot encoding for multi-class classification\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], init = 'normal', activation='relu'))\n",
    "#init='uniform',he_normal,he_uniform\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(5, init = 'normal', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, init = 'normal', activation='sigmoid')) # for binary classification\n",
    "#model.add(Dense(3, init='normal', activation = 'softmax')) # for probabilistic multiple-class classification\n",
    "#model.add(Dense(1, init = 'normal', activation='linear')) # for regression\n",
    "#activation='softmax','relu','tanh','sigmoid','linear'\n",
    "\n",
    "#from sklearn.utils import class_weight\n",
    "#class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizer='adamax'#'adadelta','adam','rmsprop','adagrad', sgd\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizer,metrics=['accuracy']) #for binary classification\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer = optimizer,metrics=['accuracy']) #for multi-class classification\n",
    "#model.compile(loss='categorical_crossentropy',optimizer = optimizer,metrics=['accuracy']) #for multi-class classification\n",
    "#model.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics=['mse'])  # for regression\n",
    "#loss = 'categorical_crossentropy','sparse_categorical_crossentropy' for multi-class classification\n",
    "#if use categorical_crossentropy, when you have 10 classes, the target for each sample should be a 10-dimensional vector \n",
    "#loss = 'mean_squared_error', 'mean_absolute_error' \n",
    "#loss = 'binary_crossentropy' ,  Also known as logloss. \n",
    "\n",
    "#def root_mean_squared_error(y_true, y_pred):\n",
    "#    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "#model.compile(loss = root_mean_squared_error, optimizer = optimizer) # for regression\n",
    "\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')]\n",
    "\n",
    "testnumber=1\n",
    "filepath=\"model_{}.best.hdf5\".format(testnumber)\n",
    "saveBestModel = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "#saveBestModel = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "model.fit(X_train, y_train, batch_size=500, nb_epoch=100, \n",
    "          validation_data=(X_valid, y_valid),\n",
    "          #validation_split=0.1,\n",
    "          #class_weight=class_weight,\n",
    "          callbacks=[saveBestModel],\n",
    "          #callbacks=callbacks,\n",
    "          shuffle=True, verbose=1) \n",
    "#train the network for nb_epoch epochs, with batch_size samples per mini-batch.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "###### model evaluation and prediction\n",
    "model.load_weights(filepath)\n",
    "#for classification\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics=['accuracy']) \n",
    "scores_valid = model.evaluate(X_valid, y_valid, batch_size=128, verbose=0)\n",
    "print \"{} in the validation set: {}\".format(model.metrics_names[0], scores_valid[0])\n",
    "print \"{} in the validation set: {}\".format(model.metrics_names[1], scores_valid[1])\n",
    "\n",
    "y_prob = model.predict_proba(X_test, batch_size=500, verbose=0)\n",
    "y_pred=np.where(y_prob[:, 0]>i, 1,0)\n",
    "#~~~~~~~~~~~~\n",
    "y_pred = model.predict_classes(X_test, batch_size=500, verbose=0)\n",
    "#~~~~~~~~~~~~\n",
    "y_pred = model.predict(X_test, batch_size=500, verbose=0)\n",
    "y_pred = np.where(y_pred>0.5, 1, 0)\n",
    "#~~~~~~~~~~~~\n",
    "#for regression\n",
    "model.compile(loss = 'mean_squared_error', optimizer = optimizer, metrics=['mse'])\n",
    "scores_valid = model.evaluate(X_valid, y_valid, batch_size=128, verbose=0)\n",
    "print(\"MSE in the validation set: %.6f\" % (scores_valid))\n",
    "print(\"RMSE in the validation set: %.6f\" % (np.sqrt(scores_valid)))\n",
    "y_pred = model.predict(X_test, batch_size=10, verbose=0) \n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #model.compile(loss='mean_squared_error', optimizer='adam') for KerasRegressor\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "#model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "model.fit(X, Y)\n",
    "\n",
    "#use grid_search to search for the best batch_size or epochs as well as the model parameters.\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne.layers import InputLayer, DenseLayer\n",
    "from lasagne.nonlinearities import softmax, tanh, sigmoid\n",
    "from lasagne.updates import nesterov_momentum\n",
    "# three layers: one hidden layer\n",
    "model = NeuralNet(layers=[('input', InputLayer),('hidden', DenseLayer),('output', DenseLayer)],\n",
    "                 # layer parameters:\n",
    "                 input_shape=(None, X_train.shape[1]), \n",
    "                 hidden_num_units=10,  # number of units in hidden layer\n",
    "                 output_nonlinearity=softmax,  # output layer uses identity function\n",
    "                 output_num_units=2, #number of possible outputs (for binary classification this would be 2) \n",
    "                 # optimization method:\n",
    "                 update=nesterov_momentum,\n",
    "                 update_learning_rate=0.01, \n",
    "                 update_momentum=0.9, \n",
    "\n",
    "                 regression=False,  # If you're doing classification you want this off\n",
    "                 max_epochs=400,  # more epochs can be good, \n",
    "                 verbose=1, # enabled so that you see meaningful output when the program runs\n",
    "                )\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ex: X_train shape =(1000, 3, 32, 32) for 1000 images\n",
    "#convolutional neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D \n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop, SGD, RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "import h5py\n",
    "# for color image classification\n",
    "model=Sequential()\n",
    "model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='same',input_shape=(3,32,32), activation='relu'))\n",
    "#Each convolutional layer has a filter size of 3 x 3 and a rectifier activation function\n",
    "model.add(Convolution2D(nb_filter=32, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "#a max pool layer with a window size of 2×2.\n",
    "\n",
    "model.add(Convolution2D(nb_filter=16, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "model.add(Convolution2D(nb_filter=16, nb_row=3, nb_col=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, init='he_normal', activation = 'tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(3, init='he_normal', activation = 'softmax')) # for 3-class probability classification\n",
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adamax',#optimizer='adadelta', 'adamax','rmsprop','adam',sgd\n",
    "              metrics=['accuracy'])\n",
    "#print(model.summary())\n",
    "datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,\n",
    "                             zoom_range=0.2, horizontal_flip=True, fill_mode='nearest')  \n",
    "##apply image augmentations to artificially increase the size of the training set with new transformed images. \n",
    "##A number of random transformations are applied to the initial data by zooming, rotating, shifting or shearing \n",
    "##images to prevent overfitting and improving the model generalization error\n",
    "\n",
    "testnumber=1\n",
    "filepath=\"weights32_{}.best.hdf5\".format(testnumber)\n",
    "saveBestModel = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')  \n",
    "     \n",
    "model.fit_generator(datagen.flow(X_train, y_train, batch_size=500, shuffle=True), \n",
    "                    nb_epoch=nb_epoch, samples_per_epoch=len(X_train), verbose=1, \n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[saveBestModel])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "model.load_weights(filepath)\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adamax',metrics=['accuracy'])\n",
    "###### model evaluation\n",
    "scores_valid = model.evaluate(X_valid, y_valid, batch_size=128, verbose=0)\n",
    "print \"{} in the validation set: {}\".format(model.metrics_names[0], scores_valid[0])\n",
    "print \"{} in the validation set: {}\".format(model.metrics_names[1], scores_valid[1]) \n",
    "#print(\"Best %s: %.6f\" % (model.metrics_names[0], scores_valid[0])) # loss\n",
    "#print(\"Best %s: %.2f%%\" % (model.metrics_names[1], scores_valid[1]*100))  # accuracy  \n",
    "###### predication\n",
    "y_pred = model.predict_proba(x_test)\n",
    "##########################################################################################\n",
    "##########################################################################################\n",
    "from lasagne import layers\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.nonlinearities import softmax, tanh, sigmoid, rectify\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "# make sure data is in right format, & properly label-encoded\n",
    "SIZE=32\n",
    "X_train = X_train.reshape(-1, 1, SIZE, SIZE)\n",
    "X_test = X_test.reshape(-1, 1, SIZE, SIZE)\n",
    "\n",
    "model = NeuralNet(layers=[('input', layers.InputLayer),\n",
    "                          ('conv1', layers.Conv2DLayer),\n",
    "                          ('pool1', layers.MaxPool2DLayer),\n",
    "                          ('conv2', layers.Conv2DLayer),\n",
    "                          ('pool2', layers.MaxPool2DLayer),\n",
    "                          ('conv3', layers.Conv2DLayer),\n",
    "                          ('pool3', layers.MaxPool2DLayer),\n",
    "                          ('hidden4', layers.DenseLayer),\n",
    "                          ('hidden5', layers.DenseLayer),\n",
    "                          ('output', layers.DenseLayer)],\n",
    "                  input_shape=(None, 1, SIZE, SIZE),\n",
    "                  conv1_num_filters=32, conv1_filter_size=(3, 3), conv1_nonlinearity=rectify, pool1_pool_size=(2, 2), \n",
    "                  conv2_num_filters=64, conv2_filter_size=(3, 3), conv2_nonlinearity=rectify, pool2_pool_size=(2, 2), \n",
    "                  conv3_num_filters=128, conv3_filter_size=(3, 3), conv3_nonlinearity=rectify, pool3_pool_size=(2, 2), \n",
    "                  hidden4_num_units=1000,\n",
    "                  hidden5_num_units=1000, \n",
    "                  output_num_units=2, #number of possible outputs (for binary classification this would be 2) \n",
    "                  output_nonlinearity=softmax,\n",
    "                  \n",
    "                  update_learning_rate=0.01,\n",
    "                  update_momentum=0.9,\n",
    "\n",
    "                  regression=False,\n",
    "                  max_epochs=20,\n",
    "                  verbose=1,\n",
    "                 )\n",
    "model.fit(X_train, Y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use theano to build a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.tensor import fscalar\n",
    "from theano.tensor import fmatrix\n",
    "from theano.tensor import fvector\n",
    "from theano import shared\n",
    "from theano import function\n",
    "\n",
    "X_train = np.asarray([[0.0], [1.0], [2.0], [3.0], [4.0],\n",
    "                      [5.0], [6.0], [7.0], [8.0], [9.0]], dtype=np.float32)\n",
    "\n",
    "y_train = np.asarray([1.0, 1.3, 3.1, 2.0, 5.0, 6.3, 6.6, 7.4, 8.0, 9.0],dtype=np.float32)\n",
    "\n",
    "eta=0.001\n",
    "epochs=10\n",
    "# Initialize arrays\n",
    "eta0 = fscalar('eta0')\n",
    "y = fvector(name='y')\n",
    "X = fmatrix(name='X')\n",
    "w = theano.shared(np.zeros(shape=(X_train.shape[1] + 1), dtype=np.float32), name='w')\n",
    "\n",
    "# calculate cost function\n",
    "net_input = T.dot(X, w[1:]) + w[0]\n",
    "errors = y - net_input\n",
    "cost = T.sum(T.pow(errors, 2))\n",
    "# perform gradient update\n",
    "gradient = T.grad(cost, wrt=w)\n",
    "update = [(w, w - eta0 * gradient)]\n",
    "# compile model\n",
    "train = function(inputs=[eta0],\n",
    "                 outputs=cost,\n",
    "                 updates=update,\n",
    "                 allow_input_downcast=True,\n",
    "                 givens={X: X_train,y: y_train})\n",
    "\n",
    "costs=[]\n",
    "for _ in range(epochs):\n",
    "           costs.append(train(eta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "estimator=GaussianMixture(n_components=2, covariance_type='tied', n_init=5, init_params='kmeans', random_state=0)\n",
    "#n_components:The number of mixture components.\n",
    "#covariance_type:'full' (each component has its own general covariance matrix),\n",
    "#'tied' (all components share the same general covariance matrix),\n",
    "#'diag' (each component has its own diagonal covariance matrix),\n",
    "#'spherical' (each component has its own single variance).\n",
    "#n_init : The number of initializations to perform. The best results are kept.\n",
    "#init_params : {‘kmeans’, ‘random’}, defaults to ‘kmeans’. used to initialize the weights\n",
    "estimator.fit(X_train)\n",
    "y_pred=estimator.predict(X_test)\n",
    "y_pred=estimator.predict_proba(X_test)\n",
    "estimator.means_ # show all cluster centers, shape=(cluster num, feature num)\n",
    "###########################################################################################\n",
    "from sklearn.cluster import KMeans\n",
    "estimator=KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, random_state=0, n_jobs=-1)\n",
    "#init :‘k-means++’, ‘random’ \n",
    "#n_init :Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "#max_iter : Maximum number of iterations of the k-means algorithm for a single run.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "#use the KMeans algorithm by Mini-batches which are subsets of the input data, randomly sampled in each training iteration.  \n",
    "estimator=MiniBatchKMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, batch_size=100, random_state=0)\n",
    "#batch_size : Size of the mini batches\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train)\n",
    "estimator.fit_transform(X_train) #Transform data to a cluster-distance space.\n",
    "#array([[  1.39754249,  10.67707825],\n",
    "#       [  1.39754249,  10.67707825],\n",
    "#       [  1.39754249,  10.67707825],\n",
    "#shape=(samplenumber, clusternumber), each value is the distance between a sample and a cluster centroid\n",
    "y_pred=estimator.predict(X_test)\n",
    "estimator.cluster_centers_  # show all cluster centers, shape=(cluster num, feature num)\n",
    "estimator.labels_ #Labels of each point\n",
    "estimator.inertia_ #Sum of distances of samples to their closest cluster center. It is SSE value\n",
    "###########################################################################################\n",
    "from sklearn.cluster import AgglomerativeClustering  \n",
    "estimator=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\n",
    "#linkage : {“ward”, “complete”, “average”}, \n",
    "#ward minimizes the variance of the clusters being merged.\n",
    "#average uses the average of the distances of each observation of the two sets.\n",
    "#complete uses the maximum distances between all observations of the two sets.\n",
    "estimator.fit(X_train)\n",
    "y_pred=estimator.fit_predict(X_train) #Performs clustering on X and returns cluster labels\n",
    "estimator.labels_ #Labels of each point\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "### Agglomerative clustering using the complete linkage algorim\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage \n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import maxdists\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "X = np.array([[0,0], [3,4], [60,80], [66,88]]) \n",
    "pdst=pdist(X, 'euclidean')  #array([   5.,  100.,  110.,   95.,  105.,   10.])\n",
    "s=squareform(pdst)\n",
    "#array([[   0.,    5.,  100.,  110.],\n",
    "#       [   5.,    0.,   95.,  105.],\n",
    "#       [ 100.,   95.,    0.,   10.],\n",
    "#       [ 110.,  105.,   10.,    0.]])\n",
    "####find similar samples \n",
    "sample0_similar=s[0].argsort() #array([0, 1, 2, 3], dtype=int64)\n",
    "X[sample0_similar[:3]] # find 3 most similar samples for sample0\n",
    "#array([[ 0,  0],\n",
    "#       [ 3,  4],\n",
    "#       [60, 80]])\n",
    "\n",
    "lin=linkage (pdst, method='complete') #method='ward' ward hierarchical clustering\n",
    "###### Draw dendrogram\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(lin,\n",
    "           leaf_rotation=90.,  # rotates the x axis labels\n",
    "           leaf_font_size=12.,  # font size for the x axis labels\n",
    "           orientation='left')\n",
    "plt.show()\n",
    "\n",
    "###### show cluster labels for each data points\n",
    "maxdists(lin) #array([   5.,   10.,  110.]) #Returns the maximum distance between any non-singleton cluster.\n",
    "t=np.median(maxdists(lin)) #t is The threshold to apply when forming flat clusters.\n",
    "fcluster(lin,t,'distance') #array([1, 1, 2, 2])\n",
    "#'distance' : points in each cluster have < t.\n",
    "###########################################################################################\n",
    "from sklearn.cluster import DBSCAN #Density-based spatial clustering of application with noise\n",
    "estimator=DBSCAN(eps=0.5, min_samples=5, metric='euclidean', n_jobs=-1)\n",
    "#eps : The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "#min_samples :The number of samples in a neighborhood for a point to be considered as a core point. \n",
    "#metric : The metric to use when calculating distance between instances in a feature array\n",
    "estimator.fit(X_train)\n",
    "y_pred=estimator.fit_predict(X_train) #Performs clustering on X and returns cluster labels\n",
    "estimator.labels_ #Labels of each point #Noisy samples are given the label -1.\n",
    "###########################################################################################\n",
    "from sklearn.cluster import AffinityPropagation  #cluster_centers_indices_\n",
    "#finds instances that are the most representative of others\n",
    "estimator=AffinityPropagation(max_iter=200)\n",
    "#max_iter : Maximum number of iterations.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.cluster import MeanShift\n",
    "# see https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "estimator.fit(X_train)\n",
    "y_pred=estimator.predict(X_test)\n",
    "estimator.labels_ #Labels of each point #Noisy samples are given the label -1.\n",
    "estimator.cluster_centers_ # show all cluster centers, shape=(cluster num, feature num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Load estimator and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "# save the model to disk\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "# now you can save it to a file\n",
    "joblib.dump(model, 'filename.pkl') \n",
    "\n",
    "# and later you can load it\n",
    "model = joblib.load('filename.pkl')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import pickle\n",
    "# now you can save it to a file\n",
    "with open('filename.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# and later you can load it\n",
    "with open('filename.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "##########################################################\n",
    "##########################################################\n",
    "data=np.array([1])\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "# save data\n",
    "joblib.dump(data,'a.pkl')\n",
    "# load data \n",
    "data=joblib.load('a.pkl', mmap_mode='c')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# save data\n",
    "from scipy import save\n",
    "save('filename.npy', data)\n",
    "# load data \n",
    "from scipy import load\n",
    "load('filename.npy')\n",
    "##########################################################\n",
    "##########################################################\n",
    "# save SQLite database\n",
    "from sqlite3 import connect \n",
    "import os\n",
    "if os.path.exists('test.sqlite'): \n",
    "    os.remove('test.sqlite')\n",
    "co=connect('test.sqlite')\n",
    "curs=co.cursor()\n",
    "curs.execute(\"CREATE TABLE employees (firstname varchar(32), lastname varchar(32), title varchar(32));\")\n",
    "curs.execute(\"INSERT INTO employees VALUES('Kelly', 'Koe', 'Engineer');\")\n",
    "co.commit()\n",
    "co.close()\n",
    "\n",
    "# load SQLite database\n",
    "from sqlite3 import connect \n",
    "co=connect('test.sqlite')\n",
    "curs=co.cursor()\n",
    "curs.execute(\"SELECT lastname FROM employees;\")\n",
    "for (name) in curs.fetchall():\n",
    "    print name\n",
    "co.close()\n",
    "#(u'Koe',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for classification\n",
    "y_pred = [0, 1, 1, 0]\n",
    "y_true = [0, 1, 1, 1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred) #0.75\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_true, y_pred) \n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_true, y_pred) \n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_true, y_pred) \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Plot f1_score vs. threshold\n",
    "y_prob=estimator.predict_proba(X_test)\n",
    "threshold=np.linspace(0.1, 1.0, 100)\n",
    "f1_list=[]\n",
    "for i in threshold:\n",
    "    y_pred=np.where(y_prob>i, 1,0)\n",
    "    f1_list.append(f1_score(y_true, y_pred))\n",
    "plt.plot(threshold, f1_list)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(y_true, y_pred) \n",
    "#            precision    recall  f1-score   support\n",
    "#\n",
    "#          0       0.50      1.00      0.67         1\n",
    "#          1       1.00      0.67      0.80         3\n",
    "#\n",
    "#avg / total       0.88      0.75      0.77         4\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef  #-1 -- 1 (perfect)\n",
    "matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_true, y_pred)\n",
    "#array([[1, 0, 0],\n",
    "#       [0, 1, 0],\n",
    "#       [0, 1, 1]])\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "classes=[0,1,2]\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "fmt = '.2f' \n",
    "thresh = cm.max() / 2.\n",
    "import itertools\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], fmt),\n",
    "    horizontalalignment=\"center\",\n",
    "    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c=confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "np.set_printoptions(precision=5)\n",
    "c=c.astype('float')/c.sum()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(c, interpolation='nearest',cmap=plt.cm.Blues)\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Up', 'Down'])\n",
    "plt.yticks(tick_marks, ['Up', 'Down'])\n",
    "plt.colorbar()\n",
    "thresh = c.max() / 2.\n",
    "\n",
    "for i, j in itertools.product(range(c.shape[0]), range(c.shape[1])):\n",
    "    plt.text(j, i, '%.2f' % c[i, j],horizontalalignment=\"center\",\n",
    "             color=\"white\" if c[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion matrix in the validation set')\n",
    "#############################################################################\n",
    "#############################################################################\n",
    "y_true = [0, 1, 1, 1]\n",
    "y_prob = [0.3,0.7,0.8,0.9]\n",
    "#y_prob= estimator.predict_proba(X_test)[:,1]\n",
    "from sklearn.metrics import precision_recall_curve#x axis is recall, y axis is precision\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_probability_pred)\n",
    "zip(precision, recall, thresholds)\n",
    "#[(1.0, 1.0, 0.34999999999999998),\n",
    "# (1.0, 0.66666666666666663, 0.40000000000000002),\n",
    "# (1.0, 0.33333333333333331, 0.80000000000000004)]\n",
    "from sklearn.metrics import auc\n",
    "print auc(precision, recall)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true, y_prob)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.metrics import roc_curve #Receiver Operating Characteristic\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probability_pred, pos_label=1)\n",
    "zip(fpr, tpr, thresholds)\n",
    "#[(0.0, 0.5, 0.80000000000000004),\n",
    "# (0.5, 0.5, 0.40000000000000002),\n",
    "# (0.5, 1.0, 0.34999999999999998),\n",
    "# (1.0, 1.0, 0.10000000000000001)]\n",
    "from sklearn.metrics import auc \n",
    "roc_auc = auc(fpr, tpr) #0.75\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_true, y_prob) #0.75\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.metrics import log_loss\n",
    "#-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n",
    "log_loss(y_true,y_prob)\n",
    "#############################################################################\n",
    "#############################################################################\n",
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "#for multi-label classification\n",
    "from sklearn.metrics import hamming_loss # 1 --- 0 (perfect)\n",
    "hamming_loss(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score # larger is better\n",
    "jaccard_similarity_score(y_true, y_pred)\n",
    "#############################################################################\n",
    "#############################################################################\n",
    "Y = [1,0,0,1,0,1,0,0,1,0]\n",
    "X = [[10,0, 5], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [2, 1, 10], \n",
    "     [3, 1, 12], \n",
    "     [10,0, 20]]\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(random_state=0,C=1)\n",
    "\n",
    "from sklearn.learning_curve import learning_curve#x axis is the number  of train dataset, y axis is accuracy\n",
    "train_sizes, train_scores, test_scores = learning_curve(classifier, \n",
    "                                                        X, Y, cv=2, n_jobs=-1, train_sizes=np.linspace(0.1,1,10), verbose=0)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.figure()\n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    "\n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n",
    "\n",
    "# Draw the plot and reset the y-axis\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "#plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(random_state=0)\n",
    "param_range=[0.01,0.1,1, 10, 100]\n",
    "from sklearn.learning_curve import validation_curve#x axis is the tuning parameter, y axis is accuracy\n",
    "train_scores, test_scores = validation_curve(classifier, \n",
    "                                             X, Y, param_name=\"C\", param_range=param_range,\n",
    "                                             cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve\")\n",
    "plt.xlabel(\"Model Parameter\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############  image quatization (convert 255 color levels into 64 color levles)  ####\n",
    "from mahotas import imread\n",
    "image=imread('test.jpg') / 255\n",
    "width, height, depth = tuple(image.shape)\n",
    "image= image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "print image.shape #(12780288L, 3L)\n",
    "# Use K-Means to create 64 clusters from 1000 sample colors.\n",
    "i=np.random.permutation(image.shape[0])\n",
    "image_train=image[i][:1000]\n",
    "# cluster the pixel intensities\n",
    "from sklearn.cluster import KMeans\n",
    "estimator = KMeans(n_clusters = 64, random_state=0)\n",
    "estimator.fit(image_train)\n",
    "y_pred=estimator.predict(image)\n",
    "compressed_colors=estimator.cluster_centers_ # shape =(64,3)\n",
    "compressed_image=np.zeros((width, height, depth))\n",
    "label_idx=0\n",
    "for i in range(width):\n",
    "    for j in range(height):\n",
    "        compressed_image[i][j] = compressed_colors[y_pred[label_idx]]\n",
    "        label_idx +=1\n",
    "plt.imshow(compressed_image)\n",
    "plt.axis('off')\n",
    "####################################################################################\n",
    "###############   identify dog or cat images   #####################################\n",
    "import mahotas as mh\n",
    "from mahotas.features import surf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import glob\n",
    "#Load the images\n",
    "all_instance_filenames = []\n",
    "all_instance_targets = []\n",
    "for f in glob.glob('train/*.jpg'):\n",
    "    target = 1 if 'cat' in f else 0\n",
    "    all_instance_filenames.append(f)\n",
    "    all_instance_targets.append(target)\n",
    "#Convert the images to grayscale, and extract the SURF descriptors\n",
    "surf_features = []\n",
    "counter = 0\n",
    "for f in all_instance_filenames:\n",
    "    image = mh.imread(f, as_grey=True)\n",
    "    surf_features.append(surf.surf(image, decriptor_only=True))\n",
    "\n",
    "#Split the images into training and testing data\n",
    "train_len = int(len(all_instance_filenames)*.80)\n",
    "X_train_surf_features = np.concatenate(surf_features[:train_len])\n",
    "X_test_surf_features = np.concatenate(surf_features[train_len:])\n",
    "y_train = all_instance_targets[:train_len]\n",
    "y_test = all_instance_targets[train_len:]\n",
    "\n",
    "#Group the extracted descriptors into 300 clusters. Use MiniBatchKMeans\n",
    "#to compute the distances to the centroids for a sample of the instances.\n",
    "n_clusters = 300\n",
    "estimator = MiniBatchKMeans(n_clusters=n_clusters)\n",
    "estimator.fit_transform(X_train_surf_features)\n",
    "\n",
    "#Construct feature vectors for training and testing data. Find the cluster\n",
    "#associated with each of the extracted SURF descriptors, and count them using np.bincount()\n",
    "X_train = []\n",
    "for instance in surf_features[:train_len]:\n",
    "    clusters = estimator.predict(instance)\n",
    "    features = np.bincount(clusters)\n",
    "    if len(features) < n_clusters:\n",
    "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
    "    X_train.append(features)\n",
    "\n",
    "X_test = []\n",
    "for instance in surf_features[train_len:]:\n",
    "    clusters = estimator.predict(instance)\n",
    "    features = np.bincount(clusters)\n",
    "    if len(features) < n_clusters:\n",
    "        features = np.append(features, np.zeros((1, n_clusters-len(features))))\n",
    "    X_test.append(features)\n",
    "    \n",
    "#Train a logistic regression classifier on the feature vectors and targets,\n",
    "clf = LogisticRegression(C=0.001, penalty='l2')\n",
    "clf.fit_transform(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print 'Accuracy: ', accuracy_score(y_test, predictions)\n",
    "\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "image_surf=[]\n",
    "for f in df['filename']:\n",
    "    image=mh.imread(f,as_grey=True)\n",
    "    s=surf.surf(image, 4, 6, 1, descriptor_only=True)\n",
    "    image_surf.append(np.array(s))\n",
    "image_surf_combined=np.concatenate(image_surf)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "distortions = [] #Sum of distances of samples to their closest cluster center#within_cluster SSE\n",
    "for i in range(400, 850, 50):\n",
    "    km=KMeans(n_clusters=i,init='k-means++',random_state=0, n_jobs=-1)\n",
    "    km.fit(image_surf_combined)\n",
    "    distortions.append(km.inertia_)\n",
    "plt.plot(range(50,450,50), distortions)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music genre classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "from scipy import fft\n",
    "sample_rate, x =read('xx.wav')#sample_rate unit: Hz\n",
    "####### plot specgram\n",
    "plt.specgram(x, Fs=sample_rate, xextent=(0,30))#plot first 30 sec \n",
    "####### use Fast Fourier transform frequency as features\n",
    "fft_features=abs(fft(x)[:1000])#choose first 10000 fft components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## predict what is the next letter \n",
    "text='SUPPOSING that Truth is a woman what then  Is'\n",
    "# unique words\n",
    "chars = sorted(list(set(text))) \n",
    "#[' ', 'G', 'I', 'N', 'O', 'P', 'S', 'T', 'U', 'a', 'e', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', 'w']\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 5\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences)) #('nb sequences:', 14)\n",
    "zip(sentences, next_chars)\n",
    "#[('SUPPO', 'S'),\n",
    "# ('POSIN', 'G'),\n",
    "# ('ING t', 'h'),\n",
    "# (' that', ' '),\n",
    "# ('at Tr', 'u'),\n",
    "# ('Truth', ' '),\n",
    "# ('th is', ' '),\n",
    "# ('is a ', 'w'),\n",
    "# ('a wom', 'a'),\n",
    "# ('oman ', 'w'),\n",
    "# ('n wha', 't'),\n",
    "# ('hat t', 'h'),\n",
    "# (' then', ' '),\n",
    "# ('en  I', 's')]\n",
    "\n",
    "## create X, y data\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)))\n",
    "print X.shape #(14L, 5L, 21L)\n",
    "y = np.zeros((len(sentences), len(chars)))\n",
    "print y.shape #(14L, 21L)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "#{'a': 9, ' ': 0, 'e': 10, 'i': 12, 'G': 1, 'o': 15, 'I': 2, 'h': 11, 'm': 13, \n",
    "# 's': 17, 'O': 4, 'N': 3, 'P': 5, 'S': 6, 'r': 16, 'U': 8, 'T': 7, 'w': 20, 'u': 19, 'n': 14, 't': 18}\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "## create LSTM model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(5, 21)))\n",
    "model.add(Dense(21))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.fit(X, y, batch_size=3, nb_epoch=1)\n",
    "\n",
    "## model prediction\n",
    "import random\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "print sentence #POSIN\n",
    "X_test = np.zeros((1, maxlen, len(chars)))\n",
    "for t, char in enumerate(sentence):\n",
    "    X_test[0, t, char_indices[char]] = 1.\n",
    "preds = model.predict(X_test, verbose=0)[0]\n",
    "#array([ 0.0512807 ,  0.07149747,  0.04306439,  0.04000025,  0.04201484,\n",
    "#        0.03914934,  0.06487428,  0.04007942,  0.03958567,  0.04070712,\n",
    "#        0.03861614,  0.05520309,  0.04078073,  0.04078181,  0.04187801,\n",
    "#        0.04210004,  0.03678251,  0.04980318,  0.05945305,  0.06003801,\n",
    "#        0.06230993], dtype=float32)\n",
    "probas=np.random.multinomial(1, preds, 1)\n",
    "#array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "next_index = np.argmax(probas)  #1\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "next_char = indices_char[next_index]\n",
    "print next_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Classify Movie reviews as good or bad\n",
    "from keras.datasets import imdb\n",
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "#X_train has 25000 reviews. each review has a lot of words represented by numbers\n",
    "#y_train is array([1, 0, 0, ..., 0, 1, 0]), 25000 elements. Good review is 1. \n",
    "from keras.preprocessing import sequence\n",
    "maxlen = 100  # for each review, choose the first 100 common words\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape) #('X_train shape:', (25000, 100))\n",
    "print('X_test shape:', X_test.shape)   #('X_test shape:', (25000, 100))\n",
    "\n",
    "## train data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# choose one of three layers\n",
    "model.add(GRU(128)) \n",
    "model.add(SimpleRNN(128)) \n",
    "model.add(LSTM(128))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from words 1, 2, 3, ... , (n-1) to predict words 2, 3, 4, ... , (n) in a sentence\n",
    "X=[('A very clean and well decorated empty bathroom'),\n",
    "  ('This is very bad house')]\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "vocabularySize = 100  # vocabulary size.\n",
    "# Split sentences into words, and define a vocabulary with the most common words.\n",
    "tokenizer = Tokenizer(nb_words = vocabularySize, filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n') \n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert the sentences into sequences of word ids using our vocabulary.\n",
    "X_Sequences = tokenizer.texts_to_sequences(X) #[[2, 1, 3, 4, 5, 6, 7, 8], [9, 10, 1, 11, 12]]\n",
    "# Keep dictionaries that map ids -> words, and words -> ids.\n",
    "word2id = tokenizer.word_index\n",
    "#{'a': 2,\n",
    "# 'and': 4,\n",
    "# 'bad': 11,\n",
    "# 'bathroom': 8,\n",
    "# 'clean': 3,\n",
    "# 'decorated': 6,\n",
    "# 'empty': 7,\n",
    "# 'house': 12,\n",
    "# 'is': 10,\n",
    "# 'this': 9,\n",
    "# 'very': 1,\n",
    "# 'well': 5}\n",
    "id2word = {idx: word for (word, idx) in word2id.items()}\n",
    "#{1: 'very',\n",
    "# 2: 'a',\n",
    "# 3: 'clean',\n",
    "# 4: 'and',\n",
    "# 5: 'well',\n",
    "# 6: 'decorated',\n",
    "# 7: 'empty',\n",
    "# 8: 'bathroom',\n",
    "# 9: 'this',\n",
    "# 10: 'is',\n",
    "# 11: 'bad',\n",
    "# 12: 'house'}\n",
    "maxSequenceLength = max([len(seq) for seq in X_Sequences])  # 8 Find the sentence with most words.\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "data = pad_sequences(X_Sequences, maxlen = (maxSequenceLength + 1), padding = 'post', truncating = 'post')\n",
    "id2word[0] = 'END'\n",
    "word2id['END'] = 0\n",
    "#data is array([[ 2,  1,  3,  4,  5,  6,  7,  8,  0],\n",
    "#                [ 9, 10,  1, 11, 12,  0,  0,  0,  0]], dtype=int32)\n",
    "\n",
    "inputData = data[:, :-1]  # for training model\n",
    "print inputData.shape #(2, 8)\n",
    "outputData = data[:, 1:]  # for target variables\n",
    "print outputData.shape #(2, 8)\n",
    "outputLabels = np.expand_dims(outputData, -1)\n",
    "print outputLabels.shape  #(2, 8, 1)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from keras.layers import Input, Dense, Activation, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "words = Input(batch_shape=(None, maxSequenceLength), name = \"input\")\n",
    "# Build a matrix of size vocabularySize x 300 where each row corresponds to a \"word embedding\" vector.\n",
    "# This layer will convert replace each word-id with a word-vector of size 300.\n",
    "embeddings = Embedding(vocabularySize, 300, name = \"embeddings\")(words)\n",
    "# Pass the word-vectors to the LSTM layer.\n",
    "hiddenStates = SimpleRNN(512, return_sequences = True, input_shape=(maxSequenceLength, 300), name = \"rnn\")(embeddings)\n",
    "# Apply a linear (Dense) layer to the outputs of the LSTM at each time step.\n",
    "denseOutput = TimeDistributed(Dense(vocabularySize), name = \"linear\")(hiddenStates)\n",
    "predictions = TimeDistributed(Activation(\"softmax\"), name = \"softmax\")(denseOutput)                                      \n",
    "# Build the computational graph by specifying the input, and output of the network.\n",
    "model = Model(input = words, output = predictions)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer = RMSprop(lr = 0.001))\n",
    "print(model.summary())\n",
    "#_________________________________________________________________\n",
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#input (InputLayer)           (None, 8)                 0         \n",
    "#_________________________________________________________________\n",
    "#embeddings (Embedding)       (None, 8, 300)            30000     \n",
    "#_________________________________________________________________\n",
    "#rnn (SimpleRNN)              (None, 8, 512)            416256    \n",
    "#_________________________________________________________________\n",
    "#linear (TimeDistributed)     (None, 8, 100)            51300     \n",
    "#_________________________________________________________________\n",
    "#softmax (TimeDistributed)    (None, 8, 100)            0    \n",
    "\n",
    "model.fit(inputData, outputLabels, batch_size = 1, nb_epoch = 2)\n",
    "\n",
    "y_pred=model.predict(data[:,:-1])\n",
    "print y_pred.shape #(2, 8, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
