{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "from __future__ import division # show flaoting number\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#Print all rows and columns. Dont hide any\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "np.array([1.3333]) # show array([ 1.33])\n",
    "\n",
    "np.nan #nan\n",
    "np.inf #inf\n",
    "\n",
    "np.array#([], dtype=np.float64(np.int32, np.string))#([[],[]])#([[[],[]],[[],[]]])\n",
    "np.asarray#same as np.array\n",
    "\n",
    "####################################\n",
    "np.mgrid[0:3,0:2]\n",
    "#array([[[0, 0],\n",
    "#        [1, 1],\n",
    "#        [2, 2]],\n",
    "#       [[0, 1],\n",
    "#        [0, 1],\n",
    "#        [0, 1]]])\n",
    "\n",
    "x = [-1,0,1]\n",
    "y = [-2,0,-2]\n",
    "xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "#xx : array([[-1,  0,  1]])\n",
    "#yy : array([[-2],\n",
    "#            [ 0],\n",
    "#            [-2]])\n",
    "np.concatenate([x, y]) #array([-1,  0,  1, -2,  0, -2])\n",
    "####################################\n",
    "x = np.array([[1,2,3], [4,5,6]])\n",
    "np.reshape(x,(3,2)) \n",
    "#array([[1, 2],\n",
    "#       [3, 4],\n",
    "#       [5, 6]])\n",
    "np.transpose(x) #array([[1, 2],[3, 4],[5, 6]])\n",
    "#array([[1, 4],\n",
    "#       [2, 5],\n",
    "#       [3, 6]])\n",
    "np.delete(x,2,1) # delete 2th col, axis=1 #array([[1, 2], [4, 5]])\n",
    "\n",
    "x=np.arange(24).reshape((2,3,4))\n",
    "#x=array([[[ 0,  1,  2,  3],\n",
    "#          [ 4,  5,  6,  7],\n",
    "#          [ 8,  9, 10, 11]],\n",
    "#        [[12, 13, 14, 15],\n",
    "#         [16, 17, 18, 19],\n",
    "#         [20, 21, 22, 23]]])\n",
    "x.transpose((0,2,1)) #the 1 and 2 axes exchage\n",
    "x.swapaxes(1,2)\n",
    "#array([[[ 0,  4,  8],\n",
    "#        [ 1,  5,  9],\n",
    "#        [ 2,  6, 10],\n",
    "#        [ 3,  7, 11]],\n",
    "#       [[12, 16, 20],\n",
    "#        [13, 17, 21],\n",
    "#        [14, 18, 22],\n",
    "#        [15, 19, 23]]])\n",
    "\n",
    "x =np.array([1,2,3,4,5,6,7,8])\n",
    "np.array_split(x, 2) \n",
    "np.split(x, 2)       \n",
    "#[array([1, 2, 3, 4]), array([5, 6, 7, 8])]\n",
    "np.split(x, [2,5,7])  #[array([1, 2]), array([3, 4, 5]), array([6, 7]), array([8])]\n",
    "\n",
    "x = np.array([[1,2,3], [4,5,6]])\n",
    "np.ravel(x) \n",
    "x.flatten()\n",
    "#array([1, 2, 3, 4, 5, 6])\n",
    "np.repeat(x, 2) #array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6])\n",
    "np.repeat(x, [1, 2], axis=0)\n",
    "#array([[1, 2, 3],\n",
    "#       [4, 5, 6],\n",
    "#       [4, 5, 6]])\n",
    "np.tile(x,2)\n",
    "#array([[1, 2, 3, 1, 2, 3],\n",
    "#       [4, 5, 6, 4, 5, 6]])\n",
    "np.tile(x, [2,3])\n",
    "#array([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "#       [4, 5, 6, 4, 5, 6, 4, 5, 6],\n",
    "#       [1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
    "#       [4, 5, 6, 4, 5, 6, 4, 5, 6]])\n",
    "####################################\n",
    "x=np.array([1,2,3])\n",
    "np.atleast_2d(x)#array([[1, 2, 3]])\n",
    "x[np.newaxis,:] #array([[1, 2, 3]])\n",
    "\n",
    "x = np.array([1,2])\n",
    "np.expand_dims(x, axis=0)  #array([[1, 2]])\n",
    "np.expand_dims(x, axis=1)\n",
    "#array([[1],\n",
    "#       [2]])\n",
    "x = np.array([[1,2],[2,3]])\n",
    "np.expand_dims(x, -1)\n",
    "x[:,:,np.newaxis]\n",
    "#array([[[1],\n",
    "#        [2]],\n",
    "#       [[2],\n",
    "#        [3]]])\n",
    "#.shape (2L, 2L, 1L)\n",
    "\n",
    "x = np.array([[1,2], [3,4]]); y= np.array([[10,20], [30,40]])\n",
    "np.append(x,y)  #array([ 1,  2,  3,  4, 10, 20, 30, 40])\n",
    "np.vstack((x,y))\n",
    "np.r_[x,y]\n",
    "np.append(x,y,axis=0)\n",
    "np.concatenate((x, y), axis=0)\n",
    "#array([[ 1,  2],\n",
    "#       [ 3,  4],\n",
    "#       [10, 20],\n",
    "#       [30, 40]])\n",
    "np.hstack((x,y))\n",
    "np.c_[x,y]\n",
    "np.column_stack((x,y))\n",
    "np.append(x,y,axis=1)\n",
    "np.concatenate((x, y), axis=1)\n",
    "#array([[ 1,  2, 10, 20],\n",
    "#       [ 3,  4, 30, 40]])\n",
    "\n",
    "####################################\n",
    "x = [1,2]\n",
    "y = [3,4]\n",
    "np.subtract.outer(x,y) #1-[3,4] , 2-[3,4]\n",
    "#array([[-2, -3],\n",
    "#       [-1, -2]])\n",
    "np.add.outer(x,y)\n",
    "#array([[4, 5],\n",
    "#       [5, 6]])\n",
    "np.multiply.outer(x,y)\n",
    "#array([[3, 4],\n",
    "#       [6, 8]])\n",
    "np.divide.outer(x,y)\n",
    "np.multiply.reduce(x) #1*2=2 #multiply values at the same axis\n",
    "\n",
    "x = np.array([[1,2], [3,4]])\n",
    "np.multiply.reduce(x, axis=0) #array([3, 8])\n",
    "np.divide.reduce(x, axis=0) \n",
    "np.add.reduce(x, axis=0) \n",
    "np.subtract.reduce(x, axis=0) \n",
    "np.multiply.accumulate(x, axis=0) #array([[1, 2],[3, 8]])\n",
    "np.divide.accumulate(x, axis=0)\n",
    "np.add.accumulate(x, axis=0)\n",
    "np.subtract.accumulate(x, axis=0)\n",
    "\n",
    "x=[0, 1, 2, 3, 4, 5]\n",
    "np.multiply.reduceat(x,[1,3], axis=0) #array([ 2, 60]) #2=x[1]*x[2], 60=x[3]*x[4]*x[5]\n",
    "np.multiply.reduceat(x,[0,3, 5], axis=0) # array([ 0, 12,  5]) #will have three values # x[0]*x[1]*x[2], x[3]*x[4], x[5]\n",
    "np.divide.reduceat(x,[1,3], axis=0)\n",
    "np.add.reduceat(x,[1,3], axis=0)\n",
    "np.subtract.reduceat(x,[1,3], axis=0)\n",
    "\n",
    "np.logical_not([True, False, 0, 1]) #array([False,  True,  True, False], dtype=bool) # T become F, 0 become True\n",
    "np.logical_and([True, True, False], [False, True, False])  #array([False,  True, False], dtype=bool) \n",
    "np.logical_xor([True, True, False], [False, True, False]) #array([ True, False, False], dtype=bool)# T, F -> T # T, T->F\n",
    "np.logical_or([True, True, False], [False, True, False])  #array([ True,  True, False], dtype=bool)\n",
    "\n",
    "x=np.array([[True, True, False],[False, True, False]])\n",
    "np.logical_and.reduce(x, axis=0) #array([False,  True, False], dtype=bool)\n",
    "np.logical_xor.reduce(x, axis=0) \n",
    "np.logical_or.reduce(x, axis=0)\n",
    "np.where(x) #(array([0, 0, 1], dtype=int64), array([0, 1, 1], dtype=int64)) # show where is true\n",
    "\n",
    "x=np.array([[0, 1], [2,0]])\n",
    "np.where(x)  #(array([0, 1], dtype=int64), array([1, 0], dtype=int64))\n",
    "np.where(x>0, 1,-1)\n",
    "#array([[-1,  1],\n",
    "#       [ 1, -1]])\n",
    "\n",
    "np.in1d([1,2,3],[1,7]) #array([ True, False, False], dtype=bool)\n",
    "\n",
    "np.isclose([1,2,3], 1) #array([ True, False, False], dtype=bool) check if array elements have 1\n",
    "np.isclose([1,2,3], [1,2,5]) #array([ True,  True, False], dtype=bool)\n",
    "x=np.array([[True,False],[True,True]])\n",
    "np.all(x, axis=0) #check all array elements are True   check all array elements are True\n",
    "np.any(x, axis=0) #array([ True,  True], dtype=bool)   check any array elements is True\n",
    "x=np.array([[0,2],[3,np.nan]]) #0 is false, NaN is True\n",
    "np.all(x, axis=0) #array([False,  True], dtype=bool)\n",
    "np.any(x, axis=0) #array([ True,  True], dtype=bool\n",
    "\n",
    "np.isnan([np.nan,1,2])            #array([ True, False, False], dtype=bool)\n",
    "np.isfinite([np.nan,np.inf, 0,1]) #array([False, False,  True,  True], dtype=bool)\n",
    "np.isinf([np.nan,np.inf, 0,1])    #array([False,  True, False, False], dtype=bool)\n",
    "\n",
    "a=np.array([True,True,False]);b=np.array([3,6,9])\n",
    "b[~a]  #show array([9])\n",
    "b[a]   #show array([3, 6])\n",
    "b==3   #show array([ True, False, False]\n",
    "####################################\n",
    "np.zeros(3)=np.empty(3) #array([ 0.,  0.,  0.])\n",
    "np.zeros((2,3))=np.empty((2,3))\n",
    "#array([[ 0.,  0.,  0.],\n",
    "#       [ 0.,  0.,  0.]])\n",
    "np.zeros_like([1,2,3]) #array([0, 0, 0]) return an array of zeros with the same shape of x\n",
    "np.ones(3)  #array([ 1.,  1.,  1.])\n",
    "np.ones((2,3))\n",
    "np.eye(3)\n",
    "#array([[ 1.,  0.,  0.],\n",
    "#       [ 0.,  1.,  0.],\n",
    "#       [ 0.,  0.,  1.]])\n",
    "np.linspace(1, 2, num=5) #array([ 1.  ,  1.25,  1.5 ,  1.75,  2.  ])\n",
    "np.logspace(1, 2, num=5) #array([  10.  ,   17.78,   31.62,   56.23,  100.  ])\n",
    "\n",
    "np.arange(2,6) #array([2, 3, 4, 5])\n",
    "np.arange(2,6,2) #array([2, 4])\n",
    "np.arange(4).reshape((2,2))\n",
    "#array([[0, 1],\n",
    "#       [2, 3]])\n",
    "\n",
    "x = [4, 3, 5, 7, 6, 8]\n",
    "indices = [0, 1, 4]\n",
    "np.take(x, indices) #array([4, 3, 6])\n",
    "\n",
    "x = np.array([1,2,3,4,5])\n",
    "np.put(x, [0, 2], [-44, -55])\n",
    "x # array([-44,   2, -55,   4,   5])\n",
    "\n",
    "x = np.array([0,1,2,3,4,5])\n",
    "np.roll(x, 2)  #array([4, 5, 0, 1, 2, 3]) each element shift 2 position\n",
    "####################################\n",
    "import random\n",
    "\n",
    "np.random.seed(0)#the seed for the next random number\n",
    "random.seed(0)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.randn() # 1.764052345967664 #Return a sample value from the “standard normal”distribution (mean 0 and variance 1 )\n",
    "np.random.randn(2,2)\n",
    "#array([[ 1.76,  0.4 ],\n",
    "#       [ 0.98,  2.24]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.rand(2,2) # random samples from a uniform distribution over [0, 1)\n",
    "#array([[ 0.44,  0.89],\n",
    "#       [ 0.96,  0.38]])\n",
    "np.random.rand() #0.9851111241835098\n",
    "random.random()    #0.8462617713735385 #show a number between 0-1 uniform distribution #can return 0 but not 1\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.randint(0,4, size=(2,2)) #each value is 0, 1, 2 or 3\n",
    "#array([[2, 3],\n",
    "#       [3, 2]]\n",
    "random.randint(0,4)    #0  #0, 1, 2 or 3 \n",
    "np.random.randint(0,4) \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.choice(5,3) #array([4, 2, 0]) show one value between 0, 1, 2, 3, 4\n",
    "np.random.choice(5,3,replace=False) # draw sample without replacement\n",
    "np.random.choice([1,2,4], 3) #array([1, 2, 2])\n",
    "random.choice([1,2,4])       # show 1, 2 , or 4\n",
    "\n",
    "import string\n",
    "A=string.ascii_uppercase  #'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "A=string.ascii_lowercase#'abcdefghijklmnopqrstuvwxyz'\n",
    "random.choice(A)          #'P'\n",
    "\n",
    "random.sample(range(5),3)   #[3, 0, 4] # from 0,1,2,3,4 choose 3 values\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.normal(loc=0,scale=1, size=(3,3)) #scale is std\n",
    "np.random.standard_normal(size=(3,3))    #Draw samples from a standard Normal distribution (mean=0, stdev=1).\n",
    "\n",
    "random.gauss(mu=0, sigma=1)               # 0.7764926257342503\n",
    "gaussiabsample=[random.gauss(mu=0, sigma=1) for i in range(100)]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "mean=[10, 0]\n",
    "covariance_matrix = [[3, 1], [1, 4]]\n",
    "np.random.multivariate_normal(mean, covariance_matrix , size=[3,])\n",
    "#Draw random samples from a multivariate normal distribution.\n",
    "#array([[  9.65297936,  -2.81245502],\n",
    "#       [ 12.9823395 ,  -1.29366211],\n",
    "#       [  8.95179546,   4.47125308]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Draw samples from a multinomial distribution\n",
    "#Throw a dice 10 times, the outcome can be 1 through 6\n",
    "#the probabality of [1,2,3,4,5,6] is [1/6,1/6,1/6,1/6,1/6,1/6]\n",
    "np.random.multinomial(10, [1/6,1/6,1/6,1/6,1/6,1/6], size=1)\n",
    "#array([[0, 3, 1, 4, 2, 0]])\n",
    "# the number 2 show 3 times\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "np.random.exponential(scale=1.0, size=3) #Draw samples from an exponential distribution p(x)=(1/scale)exp(-x/scale)\n",
    "np.random.uniform(-1,1,size=(3,3)) #a uniform distribution from -1 and 1\n",
    "np.random.permutation(5) #array([4, 2, 0, 1, 3])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "x=np.array([1,2,3,4])\n",
    "np.random.shuffle(x)\n",
    "random.shuffle(x)\n",
    "x #array([4, 1, 3, 2])\n",
    "####################################\n",
    "x = np.array([1,1,2])\n",
    "np.unique(x) #array([1, 2])\n",
    "\n",
    "x = np.array([[1,2,0],[0,4,5]])\n",
    "np.argsort(x, axis=0)# show the index from the smallest value to the largest\n",
    "#array([[1, 0, 0],\n",
    "#       [0, 1, 1]], dtype=int64)\n",
    "np.argmin(x, axis=0) #array([1, 0, 0], dtype=int64)\n",
    "np.argmax(x, axis=0) #array([0, 1, 1], dtype=int64)\n",
    "np.sort(x, axis=0) \n",
    "#array([[0, 2, 0],\n",
    "#       [1, 4, 5]]\n",
    "\n",
    "x = [1,2,2,3] \n",
    "y = [5,4,3,2] \n",
    "np.lexsort((y, x)) # Sort by x, then by y # list each element in x ranking (smallest =0), if the elements are the same, use y\n",
    "#array([0, 2, 1, 3], dtype=int64)\n",
    "####################################\n",
    "x = np.array([220,330,4])\n",
    "bins = np.array([0,100,200,300])\n",
    "np.digitize(x, bins)   \n",
    "np.searchsorted(bins,x)\n",
    "#array([3, 4, 1], dtype=int64)\n",
    "np.searchsorted(bins, 110)  #2\n",
    "np.searchsorted(bins, 100, side='left')  #1\n",
    "np.histogram(x, bins)   #(array([1, 0, 1], dtype=int64), array([  0, 100, 200, 300]))=counts, bins\n",
    "\n",
    "import bisect\n",
    "A=[10,11,12,13,14]\n",
    "bisect.bisect(A, 12.5)          #3  #find which index that 12.5 should be inserted to\n",
    "bisect.insort(A,12.5)           #A become [10, 11, 12, 12.5, 13, 14]\n",
    "bisect.bisect_left(A, 12)     #2\n",
    "bisect.bisect_right(A, 12)    #3\n",
    "\n",
    "x = np.array([220,330,40])\n",
    "y = np.array([22,33,4])\n",
    "binx = np.array([0,100,200,300])\n",
    "biny = np.array([0,10,20,30])\n",
    "np.histogram2d(x, y, bins=(binx, biny))\n",
    "#(array([[ 1.,  0.,  0.],\n",
    "#        [ 0.,  0.,  0.],\n",
    "#        [ 0.,  0.,  1.]]),\n",
    "# array([   0.,  100.,  200.,  300.]),\n",
    "# array([  0.,  10.,  20.,  30.]))  =counts, binx, biny\n",
    "\n",
    "\n",
    "x=np.array([0, 1, 1, 3, 3])\n",
    "weight = np.array([0.1,0.2,0.3,0.4,0.5]) \n",
    "np.bincount(x)                     #array([1, 2, 0, 2], dtype=int64) Count number of occurrences of each value \n",
    "np.bincount(x,weight)              #array([ 0.1,  0.5,  0. ,  0.9])\n",
    "np.bincount(x, weight, minlength=7) #array([ 0.1,  0.5,  0. ,  0.9,  0. ,  0. ,  0. ])\n",
    "####################################\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "np.mean(x, axis=0)     #array([ 2.5,  3.5,  4.5])\n",
    "np.average(x, axis=0)  #array([ 2.5,  3.5,  4.5])\n",
    "np.median(x, axis=0)\n",
    "\n",
    "np.std(x, axis=0)\n",
    "from scipy.stats import sem\n",
    "sem(x)\n",
    "\n",
    "np.var(x, axis=0)\n",
    "np.ptp(x, axis=0) # array([3, 3, 3]) return max-min\n",
    "np.sum(x, axis=0)\n",
    "np.min(x, axis=0)\n",
    "np.max(x, axis=0) \n",
    "np.diff(x, axis=0)  #array([[3, 3, 3]])\n",
    "np.prod(x, axis=0) #array([ 4, 10, 18])\n",
    "Y = np.array([[1,2,3],[4,5,6],[4,5,6]])\n",
    "np.diff(Y, axis=0)\n",
    "#array([[3, 3, 3],\n",
    "#       [0, 0, 0]])\n",
    "np.prod(Y, axis=0) #array([ 16,  50, 108])\n",
    "\n",
    "np.cumsum(x, axis=0)\n",
    "#array([[1, 2, 3],\n",
    "#       [5, 7, 9]])\n",
    "np.cumprod(x, axis=0) \n",
    "#array([[ 1,  2,  3],\n",
    "#       [ 4, 10, 18]])\n",
    "np.apply_along_axis(lambda x:(x[0] + x[-1]) * 0.5,  1, x) #array([ 2.,  5.]) # 1 means axis=1\n",
    "\n",
    "np.abs(x)\n",
    "#array([[1, 2, 3],\n",
    "#       [4, 5, 6]])\n",
    "np.sqrt(x)\n",
    "#array([[ 1.  ,  1.41,  1.73],\n",
    "#       [ 2.  ,  2.24,  2.45]]\n",
    "np.log(x)\n",
    "np.log1p(x) #return log(1+x)\n",
    "\n",
    "np.exp(x)\n",
    "from scipy import exp\n",
    "exp(x)\n",
    "\n",
    "from scipy.special import expit#expit(z)=1/(1+exp(-z))\n",
    "expit(x) #expit(x) = 1/(1+exp(-x)). \n",
    "\n",
    "np.expm1(x) #return exp(x)-1\n",
    "np.diag(x)  #array([1, 5])\n",
    "np.diag([1, 5]) \n",
    "#array([[1, 0],\n",
    "#       [0, 5]])\n",
    "\n",
    "x = np.array([[1,2,3],[4,5,6]])\n",
    "np.percentile(x, 50)         #3.5\n",
    "np.percentile(x, 50, axis=0) #array([ 2.5,  3.5,  4.5])\n",
    "np.percentile(x, [25,50,75], axis=0)\n",
    "#array([[ 1.75,  2.75,  3.75],\n",
    "#       [ 2.5 ,  3.5 ,  4.5 ],\n",
    "#       [ 3.25,  4.25,  5.25]])\n",
    "\n",
    "np.modf([2.7, 3.5]) #(array([ 0.7,  0.5]), array([ 2.,  3.]))\n",
    "np.maximum([2, 3, 4], [1, 5, 2]) #array([2, 5, 4])\n",
    "np.sign([-5., 4.5])  #array([-1.,  1.])\n",
    "np.floor([1.2,2.2,3.2]) #array([ 1.,  2.,  3.])\n",
    "\n",
    "import math\n",
    "math.ceil(2.3) #3\n",
    "math.sqrt(2)\n",
    "math.log(2)\n",
    "math.factorial(4) #4!=24\n",
    "####################################\n",
    "x = [1,2,3,4]\n",
    "y = [40,30,20,10]\n",
    "np.corrcoef(x,y)  #return the correlation coefficient matrix of the variables\n",
    "#array([[ 1., -1.],\n",
    "#       [-1.,  1.]])\n",
    "x = np.array([[1,2,3,4],[40,30,20,10]])\n",
    "np.corrcoef(x)\n",
    "#rray([[ 1., -1.],\n",
    "#      [-1.,  1.]])\n",
    "np.cov(x,y)      #return the covariance matrix of the variables\n",
    "x = np.array([[1,2,3,4],[40,30,20,10]])\n",
    "np.cov(x)\n",
    "#rray([[   1.67,  -16.67],\n",
    "#      [ -16.67,  166.67]])\n",
    "\n",
    "x = np.array([[1,0],[0,2]])\n",
    "#(1)\n",
    "eigenvalues, eigenvectors=np.linalg.eig(x)\n",
    "#(2)\n",
    "from scipy.linalg import eigh\n",
    "eigenvalues, eigenvectors=eigh(x)\n",
    "eigenvalues  #array([ 1.,  2.])\n",
    "eigenvectors \n",
    "#array([[ 1.,  0.],\n",
    "#       [ 0.,  1.]])\n",
    "np.linalg.inv(x)  # inverse of the matrix x\n",
    "#array([[ 1. ,  0. ],\n",
    "#       [ 0. ,  0.5]])\n",
    "\n",
    "x = np.array([3,4])\n",
    "np.linalg.norm(x)  #5\n",
    "x = np.array([[3,0],[0,4]])\n",
    "np.linalg.norm(x)  #5.0 # sqrt(3^2 +0+0+4^2)\n",
    "np.linalg.norm(x,axis=0) #array([ 3.,  4.])  # (sqrt(3^2+0), sqrt(4^2+0))\n",
    "\n",
    "x = np.array([10,20])\n",
    "y = np.array([[1,2],\n",
    "              [3,4]])\n",
    "np.dot(x,y) \n",
    "x.T.dot(y)\n",
    "#array([ 70, 100])\n",
    "\n",
    "x = np.array([[1,2],[3,4]])\n",
    "np.matrix(x)\n",
    "#matrix([[1, 2],\n",
    "#        [3, 4]])\n",
    "np.matrix(x).T    #transpose matrix\n",
    "#matrix([[1, 3],\n",
    "#        [2, 4]])\n",
    "np.matrix(x).I    #inverse matrix\n",
    "#matrix([[-2. ,  1. ],\n",
    "#        [ 1.5, -0.5]])\n",
    "####################################\n",
    "x = np.array([1,2])\n",
    "y= np.array([3,4])\n",
    "np.save('filename.npy', x)\n",
    "np.load('filename.npy')\n",
    "\n",
    "np.savetxt('filename.txt',x)\n",
    "np.loadtxt('filename.txt')\n",
    "\n",
    "np.save('filename.npz', ,a=x, b=y)\n",
    "data=np.load('filename.npz') \n",
    "data['a'] #is x\n",
    "\n",
    "y=x.copy()\n",
    "####################################\n",
    "x = np.array([1, 2, 2.5])\n",
    "x.astype(int)   #array([1, 2, 2])\n",
    "x.astype(float) #array([ 1. ,  2. ,  2.5])\n",
    "x.astype(str)   #array(['1.0', '2.0', '2.5'])\n",
    "x.dtype         #dtype('float64'\n",
    "x.all()         #True #ask all true\n",
    "x.any()         #True #ask any true\n",
    "x.ndim          #1\n",
    "x.shape         #(3L,)\n",
    "\n",
    "x.shape[0]\n",
    "len(x)          \n",
    "#3\n",
    "####################################\n",
    "x=np.array([[1,2],[3,4],[5,6],[7,8]])\n",
    "x[::2]# take every other row\n",
    "#array([[1, 2],\n",
    "#       [5, 6]])\n",
    "x[::-1]#row sequence flip\n",
    "#array([[7, 8],\n",
    "#       [5, 6],\n",
    "#       [3, 4],\n",
    "#       [1, 2]])\n",
    "x[:-1]#delete the last row\n",
    "#array([[1, 2],\n",
    "#       [3, 4],\n",
    "#       [5, 6]])\n",
    "x[0,1] #2\n",
    "\n",
    "x=[1,2,3]\n",
    "x[::-1]# return [3,2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Built-in Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "zipped = zip(x, y)\n",
    "zipped  #[(1, 4), (2, 5), (3, 6)]\n",
    "a,b=zip(*zipped) #[(1, 2, 3), (4, 5, 6)]=a,b , a=(1,2,3)\n",
    "\n",
    "x = ['A', 'B']\n",
    "list(enumerate(x)) #[(0, 'A'), (1, 'B')]\n",
    "list(enumerate(x, start=1)) #[(1, 'A'), (2, 'B')]\n",
    "\n",
    "dict(one=1, two=2, three=3)  #{'one': 1, 'three': 3, 'two': 2}\n",
    "dict(zip(['one', 'two', 'three'], [1, 2, 3]))\n",
    "dict([('two', 2), ('one', 1), ('three', 3)])\n",
    "dict({'three': 3, 'one': 1, 'two': 2})\n",
    "#{'one': 1, 'three': 3, 'two': 2}\n",
    "####################################\n",
    "x=[1,3,2,4]\n",
    "sorted(x)  #[1, 2, 3, 4]\n",
    "sorted(x, reverse=True) #[4, 3, 2, 1]\n",
    "#x=[[1,6,5],[2,3,10]]\n",
    "sorted(x, key=lambda x: x[1]) #[[2, 3, 10], [1, 6, 5]]\n",
    "sorted(['aaa','caa','baa'])  #['aaa', 'baa', 'caa']\n",
    "sorted('ccaab')              #['a', 'a', 'b', 'c', 'c']\n",
    "sorted(set('ccaab'))         #['a', 'b', 'c']\n",
    "\n",
    "x=[1,3,2,4]\n",
    "from random import random\n",
    "sorted(x, key=lambda x: random()) #[1, 4, 3, 2] \n",
    "import random\n",
    "random.shuffle(x)\n",
    "x # [3, 2, 4, 1]\n",
    "\n",
    "x=[1,3,2,4]\n",
    "x.sort()\n",
    "x # = [1, 2, 3, 4]\n",
    "\n",
    "x = ['ab','bbb','b']\n",
    "x.sort()\n",
    "x # = ['ab', 'b', 'bbb']\n",
    "x.sort(key=len)\n",
    "x  # = ['b', 'ab', 'bbb'] \n",
    "\n",
    "list(reversed([1,4,5,2])) #[2, 5, 4, 1]\n",
    "####################################\n",
    "range(5) #[0, 1, 2, 3, 4]\n",
    "\n",
    "x=[1,2,3]\n",
    "sum(x)  #6\n",
    "min(x)  #1\n",
    "max(x)  #3\n",
    "x=['aaa','b','cc']\n",
    "max(x,key=len)  #'aaa'\n",
    "\n",
    "7/3 #2.3333333333333335\n",
    "7//3 #2\n",
    "####################################\n",
    "set([1,3,2,2])  #{1, 2, 3}\n",
    "x = set([\"a\",\"a\"])\n",
    "y = set([\"a\",\"b\"])\n",
    "z = set([\"a\",\"c\"])\n",
    "f=(x,y,z) #f=({'a'}, {'a', 'b'}, {'a', 'c'})\n",
    "set.intersection(*f)  #{'a'}\n",
    "set.union(*f)         #{'a', 'b', 'c'}\n",
    "\n",
    "{0, 1, 2}.issuperset({0, 1})   #True\n",
    "{0, 1}.issubset({0, 1, 2})     #True\n",
    "{0, 1, 2}.isdisjoint({3,4})    #True #see if these two sets have no elements in common\n",
    "{0, 1}.union({0, 2, 5})#{0, 1, 2, 5} \n",
    "{0, 1}|{0, 2, 5}\n",
    "{0, 1}.intersection({0, 2, 5})#{0} \n",
    "{0, 1}&{0, 2, 5}\n",
    "{0, 1, 6}.difference({0, 2, 5})#{1, 6}\n",
    "{0, 1, 6}-{0, 2, 5}\n",
    "{0, 1, 6}.symmetric_difference({0, 2, 5})#{1, 2, 5, 6}     \n",
    "{0, 1, 6}^{0, 2, 5}\n",
    "\n",
    "frozenset([1,3,2,2])  #frozenset({1, 2, 3})\n",
    "frozenset([1,2])|frozenset([1,3]) #frozenset({1,2,3})\n",
    "frozenset([1,2])-frozenset([1,3]) #frozenset({2})\n",
    "frozenset([1,2])&frozenset([1,3]) #frozenset({1})\n",
    "frozenset([1,2])^frozenset([1,3]) #frozenset({2, 3})\n",
    "\n",
    "0|0 #0\n",
    "0|1 #1\n",
    "1|0 #1\n",
    "1|1 #1\n",
    "####################################\n",
    "list('abc')  #['a', 'b', 'c']\n",
    "tuple('abc') #('a', 'b', 'c')\n",
    "x=tuple([1,2,3]) #'tuple' object does not support item assignment\n",
    "x #(1, 2, 3)\n",
    "x[0] #1\n",
    "\n",
    "float(2) #show 2.0\n",
    "int(2.3) #show 2\n",
    "type(2)# show int\n",
    "str(a)#'a'\n",
    "bool()#(0),([]),(') return False #('a'),(1) return True\n",
    "\n",
    "####################################\n",
    "x={'a':1, 'b':2}\n",
    "x.keys()   #['a', 'b']\n",
    "x.values() #[1, 2]\n",
    "x.items()  #[('a', 1), ('b', 2)]\n",
    "x.get('a') # 1 \n",
    "x.update({'c':3})\n",
    "x['c'] = 3\n",
    "x  # = {'a': 1, 'b': 2, 'c': 3}\n",
    "del x['c']  #remove 'c': 3\n",
    "'a' in x #True\n",
    "list(iter(x))     \n",
    "list(x.iterkeys())\n",
    "#['a', 'b']\n",
    "\n",
    "from collections import defaultdict\n",
    "A=defaultdict(list) \n",
    "A['a'].append(1)  \n",
    "A  #defaultdict(list, {'a': [1]})\n",
    "A.items()#[('a', [1])]\n",
    "\n",
    "#can used to tuple too , ie.(123, 'xyz','xyz')\n",
    "C = [123, 'xyz','xyz'];\n",
    "C.count('xyz') # 2\n",
    "C.append([4, 5])# show [123, 'xyzxyz', 2009, [4, 5]]\n",
    "C.append(4);C# show [123, 'xyzxyz', 2009, 4]\n",
    "C.extend([4, 5, (1,2)])# C show [[123, 'xyz', 'xyz', 4, 5, (1, 2)]\n",
    "C.insert(1, 'H1')#C become [123, 'H1', 'xyz', 'xyz'] insert 'HI' to index1\n",
    "C.pop(1)\n",
    "C.remove('xyz')\n",
    "# C become [123, 'xyz']  remove the index 1\n",
    "####################################\n",
    "B='an book#pen'\n",
    "B.replace('an','a') #show a book#pen\n",
    "B.find('book')#3 show book the starting index# if B doesn't have book, it shows -1\n",
    "B.startswith('an') #show True\n",
    "B.split('#')# show ['an book', 'pen']#separate each token with #   #or (',')\n",
    "B.split()# show ['an', 'book#pen']\n",
    "B.isupper() #False #if B='BOOK', it will be True\n",
    "'an' in B #True\n",
    "B.index('b')#3\n",
    "B.count('o')#2\n",
    "B.upper()#'AN BOOK#PEN'\n",
    "\n",
    "A='3';A.isdigit() #True\n",
    "A='a';A.isdigit() #False\n",
    "A='aa';A.isalpha() #True\n",
    "\n",
    "x = ['a', 'b','c'];\n",
    "'::'.join(x)   #'a::b::c'\n",
    "x=' bc'\n",
    "x.strip()      #'bc' #delete space\n",
    "x='abc'\n",
    "x[::-1]        #'cba'\n",
    "####################################\n",
    "def f(x):\n",
    "    return x*2\n",
    "f(4)#8\n",
    "S=lambda x: x*2\n",
    "S(4)\n",
    "####################################\n",
    "#Convert an integer number to a binary string\n",
    "bin(8) #'0b1000'\n",
    "#0b represents a binary string.\n",
    "bin(4)[2:].zfill(8) #'00000100'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)]\n",
    "d = defaultdict(list)\n",
    "for k, v in s:\n",
    "    d[k].append(v)\n",
    "d  #defaultdict(list, {'blue': [2, 4], 'red': [1], 'yellow': [1, 3]})\n",
    "###############\n",
    "for _ in range(2):\n",
    "for i in range(2):\n",
    "for _ in xrange(200):#for very long range, recommand use xrange\n",
    "    \n",
    "for i in range(3):\n",
    "    print i*i\n",
    "#0\n",
    "#1\n",
    "#4\n",
    "[x*x for x in range(3)] # [0, 1, 4]\n",
    "###############    \n",
    "x = ['A', 'B']\n",
    "for i,j in enumerate(x):\n",
    "    print i,j\n",
    "#0 A\n",
    "#1 B\n",
    "\n",
    "x = [0, 1]\n",
    "y = ['A', 'B']\n",
    "for i, j in zip(x,y):\n",
    "    print i,j\n",
    "#0 A\n",
    "#1 B \n",
    "\n",
    "x = np.array([[0,1],['A','B']])\n",
    "for i in zip(*x):\n",
    "    print i\n",
    "#('0', 'A')\n",
    "#('1', 'B'\n",
    "x=[(1, 4), (2, 5), (3, 6)]\n",
    "for i in zip(*x):\n",
    "    print i\n",
    "#(1, 2, 3)\n",
    "#(4, 5, 6)  \n",
    "###############\n",
    "from itertools import combinations\n",
    "for i in combinations([0,1,2], r=2):\n",
    "    print i\n",
    "#(0, 1)\n",
    "#(0, 2)\n",
    "#(1, 2)\n",
    "###############\n",
    "from itertools import product \n",
    "for i in product([0,1],[0,1]):\n",
    "    print i\n",
    "#(0, 0)\n",
    "#(0, 1)\n",
    "#(1, 0)\n",
    "#(1, 1)\n",
    "###############\n",
    "import itertools\n",
    "A=lambda x:x[0]\n",
    "B=['abb', 'add', 'baa']\n",
    "for i, j in itertools.groupby(B,A):\n",
    "    print i, list(j)\n",
    "#a ['abb', 'add']\n",
    "#b ['baa']\n",
    "###############\n",
    "x={'a':1, 'b':2}\n",
    "for i, j in x.items():\n",
    "    print i, j\n",
    "#a 1\n",
    "#b 2\n",
    "###############\n",
    "df=DataFrame(['a','b'], columns=['A'])\n",
    "for i, j in df['A'].iteritems():\n",
    "    print i, j\n",
    "#0 a\n",
    "#1 b\n",
    "###############\n",
    "if true:\n",
    "    ...\n",
    "elif true:\n",
    "    ...\n",
    "else:\n",
    "    ...\n",
    "    \n",
    "if true or true\n",
    "###############\n",
    "for i in [1,2,3]:\n",
    "    if i ==2:\n",
    "        break #when i=2, stop for loop\n",
    "    print i\n",
    "#1\n",
    "\n",
    "for i in [1,2,3]:\n",
    "    if i ==2:\n",
    "        continue#skip i=2 loop\n",
    "    print i\n",
    "#1\n",
    "#3\n",
    "\n",
    "i=0\n",
    "while x > 0:\n",
    "    if i > 2:\n",
    "        break\n",
    "    i +=1\n",
    "    print i\n",
    "#1\n",
    "#2\n",
    "#3\n",
    "###############\n",
    "A=[(1,2),(3,4)]\n",
    "for i in A:\n",
    "    for x in i:\n",
    "        if x !=4:\n",
    "            print x\n",
    "#1\n",
    "#2\n",
    "#3\n",
    "B=[x for i in A for x in i if x != 4]\n",
    "B  #=[1, 2, 3]\n",
    "###############\n",
    "#fill a list value into a histogram\n",
    "A={}\n",
    "for i in [1,1,2,3,3]:\n",
    "    A[i]=A.get(i,0) +1\n",
    "\n",
    "from collections import Counter\n",
    "A=Counter([1,1,2,3,3])\n",
    "\n",
    "A  #= {1: 2, 2: 1, 3: 2}\n",
    "###############\n",
    "# A generator do not store all the values in memory, they generate the values on the fly.\n",
    "# Yield is a keyword that is used like return, except the function will return a generator\n",
    "def f():\n",
    "    for i in range(3):\n",
    "        yield i**2\n",
    "        \n",
    "F=f()# F is <generator object f at 0x000000000EC21900>\n",
    "for i in F:\n",
    "    print(i)\n",
    "#0\n",
    "#1\n",
    "#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expression operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "A='a b\\t    c\\t'\n",
    "re.split('\\s+',A)#['a', 'b', 'c', '']#seperate with more than one space\n",
    "re.split('\\s',A) #['a', 'b', '', '', '', '', 'c', '']\n",
    "re.compile('\\s+').split(A)#['a', 'b', 'c', '']#seperate with more than one space\n",
    "re.compile('\\s+').findall(A)#[' ', '\\t    ', '\\t']\n",
    "############################################################################\n",
    "B='lin10._%+-@yahoo.com, lin10@yahoo.com '\n",
    "patten=r'[A-Z0-9._%+-]+@[A-z0-9]+.[A-z]{2,4}'\n",
    "re.compile(patten, flags=re.IGNORECASE).findall(B)     # ['lin10._%+-@yahoo.com', 'lin10@yahoo.com']\n",
    "#search the first email\n",
    "f=re.compile(patten, flags=re.IGNORECASE).search(B)\n",
    "B[f.start():f.end()]                                   #'lin10._%+-@yahoo.com' \n",
    "#replace email address with 'email'\n",
    "re.compile(patten, flags=re.IGNORECASE).sub('email', B)#email, email \n",
    "############################################################################\n",
    "patten1=r'([A-Z0-9._%+-]+)@([A-z0-9]+).([A-z]{2,4})'\n",
    "re.compile(patten1, flags=re.IGNORECASE).findall(B)#[('lin10._%+-', 'yahoo', 'com'), ('lin10', 'yahoo', 'com')]\n",
    "re.compile(patten1, flags=re.IGNORECASE).sub(r'X:\\1,Y:\\2,Z:\\3',B)#'X:lin10._%+-,Y:yahoo,Z:com, X:lin10,Y:yahoo,Z:com '\n",
    "#(r' \\1 \\2 \\3',B)\n",
    "\n",
    "A=DataFrame(['lin10@yahoo.com'], columns=['B'])\n",
    "A['B'].str[:5]#0    lin10\n",
    "A['B'].str.findall(patten1, flags=re.IGNORECASE)\n",
    "#0    [(lin10, yahoo, com)]\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.match(patten1, flags=re.IGNORECASE) \n",
    "#0    (lin10, yahoo, com)\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.match(patten1, flags=re.IGNORECASE).str.get(1)  \n",
    "#0    yahoo\n",
    "#Name: B, dtype: object\n",
    "############################################################################\n",
    "patten2=r'(?P<X>[A-Z0-9._%+-]+)@(?P<Y>[A-z0-9]+).(?P<Z>[A-z]{2,4})'\n",
    "f1=re.compile(patten2, flags=re.IGNORECASE)\n",
    "f2=f1.match('wesm@yahoo.com')\n",
    "f2.groupdict()#{'X': 'wesm', 'Y': 'yahoo', 'Z': 'com'}\n",
    "#\\s any whitespace character, includes ' ', \\t, \\n(newline), \\r\n",
    "#[] Used to indicate a set of characters, In a set:\n",
    "# [a-z] means any lowercase letter; [0-9] means any number\n",
    "#special characters lose their special meaning inside [].\n",
    "# [._%+-] will match any of the literal characters '.', '_', '%', '+', '-'\n",
    "\n",
    "# . mean  any character except a newline\n",
    "# ^ mean the start of the string\n",
    "# $ mean the end of the string or just before the newline at the end of the string\n",
    "# *  match 0 or more repetitions \n",
    "# + match 1 or more repetitions \n",
    "# ? match 0 or 1 repetitions \n",
    "# {m} exactly m copies, ex: a{6} will match exactly six 'a' characters\n",
    "#{m,n} match from m to n repetitions , ex: a{3,5} will match from 3 to 5 'a' characters.\n",
    "# {m,n}? \n",
    "#(...) Matches whatever regular expression is inside the parentheses\n",
    "#(?P<name>...) assign a group name\n",
    "############################################################################\n",
    "A=DataFrame(['aaa bbb 6p 480kg BIM 41'], columns=['B'])\n",
    "#s(Series) can use like A'B] too\n",
    "A['B'].str.extract('(\\D*)', expand=False) \n",
    "#0    aaa bbb \n",
    "#Name: B, dtype: object \n",
    "A['B'].str.extract('(\\D*)', expand=True) #return a table\n",
    "#      0\n",
    "#0  aaa bbb\n",
    "A['B'].str.extract('.+\\s(\\D*) \\d+$', expand=False)\n",
    "#0    BIM\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.extract('(\\d+)p', expand=False)\n",
    "#0    6\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.extract('(\\d+)(kg|g)', expand=False) #return a table\n",
    "#    0  1\n",
    "#0  480 kg\n",
    "A['B'].map(lambda x: '_'.join([i for i in x.lower().split()]))\n",
    "#0    aaa_bbb_6p_480kg_bim_41\n",
    "#Name: B, dtype: object\n",
    "A['B'].map(lambda x: ' '.join([i for i in x.lower().split()]))\n",
    "#0    aaa bbb 6p 480kg bim 41\n",
    "#Name: B, dtype: object \n",
    "A['B'].str.split(' ')\n",
    "#0    [aaa, bbb, 6p, 480kg, BIM, 41]\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.split(' ').str[0]\n",
    "#0    aaa\n",
    "#Name: B, dtype: object\n",
    "A['B'].str.contains('p')\n",
    "#0    True\n",
    "#Name: B, dtype: bool\n",
    "\n",
    "#() group together and show up\n",
    "#*  0 or more\n",
    "#+  more than 1\n",
    "#\\D  non digit characters\n",
    "#\\s space\n",
    "#\\d digit number\n",
    "#. any character except new line\n",
    "# $ the end of the string\n",
    "# | or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '%.2f' % (2.3333)        #2.33\n",
    "print '%s' % ('sd')            #sd\n",
    "print '%10s' % ('sd')          #        sd # 10 spaces and sd \n",
    "print '%r' % ({'sd':1,'ee':2}) #{'ee': 2, 'sd': 1}\n",
    "print'_%03d' % 3               #_003\n",
    "print'%i' % 3.333              #3\n",
    "print'%d' % 3.333              #3\n",
    "\n",
    "print '{} {}'.format('one', 1)              #one 1\n",
    "print '{:>10}'.format('test')               #      test# (align right)\n",
    "print '{:10}'.format('test')                #test      #(align left)\n",
    "print '{:_<10}'.format('test')              #test______\n",
    "print '{:_>10}'.format('test')              #______test\n",
    "print '{:^10}'.format('test')               #   test   #(align center)\n",
    "print '{:.5}'.format('xylophone')           #xylop\n",
    "print '{:10.5}'.format('xylophone')         #xylop     #(align left)\n",
    "print '{:d}'.format(42)                     #42   #format number in integer\n",
    "print '{:f}'.format(3.14159265358)          #3.141593#float\n",
    "print '{:.3f}'.format(3.14159265358)        #3.142\n",
    "print '%.2f' % (2.3333)                     #2.33\n",
    "print ('%.2f' % (2.3333)).rjust(8)          #   2.33    #8 length string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#'test.csv'\n",
    "#   A   B\n",
    "#0  1  10\n",
    "#1  2  20\n",
    "with open ('test.csv','r') as f: \n",
    "    B=f.read()  #'A,B\\n1,10\\n2,20\\n'\n",
    "    \n",
    "list(open('test.txt'))#('xx.stdout')\n",
    "#['A\\tB\\n', '1\\t10\\n', '2\\t20\\n']\n",
    "####################################\n",
    "import csv\n",
    "with open('test.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print row\n",
    "#['A', 'B']\n",
    "#['1', '10']\n",
    "#['2', '20']\n",
    "####################################\n",
    "x=[]\n",
    "with open ('test.csv', 'rb') as f:\n",
    "    A=csv.reader(f)\n",
    "    row=A.next()\n",
    "    featurename=np.array(row) #array(['A', 'B']\n",
    "    for i in A:\n",
    "        x.append(i)\n",
    "x=np.array(x) \n",
    "x #array([['1', '10'],\n",
    "  #       ['2', '20']], \n",
    "####################################\n",
    "f=open('test.csv')\n",
    "reader=csv.reader(f)\n",
    "for line in reader:\n",
    "    print line\n",
    "#['A', 'B']\n",
    "#['1', '10']\n",
    "#['2', '20']\n",
    "lines=list(reader)\n",
    "header, values=lines[0], lines[1:]\n",
    "data_dict={columnname: columnvalues for columnname, columnvalues in zip(header, zip(*values))}\n",
    "data_dict #{'A': ('1', '2'), 'B': ('10', '20')}\n",
    "\n",
    "f=open('test.txt')\n",
    "lines=f.readlines()\n",
    "for i in lines:\n",
    "    A=i.strip().split('\\t') #if i is 'ap\\t bo\\t la\\n', show ['ap',' bo','la'] #strip() delete line terminator \\n\n",
    "    print A\n",
    "#['A', 'B']\n",
    "#['1', '10']\n",
    "#['2', '20']\n",
    "\n",
    "###################################\n",
    "df=pd.read_csv('.csv', sep=',', delimiter=None, header='infer'(None), usecols=['column1name','column2name'], \n",
    "index_col='column1name',prefix=None,dtype=None\n",
    "skiprows=[0,2], nrows=5, na_values=['NuLL'],chunksize=1000, parse_date=True)\n",
    "\n",
    "pd.read_csv('train.csv', dtype={'column1name':bool,'column2name':np.int32}, usecols=['column1name','column2name'])\n",
    "                   \n",
    "#sep=',' or '\\s+'(separate with a space) or'\\t' separate with a tab\n",
    "#index_col=#0, the first column become index#index_col='column1name', the unique values of column1 become the index level\n",
    "#usecols= [0, 1, 2] or [‘foo’, ‘bar’, ‘baz’]#pick the selected column\n",
    "#header=None show no column names\n",
    "#skiprows=[0,2] skip the first and third rows\n",
    "#na_values=['NuLL'], if any missing value, mark it as NaN\n",
    "#parse_date=True : date format become 1990-02-01, and date column become index \n",
    "#parse_dates=['timestamp'] if 'timestamp' is one of df columns\n",
    "#nrows=5 only show the first 5 rows\n",
    "#if reading a csv file has momery difficulty, need to define chuncksize, and then:\n",
    "df2 = pd.concat([chunk for chunk in df])\n",
    "\n",
    "df=pd.read_table('test.csv', sep=',')\n",
    "Series.from_csv('test.csv')\n",
    "#A     B\n",
    "#1    10\n",
    "#2    20\n",
    "#dtype: object\n",
    "\n",
    "data=pd.ExcelFile('test.xlsx')\n",
    "f=data.parse('test') #see the data in the first sheet 'test'\n",
    "#   A  B\n",
    "#0  1 10\n",
    "#1  2 20\n",
    "###################################\n",
    "#if f=imag001-0016.png\n",
    "f[3:f.index('-')] #will show 001\n",
    "###################################\n",
    "import json\n",
    "json_data = open(\"file name\")\n",
    "data = json.load(json_data)\n",
    "df=DataFrame(data)\n",
    "###################################\n",
    "import gzip\n",
    "f = gzip.open('file.txt.gz', 'rb')\n",
    "file_content = f.read()\n",
    "\n",
    "with gzip.open('file.txt.gz','r') as f:\n",
    "    for line in f: \n",
    "        print('got line', line)\n",
    "        \n",
    "from gzip import GzipFile\n",
    "data=[k for k in A.split() for A in GzipFile('file.txt.gz')]\n",
    "###################################\n",
    "from io import StringIO\n",
    "X=''' A, B\n",
    "      1, 10\n",
    "      2, 20'''\n",
    "X=unicode(X)\n",
    "pd.read_csv(StringIO(X)) # become a table\n",
    "#  A   B\n",
    "#0 1  10\n",
    "#1 2  20\n",
    "###################################\n",
    "import glob\n",
    "for i in glob.glob('C:\\Python27\\Scripts\\Anaconda2/*.txt'):\n",
    "          print i\n",
    "#C:\\Python27\\Scripts\\Anaconda2\\SentiWordNet_3.0.0_20130122.txt\n",
    "#C:\\Python27\\Scripts\\Anaconda2\\test.txt\n",
    "filelist=glob.glob('C:\\Python27\\Scripts\\Anaconda2/*.txt')\n",
    "filelist\n",
    "#['C:\\\\Python27\\\\Scripts\\\\Anaconda2\\\\SentiWordNet_3.0.0_20130122.txt',\n",
    "# 'C:\\\\Python27\\\\Scripts\\\\Anaconda2\\\\test.txt']\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Class=[]\n",
    "ID=[]\n",
    "filename=[]\n",
    "for file_name in glob.glob('train/*'):\n",
    "    label_name=file_name.split('\\\\')[-1]\n",
    "    image=glob.glob('train/'+label_name+'/*.jpg')\n",
    "    for f in image:\n",
    "        id_label=f.split('\\\\')[-1][:-4]\n",
    "        ID.append(id_label)\n",
    "        Class.append(label_name)\n",
    "        filename.append(f)\n",
    "df=DataFrame(data={'filename':filename, 'ID':ID, 'Class':Class})\n",
    "for f in df['filename']:\n",
    "    image=mh.imread(f)\n",
    "    resized = cv2.resize(image, (32, 32))\n",
    "    x_data.append(resized)\n",
    "x_data=np.array(x_data, dtype=np.uint8) # the max value is 255\n",
    "x_data=x_data.transpose((0, 3,1,2)).astype('float32')/255 # normalize the image values\n",
    "#x_data.shape is (1409L, 3L, 32L, 32L)\n",
    "###################################\n",
    "import os\n",
    "for dir_path, dir_name, files in os.walk('train/'):\n",
    "    for f in files:\n",
    "    #for f in files if f.endswith('.png')\n",
    "        image=os.path.join(dir_path, f) \n",
    "        A=mh.imread(image)\n",
    "###################################        \n",
    "import os\n",
    "path='C:\\Python27\\Scripts\\Anaconda2'\n",
    "filename= 'test.csv'\n",
    "os.path.join(path,filename) \n",
    "os.path.abspath(path+'/'+filename)\n",
    "#'C:\\\\Python27\\\\Scripts\\\\Anaconda2\\\\test.csv'\n",
    "\n",
    "os.listdir(path)            # list all files in that directory\n",
    "os.chdir(path)              # go to one directory\n",
    "os.path.dirname(path)       #'C:\\\\Python27\\\\Scripts'    #Return the directory name of pathname path\n",
    "os.path.exists(filename)    #True         #This file is existed in the current diectory\n",
    "os.getcwd()                 #'C:\\\\Python27\\\\Scripts\\\\Anaconda2'  # current directory\n",
    "os.path.splitext('C:\\\\Python27\\\\Scripts\\\\Anaconda2\\\\test.csv') #('C:\\\\Python27\\\\Scripts\\\\Anaconda2\\\\test', '.csv')\n",
    "os.remove(filename)\n",
    "\n",
    "#add Environment variables \n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark-client\"\n",
    "\n",
    "import sys\n",
    "#Python looks for its modules and packages in $PYTHONPATH. \n",
    "print(sys.path)#find out what is included in $PYTHONPATH\n",
    "#add another path (/home/myname/pythonfiles ) to your $PYTHONPATH\n",
    "sys.path.insert(0, \"/home/myname/pythonfiles\")\n",
    "###################################  \n",
    "## Load data from web\n",
    "from keras.utils.data_utils import get_file\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "###################################  \n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./*.csv\"]).decode(\"utf8\"))\n",
    "# list all the csv files under current directory\n",
    "print(check_output([\"ls\", \"./titanic/\"]))\n",
    "#test.csv\n",
    "#train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    ['Dan', 42],\n",
    "    ['Cordelia', 33]\n",
    "]\n",
    "import csv\n",
    "with open('test.csv', 'w') as outfile:\n",
    "    mywriter = csv.writer(outfile, lineterminator='\\n')\n",
    "    mywriter.writerow(['name', 'age']) #add header\n",
    "    for d in data:\n",
    "        mywriter.writerow(d)\n",
    "# test.csv become\n",
    "#   name     age\n",
    "#0  Dan      42\n",
    "#1 Cordelia  33\n",
    "#########JSON data (JAVASCRIPT object notation)\n",
    "data={'column1name':[1,2],'column2name':[3,4]}\n",
    "import json\n",
    "json_data=json.dump(data)\n",
    "##############################################\n",
    "# Save very large data array on disk as an in-memory array\n",
    "data = np.arange(4, dtype='float32').reshape(2,2)\n",
    "mmap=np.memmap('filename', dtype='float64', mode='w+', shape=(2,2))#shape=np.shape(data)\n",
    "mmap[:] = data\n",
    "mmap.flush()\n",
    "del mmap\n",
    "# Open an existing memory map\n",
    "mmap=np.memmap('filename',mode='r', dtype='float64', shape=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(nrows=1,ncols=1, figsize=(5,5), facecolor='white',sharex=True, sharey=True)\n",
    "x=[1,2,3,4]\n",
    "y=[1,4,9,16]\n",
    "ax.plot(x,y, ls='--',lw=1,marker='.',alpha=0.8,label='data',color='k');\n",
    "#alpha=(1: darkest),\n",
    "#drawstyle='steps-post',\n",
    "#hold=True ( for adding more plots on the same graph),   \n",
    "#color='k'(black),‘b’(blue),‘g’(green),‘r’(red),‘c’(cyan),‘m’(magenta),‘y’(yellow),‘w’(white)\n",
    "#marker=”.”(point),“o”(circle),“v”(triangle_down),“^”(triangle_up),“<”(triangle_left),“>”(triangle_right),“8”(octagon)\n",
    "#,“s”(square),“p”(pentagon),“*”(star),“D”(diamond)\n",
    "plt.plot(x, y, drawstyle='steps-post') #drawstyle='steps-mid', 'steps-pre', 'steps-post' #step function\n",
    "ax.semilogx(x,y); #a plot with log scaling on the x axis.\n",
    "ax.semilogy(x,y); #a plot with log scaling on the y axis.\n",
    "ax.scatter(x,y);\n",
    "ax.scatter(x,y, c=x,cmap=plt.get_cmap('RdBu')) # scatter points have different colors\n",
    "#cmap=plt.cm.RdYlGn,plt.cm.Blues,cmap=plt.cm.BrBG,cmap=plt.cm.Greens,plt.cm.RdGy,plt.cm.YlOrRd,plt.cm.autumn,plt.cm.binary,plt.cm.gist_earth,\n",
    "#plt.cm.gist_heat,plt.cm.hot,plt.cm.spring,plt.cm.summer,plt.cm.jet,plt.cm.Spectral_r, \"bone\"\n",
    "ax.axvline(x=2,ymin=0.58, ymax = 0.79, color='r', linestyle='--', lw=2) # draw a vertical line\n",
    "ax.hlines(y=2, xmin=-10, xmax=50, lw=2); # draw a horizontal line\n",
    "ax.set_xticks([0,10,20,30]);\n",
    "ax.set_yticks([0,10,20,30]);\n",
    "ax.set_xticklabels(['a','b','c','d'], rotation=90, fontsize=10);#([]) delete sticklabel\n",
    "ax.set_yticklabels\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0,100)\n",
    "ax.set_ylim(0,100)\n",
    "ax.axis([0, 100, 0, 50]) # xlim=(0,100), ylim=(0,50)\n",
    "ax.set_xlabel('xlabel')\n",
    "ax.set_ylabel('ylabel')\n",
    "ax.set_title('This is the title', fontsize='large')\n",
    "ax.legend(loc='best')\n",
    "#‘upper right’,‘upper center’,‘upper left’,‘lower left’,‘lower center’,‘lower right’,\n",
    "#‘center’,‘center left’,‘center right’,‘right’,\n",
    "ax.grid(True)\n",
    "\n",
    "ax.minorticks_on()\n",
    "ax.grid(which='major', linestyle='-', linewidth='0.5', color='black')\n",
    "ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "\n",
    "#Accuracy_validation=0.988\n",
    "#ax.text(2.5, 2,\"Accuracy: %.2f %%\" % (Accuracy_validation))\n",
    "ax.text(2.5, 2, r'$\\mu=100,\\ \\sigma=15$', bbox=dict(facecolor='red', alpha=0.5), fontsize=12) #(xposition, yposition\n",
    "ax.annotate('local max', xy=(2, 3), xytext=(3, 1.5),  #add arrow and text to a data point \n",
    "             arrowprops=dict(facecolor='black', shrink=0.05)) \n",
    "#xytext is the text location(xposition,yposition)\n",
    "#xy is the arrow final point location(xposition,yposition)\n",
    "\n",
    "# delete all edges\n",
    "for i in ax.spines.values():\n",
    "    i.set_visible(False)\n",
    "\n",
    "# set the xtick format\n",
    "for tick in ax.xaxis.get_ticklabels():\n",
    "    tick.set_fontsize('large')\n",
    "    tick.set_fontname('Times New Roman')\n",
    "    tick.set_color('blue')\n",
    "    tick.set_weight('bold')\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "# Close the figure \n",
    "plt.close(fig) #plt.close('all')\n",
    "######################  bar #######################################\n",
    "objects = ('A', 'B', 'C')\n",
    "y_pos = np.arange(len(objects))\n",
    "x = [10,8,6]\n",
    "ax.bar(y_pos, x, align='center', alpha=0.5,height=0.4, color='g',label='data', edgecolor = \"none\") # color='lawngreen', 'lightsalmon'\n",
    "ax.set_xticks(y_pos);\n",
    "ax.set_xticklabels(objects);\n",
    "\n",
    "ax.barh(y_pos, x, align='center', alpha=0.5,height=0.4, color='g',label='data', edgecolor = \"none\") \n",
    "ax.set_yticks(y_pos);\n",
    "ax.set_yticklabels(objects);\n",
    "\n",
    "df = pd.DataFrame([[5,10],[10,20],[5,9]])\n",
    "df.plot(kind='bar',ax=ax, stacked=True)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "labels=['A', 'B', 'C']\n",
    "scores=[5,3,7]\n",
    "labels_scores=zip(labels, scores)\n",
    "labels_scores=sorted(labels_scores, key=lambda x:x[1])\n",
    "print sorted(labels_scores, key=lambda x:x[1])\n",
    "fig=plt.figure (figsize=(5,5), facecolor='white')\n",
    "plt.barh(range(0, len(labels_scores)),zip(*labels_scores)[1])\n",
    "plt.yticks(range(0, len(labels_scores)), zip(*labels_scores)[0])\n",
    "#plt.xlim([0.45,0.58])\n",
    "plt.grid()\n",
    "plt.title('This is title')\n",
    "plt.show()\n",
    "######################  boxplot ######################################\n",
    "x1 = np.random.normal(0,1,50)\n",
    "x2 = np.random.normal(1,1,50)\n",
    "x3 = np.random.normal(2,1,50)\n",
    "bp = ax.boxplot([x1,x2,x3], patch_artist=True)\n",
    "for box in bp['boxes']:\n",
    "    # change outline color\n",
    "    box.set( color='#7570b3', linewidth=2)\n",
    "    # change fill color\n",
    "    box.set( facecolor = '#1b9e77' )\n",
    "\n",
    "## change color and linewidth of the whiskers\n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='#7570b3', linewidth=2)\n",
    "\n",
    "## change color and linewidth of the caps\n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='#7570b3', linewidth=2)\n",
    "\n",
    "## change color and linewidth of the medians\n",
    "for median in bp['medians']:\n",
    "    median.set(color='#b2df8a', linewidth=2)\n",
    "\n",
    "## change the style of fliers and their fill\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "    \n",
    "ax.set_xticklabels(['Sample1', 'Sample2', 'Sample3'])\n",
    "####################   hist   #######################################\n",
    "x = np.random.randn(1000)\n",
    "bins = np.linspace(-5, 5, 50)\n",
    "ax.hist(x, bins, alpha=0.5,normed=1);\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(100), columns=['A'])\n",
    "df['A'].plot(kind='hist', ax=ax)\n",
    "df['A'].plot(kind='kde', ax=ax, secondary_y=True) #kernel density estimate\n",
    "#~~~~~~~~~plot histogram and fitting Gaussian curve ~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import norm #normal continuous random variable\n",
    "# Generate 500 normal continuous random variables\n",
    "data = norm.rvs(10.0, 2.5, size=500)\n",
    "# Fit a normal distribution to the data:\n",
    "mu, std = norm.fit(data)\n",
    "# Plot the histogram.\n",
    "plt.hist(data, bins=25, normed=True, alpha=0.6, color='g')\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Fit results: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "plt.show()\n",
    "#######################  pie #########################################\n",
    "labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\n",
    "sizes = [15, 30, 45, 10]  # proportion\n",
    "explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "###################### contourf  ######################################\n",
    "xlist = np.linspace(-3.0, 3.0, 100)\n",
    "ylist = np.linspace(-3.0, 3.0, 100)\n",
    "X, Y = np.meshgrid(xlist, ylist)\n",
    "Z = np.sqrt(X**2 + Y**2)\n",
    "colors=[\"red\", \"orange\", \"gold\", \"limegreen\", \"k\", \n",
    "        \"#550011\", \"purple\", \"seagreen\"]\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(colors)\n",
    "g =ax.contourf(X, Y, Z, cmap=cmap)\n",
    "plt.colorbar(g)\n",
    "###################### spectram  ######################################\n",
    "x = np.cumsum(np.random.random(1024) - 0.2)\n",
    "data, freqs, bins, im =ax.specgram(x, NFFT=128, Fs=44100, noverlap=0,cmap='plasma',xextent=(0,30))\n",
    "#Fs is the sample rate. x axis is time(sec), y axis is frequency intensity (Hz)\n",
    "#the frequency is converted by fast fourier transform\n",
    "###################### a hexagonal binning plot  ######################\n",
    "x = np.random.standard_normal(10000)\n",
    "y = 2.0 + 3.0 * x + 4.0 * np.random.standard_normal(n)\n",
    "xmin = x.min()\n",
    "xmax = x.max()\n",
    "ymin = y.min()\n",
    "ymax = y.max()\n",
    "hb = ax.hexbin(x, y, gridsize=50, cmap='inferno')\n",
    "ax.axis([xmin, xmax, ymin, ymax])\n",
    "cb = fig.colorbar(hb, ax=ax)\n",
    "#cmap=matplotlib.cm.Blues\n",
    "#gridsize: The number of hexagons in the x-direction, default is 100\n",
    "#a 2-D histogram with hexagonal cells and color each cell according to how many data in this cell. \n",
    "#It can be much more informative than a scatter plot\n",
    "###################Rectangle, circle, Polygon  ######################\n",
    "for p in [plt.Rectangle((0.1, 0.1), 0.3, 0.6,hatch='/'),\n",
    "          plt.Rectangle((0.5, 0.1), 0.3, 0.6,hatch='\\\\',fill=False)]:\n",
    "    ax.add_patch(p)\n",
    "#((xpoition_of_bottomleftcorner,ypoition_of_bottomleftcorner ), rectanglewidth, rectanglelength)\n",
    "\n",
    "circle1 = plt.Circle((0.5, 0.5), 0.2, color='blue')\n",
    "circle2 = plt.Circle((1, 1), 0.2, color='g', clip_on=False)\n",
    "ax.add_artist(circle1)\n",
    "ax.add_artist(circle2)\n",
    "#((xposition_of_center,yposition_of_center ), radius)\n",
    "\n",
    "rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\n",
    "circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\n",
    "pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],color='g', alpha=0.5) ##([x1,y1],[x2,y2],[x3,y3])\n",
    "ax.add_patch(rect)\n",
    "ax.add_patch(circ)\n",
    "ax.add_patch(pgon)\n",
    "################### plot an image  #################################\n",
    "img=plt.imread('test.jpg') #Importing image data into Numpy arrays\n",
    "plt.imshow(img) #(image, cmap='Set3'('Greys', plt.cm.gray,plt.cm.gray_r), extent=[xmin, xmax,ymin,ymax],interpolation='nearest')\n",
    "plt.colorbar()\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('test.jpg')\n",
    "################### fill_between  #################################\n",
    "X  = np.linspace(0,3,200)\n",
    "Y1 = X**2 + 3\n",
    "Y2 = np.exp(X) + 2\n",
    "ax.plot(X,Y1,lw=4)\n",
    "ax.plot(X,Y2,lw=4)\n",
    "ax.fill_between(X, Y1,Y2,color='k',alpha=.5) #filling the regions between y1 and y2 \n",
    "################### matshow  #################################\n",
    "x = [[4,2,3],[4,2,5]]\n",
    "ax.matshow(x, cmap=plt.cm.Spectral_r, interpolation='none');# Heatmap\n",
    "################### add_subplot  #################################\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)   #top left\n",
    "ax2 = fig.add_subplot(222)   #top right\n",
    "ax3 = fig.add_subplot(223)   #bottom left\n",
    "ax4 = fig.add_subplot(224)   #bottom right \n",
    "################### add another plot #################################\n",
    "# add another plot on the existed plot. A new plot position can be adjusted\n",
    "x=[1,2,3,4]\n",
    "y=[1,4,9,16]\n",
    "ax = fig.add_axes([0.5, 0.5, 1., 1.,])#(xmin, ymin, dx, and dy) for the subplot,\n",
    "ax.plot(x,y); \n",
    "#xmin and ymin are the coordinates of the lower left corner of the subplot, \n",
    "#dx and dy are the width and height of the subplot, with all values specified in relative units\n",
    "#where 0 is left/bottom and 1 is top/right\n",
    "################### y has two different scale axis for two curve #######\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "ax2 = ax1.twinx()\n",
    "t = np.linspace(0., 10., 100)\n",
    "ax1.plot(t, t ** 2, 'b-')\n",
    "ax2.plot(t, 1000 / (t + 1), 'r-')\n",
    "ax1.set_ylabel('Density (cgs)', color='red')\n",
    "ax2.set_ylabel('Temperature (K)', color='blue')\n",
    "ax1.set_xlabel('Time (s)')\n",
    "################### save a plot into a file############################\n",
    "fig.savefig('myplot.eps', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df=pd.DataFrame({'X1':[1,2,3,4,5],'X2':[10,20,30,40,60], 'Label':[1,1,1,2,2]})\n",
    "plt.figure(figsize=(5,5))\n",
    "# Set theme\n",
    "sns.set(style='whitegrid', context='notebook', font_scale=1.2)\n",
    "#(1)\n",
    "sns.lmplot(x='X1', y='X2', data=df, fit_reg=True)   # scatter points + fitting linear line\n",
    "sns.lmplot(x=df.X1, y=df.X2)\n",
    "sns.lmplot(x='X1', y='X2', data=df, fit_reg=False, hue='Label')   # scatter points + different labels have different colors\n",
    "sns.regplot(x='X1', y='X2', data=df)\n",
    "\n",
    "plt.ylim(0, None)\n",
    "plt.xlim(0, None)\n",
    "plt.xticks(rotation=-45)\n",
    "plt.title('This is title', fontsize=12)\n",
    "plt.xlabel('Xlabel', fontsize=12)\n",
    "plt.ylabel('Ylabel', fontsize=12)\n",
    "\n",
    "sns.plt.title('This is title')\n",
    "sns.plt.xlabel('Xlabel')\n",
    "sns.plt.ylabel('Ylabel')\n",
    "sns.despine()#remove top and right edges\n",
    "\n",
    "g=sns.lmplot(...)\n",
    "g.set_xticklabels(rotation=-45)\n",
    "g.set(xlim=(0, 10), ylim=(0, 100),xticks=[10, 30, 50], yticks=[2, 6, 10])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(2)\n",
    "sns.jointplot(x='X', y='Y', data=df) # scatter points + X histogram + Y histogram\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "sns.pointplot(df.X1, df.X2, alpha=0.8) # points and lines\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(3)\n",
    "sns.boxplot(df.Label, df.X1)\n",
    "df.boxplot(by='Label')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(4)\n",
    "sns.barplot(df.Label, df.X1)\n",
    "sns.barplot(df.Label, df.X1, label='This is data', color='b', order=[2,1],alpha=0.7) #alpha : control transparent\n",
    "\n",
    "pkmn_type_colors = ['#78C850',# Grass\n",
    "                    '#F08030']# Fire\n",
    "#pkmn_type_colors = ['#78C850',  # Grass\n",
    "#                    '#F08030',  # Fire\n",
    "#                    '#6890F0',  # Water\n",
    "#                    '#A8B820',  # Bug\n",
    "#                    '#A8A878',  # Normal\n",
    "#                    '#A040A0',  # Poison\n",
    "#                    '#F8D030',  # Electric\n",
    "#                    '#E0C068',  # Ground\n",
    "#                    '#EE99AC',  # Fairy\n",
    "#                    '#C03028',  # Fighting\n",
    "#                    '#F85888',  # Psychic\n",
    "#                    '#B8A038',  # Rock\n",
    "#                    '#705898',  # Ghost\n",
    "#                    '#98D8D8',  # Ice\n",
    "#                    '#7038F8',  # Dragon]\n",
    "sns.barplot(df.Label, df.X1, palette=pkmn_type_colors)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(5)\n",
    "sns.stripplot(df.Label, df.X1, jitter=True)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(6)\n",
    "sns.violinplot(df.Label, df.X1)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(7)\n",
    "sns.swarmplot(df.Label, df.X1, split=True)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(8)\n",
    "sns.factorplot(x='Label', y='X1', data=df, kind='swarm') #kind=Swarmplot\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(9)\n",
    "sns.countplot(df.X1)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(10)\n",
    "sns.distplot(df['X1'].dropna(), bins=3, kde=True) # histogram\n",
    "sns.kdeplot(df['X1'].dropna())  # only show kde curve\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(11)\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr,square=True,xticklabels=['Class','Feature1','Feature2'], yticklabels=['Class','Feature1','Feature2'],\n",
    "            fmt='.2f', annot=True,  annot_kws={'fontsize':13},\n",
    "           cbar=True,cmap='Blues',cbar_kws={'orientation':'horizontal'}).set(xlabel='prediction',ylabel='actual')\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#'price' correlation matrix\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "corrmat = df.corr()\n",
    "k = 15 #number of variables for heatmap\n",
    "cols = corrmat.nlargest(k, 'price')['price'].index\n",
    "cm = np.corrcoef(df[cols].values.T )\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, \\\n",
    "                 yticklabels=cols.values, xticklabels=cols.values)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df=pd.DataFrame({'column1':[1,1,2,1,1],'column2':[10,20,20,20,20], 'column3':[1,2,3,4,2]})\n",
    "df1 = df.groupby([\"column1\", \"column2\"])[\"column3\"].aggregate(\"count\").reset_index()\n",
    "#   column1  column2  column3\n",
    "#0    1       10        1\n",
    "#1    1       20        3    \n",
    "#2    2       20        1\n",
    "df2 = df1.pivot('column1', 'column2', 'column3')\n",
    "#column2    10     20\n",
    "#column1\n",
    "#1          1.0   3.0\n",
    "#2          NaN   1.0\n",
    "sns.heatmap(df2)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(12)\n",
    "sns.clustermap(df)\n",
    "sns.clustermap(df,metric=\"correlation\")\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(13)\n",
    "sns.pairplot(df[['X1','X2']])\n",
    "sns.pairplot(df, hue='Label',diag_kind='kde')\n",
    "\n",
    "#every two features scatter plot\n",
    "col = ['column1', 'column2', 'column2']\n",
    "sns.pairplot(df[col], size = 2.5)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#(14)\n",
    "df=pd.DataFrame({'X':[1,2,3,4,5],'Label1':['A','A','A','A','B'],'Label2':['Y','Y','Y','N','N']})\n",
    "g = sns.FacetGrid(df, col=\"Label1\",  row=\"Label2\")\n",
    "g = g.map(plt.hist, \"X\") \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# plot boxplot and histogram for a feature variable\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "g1=sns.boxplot(x=df['X1'],ax=ax[0])\n",
    "g1.set(xlim=(0,1000))\n",
    "g2=sns.distplot(df['X1'].dropna(), kde=False, bins=1000, ax=ax[1])\n",
    "g2.set(xlim=(0,1000))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# plot the histogram of the target variable and its log transformation\n",
    "f, ax = plt.subplots(nrows=2,ncols=2, figsize=(8,12))\n",
    "F=sns.distplot(df['price'], kde=False, bins=100, ax=ax[0,0])\n",
    "F.set(title='price distribution')\n",
    "F1=sns.distplot(np.log1p(df['price']), kde=False, bins=100,ax=ax[0,1])\n",
    "F1.set(title='log(price +1) distribution')\n",
    "G=sns.boxplot(x=df['price'],ax=ax[1,0])\n",
    "#F1.set(xlim=(0,2))\n",
    "G1=sns.boxplot(x=np.log1p(df['price']),ax=ax[1,1])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(10,5))\n",
    "# suppose df['X1'] is a categorical variable\n",
    "g1 = sns.countplot(x=df['X1'], ax=ax[0])\n",
    "g2 = sns.boxplot(x=df['X1'],y=df['price'], ax=ax[1] ) # suppose 'price' is the target variable\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# plot outliers\n",
    "def outliers_percentage(x):\n",
    "    x=np.log(x) \n",
    "    q75, q25 = np.percentile(x, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    minvalue = q25 - (iqr*1.5)\n",
    "    maxvalue = q75 + (iqr*1.5)\n",
    "\n",
    "    high_outliers=len(x[x > maxvalue])\n",
    "    low_outliers=len(x[x < minvalue])\n",
    "    Total_outliers=high_outliers+low_outliers\n",
    "    #print 'The percentage of outliers : {:.2f} %'.format(np.float(Total_outliers)*100/len(df_train)) \n",
    "    return np.float(Total_outliers)*100/len(x)\n",
    "f, ax = plt.subplots(figsize=(12,5))\n",
    "object_list=df.columns[df.dtypes =='object']\n",
    "outlier_list=df_train.drop(object_list, axis=1).apply(outliers_percentage).sort_values(ascending=False)\n",
    "outlier_list = outlier_list[outlier_list.values > 0]\n",
    "g=sns.boxplot(x=(df[outlier_list.index[:10]]))\n",
    "g.set_xticklabels(labels = outlier_list.index[:10],rotation=90)\n",
    "############################################################################################\n",
    "df=pd.DataFrame({'X1':[1,2,3,4,5],'X2':[10,20,30,40,60], 'Label':[1,1,1,2,2]})\n",
    "from pandas.tools.plotting import parallel_coordinates\n",
    "parallel_coordinates(df, 'Label')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from pandas.tools.plotting import radviz \n",
    "radviz(df, 'Label')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from pandas.tools.plotting import andrews_curves\n",
    "andrews_curves(df, 'Label')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=pd.DataFrame({'date':['2016-01-15', '2016-01-13', '2016-01-11', '2016-01-14', '2016-01-15'],\n",
    "                 'hour':[12, 10, 11, 3, 0], \n",
    "                 'i':[1,2,3,1,4]})\n",
    "from bokeh.charts import HeatMap, output_file, show\n",
    "output_file('test.html')\n",
    "hm = HeatMap(df, x='date', y='hour',  values='i', stat='count')#stat=None\n",
    "show(hm)\n",
    "############################################################################################\n",
    "from scipy.stats import probplot#x axis is Quantiles, y axis is the sorted value\n",
    "X = np.random.normal(loc = 20, scale = 5, size=100)   \n",
    "probplot(X, dist=\"norm\", plot=pylab)\n",
    "plt.show()\n",
    "\n",
    "############################################################################################\n",
    "#boxcox transform\n",
    "from scipy.stats import boxcox\n",
    "fig = plt.figure()\n",
    "ax= fig.add_subplot(211)\n",
    "x = stats.loggamma.rvs(5, size=500) + 5\n",
    "xt, lambda_value = boxcox(x)\n",
    "prob = stats.probplot(xt, dist=stats.norm, plot=ax)\n",
    "plt.show()\n",
    "print lambda_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'column1name':[1,2],'column2name':[3,4]})\n",
    "#   column1name  column2name\n",
    "#0       1          3\n",
    "#1       2          4\n",
    "\n",
    "pd.DataFrame({'column1name':Series([1,2], index=['a','b']),'column2name':Series([10,20], index=['b','c'])})\n",
    "#   column1name   column2name\n",
    "#a     1.0          NaN\n",
    "#b     2.0          10.0\n",
    "#c     NaN          20.0\n",
    "\n",
    "pd.DataFrame({'column1name':{'index1':1, 'index2':2},'column2name':{'index1':3, 'index2':4}}, dtype='float')\n",
    "pd.DataFrame([[1,2],[3,4]],index=['index1','index2'],\n",
    "             columns=['column1name','column2name'], dtype='float')\n",
    "#         column1name  column2name  \n",
    "#index1      1.0         2.0\n",
    "#index2      3.0         4.0\n",
    "\n",
    "\n",
    "pd.DataFrame([[1,2],[3,4]],index=pd.Index(['index1','index2'], name='indexlevel'),\n",
    "             columns=pd.Index(['column1name','column2name'], name='columnlevel'), dtype='float')\n",
    "#columnlevel  column1name  column2name\n",
    "#indexlevel\n",
    "#index1         1.0         2.0\n",
    "#index2         3.0         4.0\n",
    "\n",
    "pd.DataFrame([[1,2],[3,4]],index=pd.Index(['index1','index2'], name='indexlevel'),\n",
    "             columns=pd.MultiIndex.from_arrays([['level1x','level1y'],['level2x','level2y']], \n",
    "            names=['level1name', 'level2name']), dtype='float')\n",
    "#level1name    level1x    level1y\n",
    "#level2name    level2x    level2y\n",
    "#indexlevel\n",
    "#index1         1.0         2.0\n",
    "#index2         3.0         4.0\n",
    "\n",
    "df=pd.DataFrame([1,2,3,4],\n",
    "                index=pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),('two', 'a'), ('two', 'b')]))\n",
    "#          0\n",
    "#one  a   1.0\n",
    "#     b   2.0\n",
    "#two  a   3.0\n",
    "#     b   4.0\n",
    "\n",
    "from collections import defaultdict\n",
    "A=defaultdict(dict)\n",
    "A['column1']['index1']=10\n",
    "A       #defaultdict(dict, {'column1': {'index1': 10}})\n",
    "df=pd.DataFrame(A)\n",
    "#        column1\n",
    "#index1  10\n",
    "\n",
    "pd.Series({'indexa':1,'indexb':2},index=['indexa','indexb'])\n",
    "pd.Series({'indexa':1,'indexb':2})#\n",
    "pd.Series([1,2],index=['indexa','indexb'])\n",
    "#indexa    1\n",
    "#indexb    2\n",
    "pd.Series([1,2],index=[['indexa','indexb'],['index1','index2']])\n",
    "#indexa  index1    1\n",
    "#indexb  index2    2\n",
    "####################################################################################\n",
    "df.head#() #or (10)\n",
    "df.tail#()#only show the last 5 rows\n",
    "df.sample(n=100) #randomly pick up 100 rows\n",
    "df.sample(frac=0.5) # randomly pick up 50% of sample\n",
    "df.describe()#show count, mean, std, min, 25%, 50%, 75%, max for each column\n",
    "\n",
    "pd.set_option('display.max_rows',None)#dispay all the rows\n",
    "pd.set_option('display.max_columns',None)\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,2],[2,4]],columns=['column1name','column2name'], dtype='float')\n",
    "df.to_csv('--.csv, index=False)\n",
    "df.to_csv('xx.stdout', na_rep='NULL', sep='|', index=False, header=False) \n",
    "#if there is a missing value, mark it as NULL\n",
    "x=df.to_dict()\n",
    "x # {'column1name': {0: 1.0, 1: 2.0}, 'column2name': {0: 2.0, 1: 4.0}}\n",
    "df1=df.copy()\n",
    "####################################################################################                  \n",
    "df=pd.DataFrame([[1,2],[3,4]],index=['index1','index2'],\n",
    "             columns=['column1name','column2name'], dtype='float')\n",
    "df[['column1name','column2name']]\n",
    "#         column1name  column2name  \n",
    "#index1      1.0         2.0\n",
    "#index2      3.0         4.0\n",
    "\n",
    "df.info()\n",
    "#Index: 2 entries, index1 to index2\n",
    "#Data columns (total 2 columns):\n",
    "#column1name    2 non-null float64\n",
    "#column2name    2 non-null float64\n",
    "#dtypes: float64(2)\n",
    "#memory usage: 48.0+ bytes\n",
    "df.irow(1) #show the values in the second row\n",
    "df.iloc[1]\n",
    "df.ix[1]\n",
    "df.iloc[1,:]\n",
    "df.ix[1,:]          \n",
    "df.loc['index2']\n",
    "df.ix['index2']\n",
    "#column1name    3.0\n",
    "#column2name    4.0\n",
    "df.iloc[:,:1]\n",
    "df.ix[:,:1]\n",
    "#         column1name\n",
    "#index1    1.0\n",
    "#index2    3.0\n",
    "df['column1name'] # show column1 values \n",
    "#index1    1.0\n",
    "#index2    3.0\n",
    "df.iloc[1,0]\n",
    "df.ix[1,0]\n",
    "#3\n",
    "df.ix[[1,0]] # the index change\n",
    "df.iloc[[1,0]]\n",
    "df.ix[['index2','index1'],['column1name','column2name']]\n",
    "df.loc[['index2','index1'],['column1name','column2name']]\n",
    "#         column1name  column2name\n",
    "#index2     3.0          4.0\n",
    "#index1     1.0          2.0\n",
    "df[:1] \n",
    "df.ix[:1]\n",
    "         column1name  column2name\n",
    "#index1     1.0           2.0\n",
    "\n",
    "df[\"column1name\"].ix[df[\"column1name\"]<2] = 10\n",
    "\n",
    "##can use df.ix too\n",
    "df.loc[df['column1name']>0]# show all rows that the values in df['column1name'] > 0\n",
    "df.loc[df['column1name']==0,'column2name'] #show all rows that  the values in df['column1name'] = 0, only show column2name column\n",
    "df.loc[np.isclose(df['column1name'],0),'column2name']#show all rows that  the values in df['column1name'] = 0, only show column2name column\n",
    "df.loc[~np.isclose(df['column1name'], 1), 'column2name']#show all rows that  the values in df['column1name'] is not 1, only show column2name column\n",
    "#index2    4.0\n",
    "#Name: column2name, dtype: float64\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=pd.DataFrame({'product_id':[2,1,102],'user_id':[1,1,1],'order_id':[1,1,2]})\n",
    "df[(df.user_id==1) & (df.order_id==2)]\n",
    "#   order_id  product_id  user_id\n",
    "#2     2        102         1          \n",
    "basket=[1,4,5]\n",
    "df[df['product_id'].isin(basket)]\n",
    "#   order_id  product_id user_id\n",
    "#1     1          1        1\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df.columns.get_loc('column1name') #0\n",
    "df.columns.get_loc('column2name') #1          \n",
    "df.values #become array\n",
    "#array([[ 1.,  2.],\n",
    "#       [ 3.,  4.]])\n",
    "df.index #show index list\n",
    "#Index([u'index1', u'index2'], dtype='object')\n",
    "df.index.map(str.upper)#(str.lower) or (str.title)\n",
    "#array(['INDEX1', 'INDEX2'], dtype=object)\n",
    "df.astype(int)\n",
    "#         column1name  column2name  \n",
    "#index1      1             2\n",
    "#index2      3             4\n",
    "df.astype(str)\n",
    "df['column1name'].dtype #dtype('float64')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df = pd.DataFrame({'column1': ['A', 'B'],'column2': [1,2] })\n",
    "df.select_dtypes(exclude=['object'])\n",
    "#    column2\n",
    "#0    1\n",
    "#1    2\n",
    "df.select_dtypes(include=['object'])\n",
    "#    column1\n",
    "#0    A\n",
    "#1    B\n",
    "\n",
    "df.columns[df.dtypes==object] #list all 'object' column names\n",
    "\n",
    "df_dtype = df.dtypes.reset_index()\n",
    "#  index      0\n",
    "#0 column1  object\n",
    "#1 column2  int64          \n",
    "df_dtype.columns = [\"Counts\", \"Data type\"]\n",
    "df_dtype.groupby('Data type').agg('count').reset_index()\n",
    "#      Data type   Counts\n",
    "#0      int64        1   \n",
    "#1      object       1\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df=pd.DataFrame({'column1':['1','2','A']})\n",
    "#convert to numeric\n",
    "pd.to_numeric(df[\"column1\"], errors=\"coerce\") #invalid parsing will be set as NaN\n",
    "#0    1.0\n",
    "#1    2.0\n",
    "#2    NaN\n",
    "#Name: column1, dtype: float64\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "df.columns#show column name list\n",
    "#Index([u'column1name', u'column2name'], dtype='object')\n",
    "df.columns.map(str.title)\n",
    "df.index.name='Indexlevel' \n",
    "df.columns.name='Columnlevel'\n",
    "#Clumnlevel  column1name  column2name\n",
    "#Indexlevel\n",
    "#index1         1.0        2.0\n",
    "#index2         3.0        4.0\n",
    "df.rename(index={'index1':'index1_newname'}, columns={'column1name':'column1name_newname'},inplace=True)\n",
    "df.rename(index=str.upper,columns=str.upper, inplace=True)\n",
    "#                 COLUMN1NAME_NEWNAME    COLUMN2NAME\n",
    "#INDEX1_NEWNAME          1.0             2.0\n",
    "#INDEX2                  3.0             4.0\n",
    "          \n",
    "old_columnnames=list(df.columns)\n",
    "new_columnnames=['new_column1','new_column2']\n",
    "df.rename(columns=dict(zip(old_columnnames,new_columnnames)),inplace=True)\n",
    "          \n",
    "#shuffle rows\n",
    "df=df.reindex(np.random.permutation(df.index))\n",
    "df=df.take(np.random.permutation(len(df)))\n",
    "\n",
    "df=df.reindex(index=['index2','index1'], columns=['column2name','column3name'],method='ffill')\n",
    "#         column2name  column3name\n",
    "#index2     4.0         NaN\n",
    "#index1     2.0         NaN\n",
    "#fill_value=0, method :'bfill' (or ’ffill’, 'nearest')\n",
    "#ffill: use last valid value to fill missing value #bfill: next valid value # nearest: nearest valid value\n",
    "df=df.reset_index()\n",
    "#    index    column1name   column2name\n",
    "#0   index1      1.0           2.0\n",
    "#1   index2      3.0           4.0\n",
    "df.reset_index(drop=True)\n",
    "#     column1name  column2name\n",
    "#0         1.0      2.0\n",
    "#1         3.0      4.0\n",
    "df=df.set_index(['column1name','column2name' ], drop=False)#the unique values in column1 and column2 become two-level indexes \n",
    "#if drop=True, the new columns won't include the column1 and column2\n",
    "#                          column1name   column2name\n",
    "#column1name  column2name\n",
    "#   1.0          2.0           1.0          2.0\n",
    "#   3.0          4.0           3.0          4.0\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,np.nan],[3,4]],index=['index1','index2'],\n",
    "             columns=['column1name','column2name'], dtype='float')\n",
    "#         column1name  column2name  \n",
    "#index1      1.0         NaN\n",
    "#index2      3.0         4.0\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "#          column1name  column2name\n",
    "#index2       3.0         4.0\n",
    "df.dropna(subset=['column1name'],inplace=True) \n",
    "#   column1name  column2name  \n",
    "#index1      1.0         NaN\n",
    "#index2      3.0         4.0\n",
    "df.dropna(axis=1,inplace=True)\n",
    "#        column1name\n",
    "#index1    1.0\n",
    "#index2    3.0\n",
    "df.dropna(how='all', thresh=None,inplace=True)\n",
    "#how='all'('any'), drop a row that are all NA\n",
    "#thresh=int value : you specify a minimum number of non-null values for the row/column to be kept\n",
    "df.fillna(0,inplace=True)#_=\n",
    "df['column2name'].fillna(df['column2name'].mean(), inplace=True)#, df['A'] will be replaced#\n",
    "df.fillna({'column2name':-1},inplace=True)#, replace nan with -1 in the 'column2name' column\n",
    "df.fillna(method='ffill',inplace=True) #ffill: use last valid value to fill missing value\n",
    "#method={‘bfill’,‘ffill’}, #bfill: next valid value\n",
    "df.isnull()\n",
    "#         column1name    column2name\n",
    "#index1    False           True\n",
    "#index2    False           False\n",
    "df.isnull().any(axis=0)\n",
    "#column1name    False\n",
    "#column2name     True\n",
    "df.isnull().sum(axis=0)#show how many nan in each feature\n",
    "#column1name    0\n",
    "#column2name    1\n",
    "df.notnull()\n",
    "#        column1name   column2name\n",
    "#index1    True          False\n",
    "#index2    True          True\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "missing=(df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "#column2name    0.5\n",
    "#column1name    0.0\n",
    "missing=missing[missing.values > 0]\n",
    "print 'The number of features that have missing values: {}'.format(len(missing))\n",
    "fig, ax = plt.subplots(figsize=(10,18))\n",
    "plt.barh(range(len(missing)), missing.values)\n",
    "ax.set_yticks(range(len(missing)))\n",
    "ax.set_yticklabels(missing.index, rotation='horizontal')\n",
    "ax.set_xlabel(\"Precentage of missing values in a column\", fontsize = 14)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,2],[2,4],[3,6],[4,10]],columns=['column1name','column2name'], dtype='float')\n",
    "#     column1name  column2name\n",
    "#0       1.0         2.0\n",
    "#1       2.0         4.0\n",
    "#2       3.0         6.0\n",
    "#3       4.0         10.0\n",
    "df.corr()#show correlation between two variables(two columns)\n",
    "#              column1name  column2name\n",
    "#column1name    1.000000     0.982708\n",
    "#column2name    0.982708     1.000000 1\n",
    "df.column1name.corr(df.column2name)#=0.98270762982399085\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# find features that are most related to the target variable\n",
    "df=pd.DataFrame([[1,2],[2,4],[3,6],[4,10]],columns=['column1name','column2name'], dtype='float')\n",
    "df.corr(method='pearson', min_periods=100)\n",
    "df_corr = np.abs(df_corr)\n",
    "target_corr=df_corr['column1name'].sort_values(ascending=False)[1:]# suppose column1name is the target variable\n",
    "\n",
    "# show which features are highly correlated\n",
    "target_corr_list=target_corr.index   \n",
    "drop_list=set([]) #if two features are highly correlated to each other, put one of them into drop_list\n",
    "threshold=0.90 # set correlation threhold\n",
    "for f in target_corr_list:\n",
    "    A=df_corr[f].sort_values(ascending=False)\n",
    "    if f not in drop_list:\n",
    "        for i in A[A.values > threshold].index:\n",
    "            if i not in drop_list and i!=f:\n",
    "                    print '{} | {} | R2={}'.format(f, i, A[i])\n",
    "                    drop_features=[i]\n",
    "                    drop_list=drop_list|set(drop_features)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df.corrwith(df.column1name)#show correlation values between column1 and other columns\n",
    "#column1name    1.000000\n",
    "#column2name    0.982708\n",
    "df.cov()#show the covariance between two variables(columns)\n",
    "#              column1name   column2name\n",
    "#column1name    1.666667     4.333333\n",
    "#column2name    4.333333    11.666667\n",
    "\n",
    "#a normal distribution will have a skewness of 0. \n",
    "df.skew() #show each feature skew number# less skew means feature value close to 0\n",
    "#column1name    0.000000\n",
    "#column2name    0.752837\n",
    "\n",
    "from scipy.stats import skew\n",
    "skew(df, axis=0) #show each feature's skewness\n",
    "#array([ 0.        ,  0.43465076])\n",
    "df.apply(lambda x: skew(x.dropna()))\n",
    "#column1name    0.000000\n",
    "#column2name    0.434651\n",
    "#dtype: float64\n",
    "    \n",
    "#a normal distribution will have a kurtosis of 0. \n",
    "df.kurt() #return kurtosis of each feature \n",
    "# larger value mean larger tails (more outliers) than a normal distribution \n",
    "\n",
    "df.sum()\n",
    "#column1name    10.0\n",
    "#column2name    22.0\n",
    "df.sum(axis=1, skipna=True)\n",
    "#0     3.0\n",
    "#1     6.0\n",
    "#2     9.0\n",
    "#3    14.0\n",
    "df.mean(axis=1, skipna=True)\n",
    "df.mad(axis=1, skipna=True) #show sum(abs(xi-meanx))/number of x (mean absolute deviation of the values) for each column (row) \n",
    "df.var(axis=1, skipna=True) #show sum((xi-meanx)**2)/(number of x -1)\n",
    "df.std(axis=1, skipna=True) #show sqrt(var) for each column\n",
    "df.median(axis=1, skipna=True)# show the median value in each column\n",
    "df.max(axis=1, skipna=True) #show the largest value in each column\n",
    "df.min(axis=1, skipna=True) #show the smallest value in each column\n",
    "df.pct_change()#if the values in a column are 1,2,3, it will become NaN, (2-1)/1, (3-2)/2\n",
    "#     column1name  column2name\n",
    "#0         NaN         NaN\n",
    "#1     1.000000     1.000000\n",
    "#2     0.500000     0.500000\n",
    "#3     0.333333     0.666667\n",
    "df['column1name']/df['column1name'].shift(1)-1\n",
    "df['column1name'] / df['column2name'].astype(float)\n",
    "#0    0.5\n",
    "#1    0.5\n",
    "#2    0.5\n",
    "#3    0.4\n",
    "#dtype: float64\n",
    "df['column1name'].pct_change()\n",
    "#0         NaN\n",
    "#1    1.000000\n",
    "#2    0.500000\n",
    "#3    0.333333\n",
    "(df==0).sum(axis=1) # how many zeros in each row\n",
    "#0    0\n",
    "#1    0\n",
    "#2    0\n",
    "#3    0\n",
    "df.count()#count non_NA values for each columns\n",
    "#column1name    4\n",
    "#column2name    4\n",
    "df.diff()\n",
    "#     column1name  column2name\n",
    "#0         NaN         NaN\n",
    "#1         1.0         2.0\n",
    "#2         1.0         2.0\n",
    "#3         1.0         4.0\n",
    "df.cumprod(skipna=True)\n",
    "df.cumsum(skipna=True)# if the values in a column are 1,2,3, it will become 1, 3, 6\n",
    "#     column1name  column2name\n",
    "#0       1.0         2.0\n",
    "#1       3.0         6.0\n",
    "#2       6.0        12.0\n",
    "#3      10.0        22.0\n",
    "df.quantile([0,0.5,1]) # 0(show minvalue) ,1(show maxvalue),0.5(median value)\n",
    "#      column1name   column2name\n",
    "#0.0      1.0           2.0\n",
    "#0.5      2.5           5.0\n",
    "#1.0      4.0          10.0\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,5.0],[10,4],[5,10]],index=['index2','index1','index3'],\n",
    "             columns=['column1name','column2name'], dtype='float')\n",
    "#         column1name   column2name\n",
    "#index2      1.0          5.0\n",
    "#index1      10.0         4.0\n",
    "#index3      5.0          10.0\n",
    "df.sort_values(by='column1name',ascending=True, inplace=True)  #Sort by the values along either axis\n",
    "df.sort_index(by='column1name',ascending=True, inplace=True)\n",
    "#        column1name   column2name\n",
    "#index2    1.0           5.0\n",
    "#index3    5.0          10.0\n",
    "#index1    10.0          4.0\n",
    "#(by=['column1name','column2name']) sort by the values in column1name first and then column2name if column1name have the same values\n",
    "df['column1name'].order(ascending=False)#, Sort by the values along either axis#(ascending=False)from largest to smallest\n",
    "#index1    10.0\n",
    "#index3     5.0\n",
    "#index2     1.0\n",
    "df.idxmax(axis= 0, skipna=True)#show an index that has the largest value in each column\n",
    "#column1name    index1\n",
    "#column2name    index3\n",
    "df.idxmin(axis= 0, skipna=True)#show an index that has the smallest value in each column\n",
    "df['column1name'].argsort() #will sort the df['column1name'] values \n",
    "#index2    0\n",
    "#index1    2\n",
    "#index3    1\n",
    "df['column1name'].argsort()[::-1] \n",
    "#index3    1\n",
    "#index1    2\n",
    "#index2    0\n",
    "df.rank(axis= 0)#replace each value with its ranking along axis 0 or 1, the ranking of the smallest value is 1\n",
    "#        column1name   column2name\n",
    "#index2     1.0          2.0\n",
    "#index1     3.0          1.0\n",
    "#index3     2.0          3.0          \n",
    "df['column1name'].isin([1,'g'])#see if each value is 1 or 'g'\n",
    "#index2     True\n",
    "#index1    False\n",
    "#index3    False\n",
    "#Name: column1name, dtype: bool\n",
    "\n",
    "df['column1name'].value_counts().sort_index(axis=0, ascending=True)\n",
    "#1.0     1\n",
    "#5.0     1\n",
    "#10.0    1\n",
    "#Name: column1name, dtype: int64\n",
    "df['column1name'].unique()\n",
    "#array([  1.,  10.,   5.])\n",
    "df['column1name'].nunique() #3\n",
    "####################################################################################\n",
    "df=pd.DataFrame([['one',1],['one',1],['two',2],['one',2]],\n",
    "             columns=['column1name','column2name'], dtype='float')\n",
    "#   column1name column2name\n",
    "#0   one           1\n",
    "#1   one           1\n",
    "#2   two           2\n",
    "#3   one           2\n",
    "df.duplicated()#see if each row is a duplicate \n",
    "#0    False\n",
    "#1     True\n",
    "#2    False\n",
    "#3    False\n",
    "\n",
    "df.drop_duplicates('column1name')#drop the #1 and #3 rows#keep the first duplicate of column1\n",
    "#   column1name column2name\n",
    "#0   one           1\n",
    "#2   two           2\n",
    "df.drop_duplicates('column1name',keep='last')#drop the #0 and #1 rows   #keep the last duplicate of column1\n",
    "#   column1name column2name\n",
    "#2   two           2\n",
    "#3   one           2\n",
    "df.drop_duplicates(['column1name','column2name'])#drop the #1 row   #keep the first duplicate of column1 and column2\n",
    "#   column1name column2name\n",
    "#0   one           1\n",
    "#2   two           2\n",
    "#3   one           2\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,10],[2,20],[3,30]],index=['index1','index2','index3'],\n",
    "                columns=['column1name','column2name'], dtype='float')\n",
    "#        column1name  column2name\n",
    "#index1      1.0         10.0\n",
    "#index2      2.0         20.0\n",
    "#index3      3.0         30.0\n",
    "df.values.flatten()\n",
    "#array([  1.,  10.,   2.,  20.,   3.,  30.])\n",
    "df.shift(2,axis=0) #move data down by 2 indexes  #(-2,axis=0) move data up by 2 indexes\n",
    "#       column1name  column2name\n",
    "#index1      NaN         NaN\n",
    "#index2      NaN         NaN\n",
    "#index3      1.0        10.0\n",
    "df.drop(['index1','index2'], axis=0)\n",
    "#        column1name  column2name\n",
    "#index3      3.0         30.0\n",
    "df.drop(['column1name'], axis=1)\n",
    "#        column1name  \n",
    "#index1      1.0        \n",
    "#index2      2.0         \n",
    "#index3      3.0 \n",
    "del df['column1name'] #delete 'column1name' column\n",
    "df.replace(10, 100,inplace=True) #replace 10 values in df with 100 #([100,1002], np.nan, inplace=True)\n",
    "#        column1name  column2name\n",
    "#index1      1.0         100.0\n",
    "#index2      2.0         20.0\n",
    "#index3      3.0         30.0\n",
    "df.replace([1,2], np.nan, inplace=True)\n",
    "#        column1name  column2name\n",
    "#index1      NaN         10.0\n",
    "#index2      NaN         20.0\n",
    "#index3      3.0         30.0\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,2],[2,4],[3,6],[4,10]],columns=['column1name','column2name'], dtype='float')\n",
    "## map is used on df['column1name']\n",
    "## applymap is used on df\n",
    "## apply is used on df and df['column1name']\n",
    "df['column1name'].map(np.log)#log the values in df['column1name']\n",
    "#0    0.000000\n",
    "#1    0.693147\n",
    "#2    1.098612\n",
    "#3    1.386294\n",
    "df['column1name'].map({1:'A',3:'B'})\n",
    "#0      A\n",
    "#1    NaN\n",
    "#2      B\n",
    "#3    NaN\n",
    "df['column1name'].map(str.lower)#(str.upper)          \n",
    "df['column1name'].map(lambda x: '%.2f' % x)\n",
    "df['column1name'].apply(lambda x: '%.2f' % x)          \n",
    "#0    1.00\n",
    "#1    2.00\n",
    "#2    3.00\n",
    "#3    4.00\n",
    "df.applymap(lambda x: '%.2f' % x)\n",
    "df['column1name'].map(lambda x : int(x))\n",
    "df['column1name'].apply(lambda x : int(x))\n",
    "df.apply(lambda x: x.max(), axis=0)\n",
    "#column1name     4.0\n",
    "#column2name    10.0\n",
    "df.apply(lambda x: x.min(), axis=0)\n",
    "df.apply(lambda x : x.dropna(), axis=1) # delete a row that all elements are nan\n",
    "def g(x): \n",
    "    return Series([x.min(),x.max()], index=('min','max'))\n",
    "df.apply(g)\n",
    "#      column1name  column2name\n",
    "#min     1.0           2.0\n",
    "#max     4.0          10.0\n",
    "df.apply(pd.value_counts)\n",
    "#     column1name  column2name\n",
    "#1.0      1.0         NaN\n",
    "#2.0      1.0         1.0\n",
    "#3.0      1.0         NaN\n",
    "#4.0      1.0         1.0\n",
    "#6.0      NaN         1.0\n",
    "#10.0     NaN         1.0\n",
    "df.apply(pd.value_counts).fillna(0)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~          \n",
    "df=pd.DataFrame({'product_id':[2,1,3],'user_id':[1,1,1]})\n",
    "df1=pd.DataFrame({'product_id':[3, 2],'user_id':[1,2] })\n",
    "          \n",
    "def function1(x):\n",
    "    y= df1[df1.user_id==x].product_id.max()\n",
    "    return y\n",
    "df.user_id.apply(function1)\n",
    "#0    3\n",
    "#1    3\n",
    "#2    3\n",
    "#Name: user_id, dtype: int64\n",
    "          \n",
    "map1=df.groupby('product_id')['user_id'].size()\n",
    "df1['product_id'].map(map1)\n",
    "#0    1\n",
    "#1    1\n",
    "#Name: product_id, dtype: int64\n",
    "\n",
    "A=df.groupby('user_id')['product_id'].max()\n",
    "df1.user_id.map(A)\n",
    "#0    3.0\n",
    "#1    NaN\n",
    "#Name: user_id, dtype: float64\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \n",
    "df=pd.DataFrame({'price':[1,2,3,5],'sub_area':['A', 'C', 'B', 'A']})\n",
    "A=df.groupby('sub_area')['price'].median()\n",
    "#sub_area\n",
    "#A    3\n",
    "#B    3\n",
    "#C    2\n",
    "#Name: price, dtype: int64\n",
    "bins = np.linspace(A.values.min(),A.values.max(),4)#array([ 2.,  2.33333333,  2.66666667,  3.])\n",
    "s=np.digitize(A.values, bins)#array([4, 4, 1], dtype=int64)\n",
    "zip(A.index, s)#[('A', 4), ('B', 4), ('C', 1)]\n",
    "df[\"sub_area\"].map(dict(zip(A.index, s)))\n",
    "#0    4\n",
    "#1    1\n",
    "#2    4\n",
    "#3    4\n",
    "#Name: sub_area, dtype: int64\n",
    "####################################################################################\n",
    "df=pd.DataFrame([[1,2],[3,4],[5,6]],\n",
    "                index=pd.MultiIndex.from_arrays([['index_1','index_2','index_3'],['index_A','index_C','index_B']], \n",
    "            names=['indexlevel1', 'indexlevel2']),\n",
    "                columns=pd.MultiIndex.from_arrays([['column1','column2'],['columnA','columnB']], \n",
    "            names=['columnlevel1', 'columnlevel2']))\n",
    "#             columnlevel1   column1   column2\n",
    "#             columnlevel2   columnA   columnB\n",
    "#indexlevel1  indexlevel2\n",
    "#index_1      index_A          1        2\n",
    "#index_2      index_C          3        4\n",
    "#index_3      index_B          5        6\n",
    "\n",
    "#df has two-level index, two-level columns\n",
    "df.index.names     #FrozenList([u'indexlevel1', u'indexlevel2'])\n",
    "df.columns.names   #FrozenList([u'columnlevel1', u'columnlevel2'])\n",
    "df.swaplevel(0,1,axis=0)#(i, j, axis), Swap levels i and j in a Multilevel on a particular axis\n",
    "#             columnlevel1   column1   column2\n",
    "#             columnlevel2   columnA   columnB\n",
    "#indexlevel2  indexlevel1\n",
    "#index_A      index_1          1         2\n",
    "#index_C      index_2          3         4\n",
    "#index_B      index_3          5         6\n",
    "\n",
    "\n",
    "df.sortlevel(1) #sort the level 2 index# original is to sort level 1 index\n",
    "#             columnlevel1   column1   column2\n",
    "#             columnlevel2   columnA   columnB\n",
    "#indexlevel1  indexlevel2\n",
    "#index_1      index_A          1        2\n",
    "#index_3      index_B          5        6\n",
    "#index_2      index_C          3        4\n",
    "df.sum(level='indexlevel1')#the index level become only level 1\n",
    "#columnlevel1   column1   column2\n",
    "#columnlevel2   columnA   columnB\n",
    "#indexlevel1\n",
    "#index_1           1        2\n",
    "#index_2           3        4\n",
    "#index_3           5        6\n",
    "df.sum(level='columnlevel1', axis=1)# the column level only become column 1 level\n",
    "#             columnlevel1   column1   column2\n",
    "#indexlevel1  indexlevel2\n",
    "#index_1      index_A          1        2\n",
    "#index_2      index_C          3        4\n",
    "#index_3      index_B          5        6\n",
    "####################################################################################\n",
    "df=pd.DataFrame([1,2,3,4],\n",
    "                index=pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),('two', 'a'), ('two', 'b')],names=['indexlevel1', 'indexlevel2']))\n",
    "#                         0\n",
    "#indexlevel1 indexlevel2\n",
    "#one            a        1.0\n",
    "#               b        2.0\n",
    "#two            a        3.0\n",
    "#               b        4.0\n",
    "df.unstack(0)\n",
    "#                0\n",
    "#indexlevel1    one  two\n",
    "#indexlevel2\n",
    "#a               1   3\n",
    "#b               2   4\n",
    "df.unstack(1)\n",
    "df.unstack()\n",
    "df.unstack('indexlevel2')\n",
    "#                0\n",
    "#indexlevel2     a  b\n",
    "#indexlevel1\n",
    "#one             1  2\n",
    "#two             3  4\n",
    "\n",
    "df=pd.DataFrame([[1,2,3,4],[5,6,7,8]],\n",
    "                columns=pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'),('two', 'a'), ('two', 'b')], \n",
    "                                                  names=['columnlevel1', 'columnlevel2']))              \n",
    "#columnlevel1     one    two\n",
    "#columnlevel2    a  b   a  b\n",
    "#0               1  2   3  4\n",
    "#1               5  6   7  8\n",
    "df.unstack(0)\n",
    "df.unstack()\n",
    "#columnlevel1  columnlevel2   \n",
    "#one           a             0    1\n",
    "#                            1    5\n",
    "#              b             0    2\n",
    "#                            1    6\n",
    "#two           a             0    3\n",
    "#                            1    7\n",
    " #             b             0    4\n",
    "#                            1    8\n",
    "df=pd.DataFrame([[1,2],[3,4]],index=['a','b'], columns=['one','two'])\n",
    "#    one  two\n",
    "#a    1    2\n",
    "#b    3    4\n",
    "df.stack()\n",
    "#a  one    1\n",
    "#   two    2\n",
    "#b  one    3\n",
    "#   two    4\n",
    "####################################################################################\n",
    "df1=pd.DataFrame([[1,2],[3,4]], columns=['one','two'])\n",
    "#   one  two\n",
    "#0   1    2\n",
    "#1   3    4\n",
    "df2=pd.DataFrame([[10,np.nan],[30,40]], columns=['one','two'])\n",
    "#    one   two\n",
    "#0   10    NaN\n",
    "#1   30    40\n",
    "\n",
    "df1.append(df2,ignore_index=True)\n",
    "#    one  two\n",
    "#0    1   2.0\n",
    "#1    3   4.0\n",
    "#2   10   NaN\n",
    "#3   30  40.0\n",
    "df1.align(df2, join='inner')\n",
    "#(   one  two\n",
    "# 0    1    2\n",
    "# 1    3    4,    one   two\n",
    "# 0   10   NaN\n",
    "# 1   30  40.0)\n",
    "          \n",
    "df1+df2\n",
    "df1.add(df2)\n",
    "#    one   two\n",
    "#0   11    2\n",
    "#1   33    44\n",
    "df1.add(df2, fill_value=0)\n",
    "#    one   two\n",
    "#0   11    2\n",
    "#1   33    44\n",
    "df1.sub(df2)\n",
    "df1.subtract(df2)\n",
    "#    one   two\n",
    "#0   -9    NaN\n",
    "#1   -27   -36\n",
    "df1.multiply(df2)\n",
    "df1.div(df2)#the values in df1 / the values in df2\n",
    "df1.divide(df2)\n",
    "\n",
    "df1.sum(1)\n",
    "#0    3\n",
    "#1    7\n",
    "df1.div(df1.sum(1), axis=0)##normalize each row\n",
    "df1.T\n",
    "#      0  1\n",
    "#one   1  3\n",
    "#two   2  4\n",
    "####################################################################################\n",
    "# find out which features have high collinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "df=pd.DataFrame([[1,2,50],[2, 4, 10]],\n",
    "             columns=['column1name','column2name','column3name'], dtype='float')\n",
    "variables = df.columns\n",
    "for var in df.columns:\n",
    "    new_vif = variance_inflation_factor(df[variables].values,df.columns.get_loc(var))\n",
    "    print 'Feature {}  |  VIF={}'.format(var,new_vif )\n",
    "#Feature column1name  |  VIF=inf\n",
    "#Feature column2name  |  VIF=inf\n",
    "#Feature column3name  |  VIF=1.6049382716\n",
    "#if VIF is greater than 5, it means this variable is highly collinear with the other explanatory variables\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "dropped=True\n",
    "r=1\n",
    "drop_list=[]\n",
    "while dropped:\n",
    "    print '#############################'\n",
    "    print 'Round : {}'.format(r)\n",
    "    dropped=False\n",
    "    vif = []\n",
    "    variables = df.columns\n",
    "    for var in df.columns:\n",
    "        new_vif = variance_inflation_factor(df[variables].values,df.columns.get_loc(var))\n",
    "        print '{}  |  VIF={}'.format(var,new_vif )\n",
    "        vif.append(new_vif)\n",
    "   \n",
    "    max_vif = max(vif)\n",
    "    if max_vif > 10:\n",
    "        maxloc=np.where(np.in1d(vif,max_vif))\n",
    "        #maxloc=np.argmax(vif)\n",
    "        print('Dropping {} with vif={}').format(df.columns[maxloc],max_vif)\n",
    "        #drop_list.append(df.columns[maxloc])\n",
    "        #joblib.dump(drop_list,'drop_list.pkl')\n",
    "        df = df.drop(df.columns[maxloc], axis=1)\n",
    "        dropped=True\n",
    "        r+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=pd.Series([10,5,20,15],index=['indexa','indexb','indexc','indexd'])\n",
    "#indexa    10\n",
    "#indexb     5\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "#dtype: int64\n",
    "\n",
    "s[::2]   #show every other row\n",
    "#indexa    10\n",
    "#indexc    20\n",
    "s.values #array([10,  5, 20, 15], dtype=int64)\n",
    "s.index  #Index([u'indexa', u'indexb', u'indexc', u'indexd'], dtype='object')\n",
    "s.reindex(['indexa','a','indexb'], fill_value=0)#(range(4), method='ffill)\n",
    "#a          0\n",
    "#indexa    10\n",
    "s.name='population'\n",
    "s.index.name='indexname'\n",
    "s\n",
    "#indexname\n",
    "#indexa    10\n",
    "#indexb     5\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "#Name: population, dtype: int64\n",
    "s.describe()\n",
    "#count     4.000000\n",
    "#mean     12.500000\n",
    "#std       6.454972\n",
    "#min       5.000000\n",
    "#25%       8.750000\n",
    "#50%      12.500000\n",
    "#75%      16.250000\n",
    "#max      20.000000\n",
    "s.index.is_unique #True    # see if the index are unique (the same index wont have more than one value)\n",
    "s.unique()        #array([10,  5, 20, 15], dtype=int64) \n",
    "s.rank(ascending=False)#the ranking of the largest value is 1\n",
    "#indexa    3.0\n",
    "#indexb    4.0\n",
    "#indexc    1.0\n",
    "#indexd    2.0\n",
    "s.order(ascending=False)\n",
    "s.sort_values(ascending=False)\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "#indexa    10\n",
    "#indexb     5\n",
    "s.sort_index()    #Sort by the index along either axis\n",
    "#indexa    10\n",
    "#indexb     5\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "s.value_counts(sort=True, ascending=False)\n",
    "#15    1\n",
    "#5     1\n",
    "#20    1\n",
    "#10    1\n",
    "s.quantile([0,0.5,1])# 0(minvalue),1(maxvalue),0.5(median value)\n",
    "#0.0     5.0\n",
    "#0.5    12.5\n",
    "#1.0    20.0\n",
    "s.isnull()\n",
    "#indexa    False\n",
    "#indexb    False\n",
    "#indexc    False\n",
    "#indexd    False\n",
    "s.notnull()\n",
    "#indexa    True\n",
    "#indexb    True\n",
    "#indexc    True\n",
    "#indexd    True\n",
    "s.dropna()\n",
    "#indexa    10\n",
    "#indexb     5\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "s.drop(['indexa','indexb'])\n",
    "#indexc    20\n",
    "#indexd    15\n",
    "s.fillna(s.mean())\n",
    "s.fillna(s.median())\n",
    "s.isin([5,10])\n",
    "#indexa     True\n",
    "#indexb     True\n",
    "#indexc    False\n",
    "#indexd    False\n",
    "s.replace(10,100)# replace value 10 with 100#\n",
    "s.replace([10,5], 100)# replace value 10 and 5 with 100\n",
    "s.replace({10:100,5:50})#replace 10 with 100, 5 with 50\n",
    "s.replace([10,5],[100,50])#replace 10 with 100, 5 with 50\n",
    "#########################################################\n",
    "s=pd.Series([1,2,3],index=[['indexa','indexb','indexc'],['index1','index2', 'index3']])\n",
    "#indexa  index1    1\n",
    "#indexb  index2    2\n",
    "#indexc  index3    3\n",
    "s['indexa']#=s[0] show the values in index a\n",
    "#index1    1\n",
    "s[0]\n",
    "s['indexa','index1']\n",
    "#1\n",
    "s['indexa':'indexb']\n",
    "s.ix[['indexa','indexb']]\n",
    "#indexa  index1    1\n",
    "#indexb  index2    2\n",
    "s[:,'index1']#show the values in index1\n",
    "#indexa    1\n",
    "s.unstack()\n",
    "#         index1  index2  index3\n",
    "#indexa    1.0     NaN     NaN\n",
    "#indexb    NaN     2.0     NaN\n",
    "#indexc    NaN     NaN     3.0\n",
    "#########################################################\n",
    "s1=pd.Series([1,2],index=['indexa','indexb'])\n",
    "s2=pd.Series([10,20],index=['indexa','index1'])\n",
    "s1.add(s2, fill_value=0)\n",
    "#index1    20.0\n",
    "#indexa    11.0\n",
    "#indexb     2.0\n",
    "#########################################################\n",
    "s.plot()\n",
    "s1=s.copy()#s1 is  a copy of s\n",
    "s.to_frame()#Convert Series to DataFrame\n",
    "s.to_csv('xx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame([['a',1,2],['b',3,4],['c',5,6]],columns=['column1name','column2name','column3name'])\n",
    "#    column1name  column2name  column3name\n",
    "#0        a            1           2\n",
    "#1        b            3           4\n",
    "#2        c            5           6\n",
    "pd.melt(df, id_vars=['column1name'], value_vars=['column2name'], var_name='myVarname', value_name='myValuename')\n",
    "#    column1name    myVarname    myValuename\n",
    "#0      a         column2name       1\n",
    "#1      b         column2name       3\n",
    "#2      c         column2name       5\n",
    "#########################################################\n",
    "df=DataFrame({'A': ['a', 'b', 'a'], 'B': [1, 2, 3]})\n",
    "pd.get_dummies(df, prefix='X_')\n",
    "#    B  X__a   X__b\n",
    "#0   1    1      0\n",
    "#1   2    0      1\n",
    "#2   3    1      0\n",
    "s=pd.Series(['a','b','a'])\n",
    "pd.get_dummies(s)\n",
    "#   a   b\n",
    "#0  1   0\n",
    "#1  0   1\n",
    "#2  1   0\n",
    "#########################################################\n",
    "df=pd.DataFrame([21,23,25,24],columns=['column1name'], dtype='float')\n",
    "bins=[20,22,24,26]\n",
    "group_names = ['Low', 'Good', 'Great']\n",
    "A=pd.cut(df['column1name'], bins,right=True)\n",
    "#0    (20, 22]\n",
    "#1    (22, 24]\n",
    "#2    (24, 26]\n",
    "#3    (22, 24]\n",
    "#Name: column1name, dtype: category\n",
    "#Categories (3, object): [(20, 22] < (22, 24] < (24, 26]]\n",
    "pd.value_counts(A)\n",
    "#(22, 24]    2\n",
    "#(24, 26]    1\n",
    "#(20, 22]    1\n",
    "#Name: column1name, dtype: int64\n",
    "pd.cut(df['column1name'], bins,right=False)\n",
    "#0    [20, 22)\n",
    "#1    [22, 24)\n",
    "#2    [24, 26)\n",
    "#3    [24, 26)\n",
    "#Name: column1name, dtype: category\n",
    "#Categories (3, object): [[20, 22) < [22, 24) < [24, 26)]\n",
    "A=pd.cut(df['column1name'], bins,labels=group_names,right=True)\n",
    "#0      Low\n",
    "#1     Good\n",
    "#2    Great\n",
    "#3     Good\n",
    "#Name: column1name, dtype: category\n",
    "#Categories (3, object): [Low < Good < Great]\n",
    "pd.value_counts(A)\n",
    "#Good     2\n",
    "#Great    1\n",
    "#Low      1\n",
    "#Name: column1name, dtype: int64\n",
    "pd.cut(df['column1name'],3, precision=1)\n",
    "#0      (21, 22.3]\n",
    "#1    (22.3, 23.7]\n",
    "#2      (23.7, 25]\n",
    "#3      (23.7, 25]\n",
    "#Name: column1name, dtype: category\n",
    "#Categories (3, object): [(21, 22.3] < (22.3, 23.7] < (23.7, 25]]\n",
    "pd.qcut(df['column1name'],3) #each bin interval has the same number of data points\n",
    "#0    [21, 23]\n",
    "#1    [21, 23]\n",
    "#2    (24, 25]\n",
    "#3    (23, 24]\n",
    "#Name: column1name, dtype: category\n",
    "#Categories (3, object): [[21, 23] < (23, 24] < (24, 25]]\n",
    "#########################################################\n",
    "df=pd.DataFrame([1,2,3,4,5],columns=['column1name'], dtype='float')\n",
    "####data average method \n",
    "pd.rolling_mean(df['column1name'], 3, min_periods=2)#it becomes NaN, NaN, 2(mean of 1+2+3), 3, 4#take 3 values to average\n",
    "#min_periods=2 for the values are NaN, recalculate by taking 2 values to average, it become NaN 1.5, 2,3,4\n",
    "#0    NaN\n",
    "#1    1.5\n",
    "#2    2.0\n",
    "#3    3.0\n",
    "#4    4.0\n",
    "#Name: column1name, dtype: float64\n",
    "pd.rolling_std(df['column1name'], 3, min_periods=2)#Moving standard deviation\n",
    "\n",
    "pd.ewma(df['column1name'], span=2)#exponentially weighted moving average\n",
    "#alpha=2/1+span, a=1-alpha\n",
    "#x1, (x2+a*x1)/1+a, (x3+a*x2+(a*a)*x1)/(1+a+a^2), ....#more closer to data distribution than rolling mean\n",
    "#0    1.000000\n",
    "#1    1.750000\n",
    "#2    2.615385\n",
    "#3    3.550000\n",
    "#4    4.520661\n",
    "#Name: column1name, dtype: float64\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "x=[1, 2, 3, 4]\n",
    "percentileofscore(x, 3)  #75.0 #75 % of members are <= 3\n",
    "\n",
    "functionA=lambda x: percentileofscore(x, 3)\n",
    "pd.rolling_apply(df['column1name'], 2, functionA)\n",
    "#0      NaN\n",
    "#1    100.0  #how many percentage of 1,2 less than or equal to 3 : 100\n",
    "#2    100.0  #how many percentage of 2,3 less than or equal to 3 : 100\n",
    "#3     50.0  #how many percentage of 3,4 less than or equal to 3 : 50\n",
    "#4      0.0  #how many percentage of 4,5 less than or equal to 3 : 0\n",
    "\n",
    "df=pd.DataFrame({'column1name':[1,2,3,4,5],'column2name':[10,20,35,40,50]})\n",
    "pd.rolling_corr(df['column1name'], df['column2name'], 3, min_periods=2)#moving sample correlation\n",
    "#take every 3 value of df['column1name'] and df['column2name'], calculate their correlation\n",
    "#0         NaN\n",
    "#1    1.000000\n",
    "#2    0.993399\n",
    "#3    0.960769\n",
    "#4    0.981981\n",
    "# fit linear regression\n",
    "model=pd.ols(y=df['column1name'], x=df['column2name'], window=3)#every 3th row show the linear slope and intercept\n",
    "model.beta\n",
    "#        x     intercept\n",
    "#2   0.078947  0.289474\n",
    "#3   0.092308  0.076923\n",
    "#4   0.128571  -1.357143\n",
    "#########################################################\n",
    "df1=pd.DataFrame({'df1column2':['a','b','a'],'df1column1':[1,2,3]})\n",
    "df2=pd.DataFrame({'df2column1':[1,3,2]}, index=['a','b','c'])\n",
    "#df1                                    df2\n",
    "#    df1column1       df1column2            df2column1\n",
    "#0       1                a             a       1\n",
    "#1       2                b             b       3\n",
    "#2       3                a             c       2\n",
    "pd.merge(df1, df2, left_on='df1column2', right_index=True)\n",
    "#   df1column1         df1column2   df2column1\n",
    "#0      1                    a          1\n",
    "#2      3                    a          1\n",
    "#1      2                    b          3\n",
    "pd.merge(df1, df2, left_on='df1column2', right_index=True,how='outer')\n",
    "pd.merge(df1, df2, left_on='df1column2', right_index=True,how='right')#based on df2 key\n",
    "#   df1column1         df1column2   df2column1\n",
    "#0      1                    a          1\n",
    "#2      3                    a          1\n",
    "#1      2                    b          3\n",
    "#2     NaN                   c          2\n",
    "pd.merge(df1, df2, left_on='df1column2', right_index=True,how='left')#based on df1 key\n",
    "#   df1column1         df1column2   df2column1\n",
    "#0      1                    a          1\n",
    "#1      2                    b          3\n",
    "#2      3                    a          1\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df1=pd.DataFrame({'key':['a','b','a'],'df1column1':[1,2,3]})\n",
    "df2=pd.DataFrame({'df2column1':[1,3,2],'key':['a','b','c']})\n",
    "#df1                                    df2\n",
    "#    df1column1          key                df2column1   key\n",
    "#0       1                a             0       1         a\n",
    "#1       2                b             1       3         b\n",
    "#2       3                a             2       2         c\n",
    "pd.merge(df1, df2, on='key')#df1column1 key df2column1,\n",
    "#   df1column1              key     df2column1\n",
    "#0      1                    a          1\n",
    "#2      3                    a          1\n",
    "#1      2                    b          3\n",
    "df1.join(df2, lsuffix='_left', rsuffix='_right')#df1column1 key_left df2column1 key_right\n",
    "#    df1column1   key_left   df2column1  key_right\n",
    "#0       1          a           1           a\n",
    "#1       2          b           3           b\n",
    "#2       3          a           2           c\n",
    "\n",
    "df1=pd.DataFrame({'time':['2011-02-25', '2011-02-28', '2011-02-20'],'df1column':[1,2,3]})\n",
    "df2=pd.DataFrame({'time':['2011-02-25', '2011-02-20', '2011-02-21'],'df2column':[1,3,3]})\n",
    "pd.merge_ordered(df1, df2, on='time', how='left')\n",
    "#      df1column       time        df2column\n",
    "#0        3         2011-02-20        3.0\n",
    "#1        1         2011-02-25        1.0\n",
    "#2        2         2011-02-28        NaN\n",
    "#the time column is sorted\n",
    "pd.merge(df1, df2, on='time', how='left')\n",
    "#       df1column       time         df2column\n",
    "#0           1       2011-02-25         1.0\n",
    "#1           2       2011-02-28         NaN\n",
    "#2           3       2011-02-20         3.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df1=pd.DataFrame({'common1':['a','b','a'],'common2':['F', 'M', 'F'],'df1column1':[1,2,3]})\n",
    "df2=pd.DataFrame({'df2column1':[1,3,2],'common1':['a','b','c'],'common2':['F','M','M']})\n",
    "#df1                                    df2\n",
    "#    common1  common2  df1column1       common1   common2  df2column1\n",
    "#0       a      F          1        0      a         F       1\n",
    "#1       b      M          2        1      b         M       3\n",
    "#2       a      F          3        2      c         M       2\n",
    "pd.merge(df1, df2, on=['common1','common2'])\n",
    "#  common1  common2  df1column1  df2column1\n",
    "#0   a        F          1           1\n",
    "#1   a        F          3           1\n",
    "#2   b        M          2           3\n",
    "pd.merge(df1, df2, on='common1', suffixes=('_left','_right'))\n",
    "#  common1 common2_left df1column1  common2_right   df2column1\n",
    "#0   a        F            1           F               1\n",
    "#1   a        F            3           F               1\n",
    "#2   b        M            2           M               3\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#if the columns in df1 are df1column1 and key1(a, b, a..), index is 0,1,2\n",
    "#if the columns in df2 are df2column1 and key2(a, b, b..), index is 0,1,2\n",
    "pd.merge(df1, df2, left_on='key1',right_on='key2')\n",
    "#key1 df1column1 df2column1 key2\n",
    "# a                          a  \n",
    "# a                          a \n",
    "# b                          b \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df1=pd.DataFrame({'df1column1':[1,2,3]}, index=['a','b','a'])\n",
    "df2=pd.DataFrame({'df2column1':[1,3,2]} ,index=['a','b','c'])\n",
    "#df1                            df2\n",
    "#    df1column1                 df2column1\n",
    "#a       1                    a       1\n",
    "#b       2                    b       3\n",
    "#a       3                    c       2\n",
    "pd.merge(df1, df2, left_index=True,right_index=True)\n",
    "df1.join(df2)\n",
    "#   df1column1  df2column1\n",
    "#a      1           1\n",
    "#a      3           1\n",
    "#b      2           3\n",
    "df1.join(df2, how='outer')\n",
    "#   df1column1  df2column1\n",
    "#a      1           1\n",
    "#a      3           1\n",
    "#b      2           3\n",
    "#c      NaN         2\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df1=pd.DataFrame({'df1column1':[1,2,3]})\n",
    "df2=pd.DataFrame({'df2column1':[1,3,2]})\n",
    "df1.join(df2).add_prefix('X_')       \n",
    "#   X_df1column1  X_df2column1\n",
    "#0      1             1\n",
    "#1      2             3\n",
    "#2      3             2\n",
    "#########################################################\n",
    "df1=pd.DataFrame({'df1column1':[1,2,3],'df1column2':[10,20,30]}, index=['a','b','c'])\n",
    "df2=pd.DataFrame({'df2column1':[100,300],'df2column2':[1000,3000]}, index=['a','c'])\n",
    "#df1                                 df2\n",
    "#   df1column1 df1column2            df2column1   df2column2\n",
    "#a     1         10               a      100          1000\n",
    "#b     2         20              \n",
    "#c     3         30               c      300          3000\n",
    "pd.concat([df1,df2], axis=1, keys=['L1','L2'], names=['columnlevel1','columnlevel2'])\n",
    "#'columnlevel1'       L1       L1              L2        L2\n",
    "#'columnlevel2'  df1column1  df1column2     df2column1  df2column2\n",
    "#   a                1          10             100        1000\n",
    "#   b                2          20             NaN        NaN\n",
    "#   c                3          30             300        3000\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df1=pd.DataFrame({'A':[1,3],'B':[2,4]})\n",
    "df2=pd.DataFrame({'A':[1]})\n",
    "#df1                                   df2\n",
    "#      A         B                      A\n",
    "#0     1         2               0      1          \n",
    "#1     3         4      \n",
    "pd.concat([df1, df2])\n",
    "#    a  b\n",
    "#0   1  2\n",
    "#1   3  4\n",
    "#0  1  NaN\n",
    "pd.concat([df1, df2], ignore_index=True)\n",
    "df1.append(df2,ignore_index=True)\n",
    "#    a  b\n",
    "#0   1  2\n",
    "#1   3  4\n",
    "#2   1  NaN\n",
    "pd.concat([df1, df2], ignore_index=True).fillna(method = 'ffill')\n",
    "#    a  b\n",
    "#0   1  2\n",
    "#1   3  4\n",
    "#2   1  4\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`\n",
    "s1=pd.Series([1,2], index=['a','c'])\n",
    "s2=pd.Series([3,4], index=['a','b'])\n",
    "# s1    s2     \n",
    "# a 1   a 3   \n",
    "# c 2   b 4                            \n",
    "pd.concat([s1,s2])\n",
    "#a    1\n",
    "#c    2\n",
    "#a    3\n",
    "#b    4\n",
    "pd.concat([s1,s2], axis=1)\n",
    "#   0     1\n",
    "#a  1     3\n",
    "#b  NaN   4\n",
    "#c  2.0  NaN\n",
    "pd.concat([s1,s2], axis=1,join='inner')#join='inner', delete a row that has NaN\n",
    "#  0    1\n",
    "#a 1   3\n",
    "pd.concat([s1,s2], axis=1,join='inner', join_axes=[['a','b']]) #select which index want to show\n",
    "#   0     1\n",
    "#a  1     3\n",
    "#b  NaN   4\n",
    " \n",
    "pd.concat([s1,s2], keys=['one','two'])\n",
    "#one  a    1\n",
    "#     c    2\n",
    "#two  a    3\n",
    "#     b    4\n",
    "pd.concat([s1,s2], axis=1, keys=['one','two'])\n",
    "#  one   two\n",
    "#a  1     3\n",
    "#b  Nan   4\n",
    "#c  2    NaN\n",
    "#########################################################\n",
    "df1=pd.DataFrame({'A':[np.nan, 3],'B':[2,4]})\n",
    "df2=pd.DataFrame({'A':[1,100,200]})\n",
    "#df1                                   df2\n",
    "#      A         B                     A\n",
    "#0     NaN       2               0     1          \n",
    "#1     3         4               1     100\n",
    "#                                2     200\n",
    "                                 \n",
    "df1.combine_first(df2)#patch miss data of df1 from df2\n",
    "#      A         B              \n",
    "#0     1         2                   \n",
    "#1     3         4  \n",
    "#2   200.0      NaN\n",
    "df1.update(df2, overwrite=True)\n",
    "df1\n",
    "#      A         B              \n",
    "#0     1         2                   \n",
    "#1   100         4 \n",
    "#########################################################\n",
    "df=pd.DataFrame({'column1':['a','a','b'],'column2':[10,20,30]})\n",
    "#df                            \n",
    "#    column1 column2          \n",
    "#0     a         1              \n",
    "#1     a         2              \n",
    "#2     b         3  \n",
    "pd.crosstab(df.column1, df.column2)#the unique values of df.column1 is the index,\n",
    "#column2   10    20   30\n",
    "#column1\n",
    "#a         1     1    0\n",
    "#b         0     0    1\n",
    "pd.crosstab(df.column1, df.column2, margins=True)\n",
    "#column2   10    20   30  All\n",
    "#column1\n",
    "#a         1     1    0   2\n",
    "#b         0     0    1   1\n",
    "#All       1     1    1   3\n",
    "#########################################################\n",
    "df=pd.DataFrame({'column1':['a','a','a','b','b','b'],'column2':[10,10,20,20,30,30],'column3':[1,2,3,4,5,6]})\n",
    "#   column1  column2  column3\n",
    "#0     a       10       1\n",
    "#1     a       10       2\n",
    "#2     a       20       3\n",
    "#3     b       20       4\n",
    "#4     b       30       5\n",
    "#5     b       30       6\n",
    "df['column3'].groupby(df['column1']).mean()\n",
    "df.groupby(df['column1'])['column3'].mean()\n",
    "#column1\n",
    "#a    2\n",
    "#b    5\n",
    "df.groupby(df['column1'],as_index=False)['column3'].mean()\n",
    "#      column1  column3\n",
    "#0        a        2\n",
    "#1        b        5\n",
    "df['column3'].groupby([df['column1'],df['column2']]).mean()\n",
    "#column1  column2\n",
    "#a        10         1.5\n",
    "#         20         3.0\n",
    "#b        20         4.0\n",
    "#         30         5.5\n",
    "#Name: column3, dtype: float64\n",
    "df.groupby(df['column1']).mean()\n",
    "#           column2      column3\n",
    "#column1\n",
    "#a         13.333333      2.0\n",
    "#b         26.666667      5.0\n",
    "df.groupby('column1').mean().add_prefix('A_') \n",
    "#          A_column2     A_column3\n",
    "#column1\n",
    "#a         13.333333      2.0\n",
    "#b         26.666667      5.0\n",
    "df.groupby('column1').size()#the times appearing\n",
    "#column1\n",
    "#a    3\n",
    "#b    3\n",
    "df.groupby('column1').quantile(0.5)#(or 1,0, 0.5)\n",
    "#0.5       column2      column3\n",
    "#column1\n",
    "#a         10            2.0\n",
    "#b         30            5.0\n",
    "df.groupby('column1').describe()\n",
    "df.groupby('column1').agg(['mean', 'std'])\n",
    "#              column2           column3\n",
    "#          mean        std      mean       std\n",
    "#column1\n",
    "#a       13.333333   5.773503    2         1.0\n",
    "#b       26.666667   5.773503    5         1.0\n",
    "def functionA(x):\n",
    "    return x.max()-x.min()\n",
    "df.groupby('column1').agg(['mean', 'std','max','min','count',functionA, np.var, 'sum'])\n",
    "df.groupby('column1').agg([('meannewname','mean'), ('stdnewname','std')]) #change mean and std names\n",
    "df.groupby('column1').agg({'column2':['max','min'],'column3':'sum'})\n",
    "df.groupby('column1').agg(lambda x: len(np.unique(x)))\n",
    "df.groupby('column1').agg('count').value_counts()\n",
    "\n",
    "df.groupby('column1').transform(np.mean)#(functionA)\n",
    "#    column2     column3\n",
    "#0   13.333333      2\n",
    "#1   13.333333      2\n",
    "#2   13.333333      2\n",
    "#3   26.666667      5\n",
    "#4   26.666667      5\n",
    "#5   26.666667      5\n",
    "def functionB(df, n=2, column='column2'):\n",
    "    return df.sort_index(by=column)[-n:]\n",
    "df.groupby('column1').apply(functionB, n=2, column='column3')\n",
    "#          column1   column2   column3\n",
    "#column1\n",
    "#a       1    a        10         2 \n",
    "#        2    a        20         3\n",
    "#b       4    b        30         5\n",
    "#        5    b        30         6\n",
    "def functionC(x):\n",
    "    return {'min':x.min(), 'max':x.max()}\n",
    "df.groupby('column1').apply(functionC)\n",
    "#column1\n",
    "#a    {u'max': [u'a', 20, 3], u'min': [u'a', 10, 1]}\n",
    "#b    {u'max': [u'b', 30, 6], u'min': [u'b', 20, 4]}\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=pd.DataFrame(np.arange(1,10).reshape(3,3), index=['Joy','Steve','May'],columns=['F','F','M'])\n",
    "#         F   F  M\n",
    "#Joy      1   2   3\n",
    "#Steve    4   5   6 \n",
    "#May      7   8   9\n",
    "A={'F':'femal','M':'male',} \n",
    "df.groupby(A, axis=1).sum()\n",
    "#      femal  male\n",
    "#Joy    3      3\n",
    "#Steve  9      6\n",
    "#May    15     9\n",
    "df.groupby(len).sum() \n",
    "#         F  F  M\n",
    "#3        8  10 12\n",
    "#5        4   5  6\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=pd.DataFrame({'product_id':[2,1,3],'user_id':[1,1,1], 'order_id':[1,1,2] })\n",
    "df.groupby(['user_id', 'order_id'])['product_id'].apply(list).reset_index()\n",
    "#   user_id  order_id  product_id\n",
    "#0    1         1      [2, 1]\n",
    "#1    1         2      [3]\n",
    "\n",
    "df.groupby('user_id')['product_id'].apply(lambda x: ' '.join([str(e) for e in set(x)])).reset_index()\n",
    "#     user_id  product_id\n",
    "#0       1        1 2 3\n",
    "\n",
    "def function1(x):\n",
    "    return x.nunique()\n",
    "df.groupby('user_id')['product_id'].apply(function1)\n",
    "#user_id\n",
    "#1    3\n",
    "#Name: product_id, dtype: int64\n",
    "\n",
    "df.groupby('user_id')['product_id'].agg(max)\n",
    "#user_id\n",
    "#1    3\n",
    "#Name: product_id, dtype: int64\n",
    "df.groupby('user_id')['product_id'].transform(max)\n",
    "#0    3\n",
    "#1    3\n",
    "#2    3\n",
    "#Name: product_id, dtype: int64\n",
    "#########################################################\n",
    "df=pd.DataFrame({'column1':[1,2,3],'column2':[10,20,30],'column3':['F','F','M'],'column4':['one','two','two'],\n",
    "                 'column5':['A','A','B']})\n",
    "#   column1  column2  column3  column4  column5\n",
    "#0     1       10        F       one       A  \n",
    "#1     2       20        F       two       A  \n",
    "#2     3       30        M       two       B     \n",
    "    \n",
    "df.pivot_table(['column1', 'column2'], index=['column3', 'column4'], columns=['column5'], aggfunc=np.sum)\n",
    "#                  column1     column2\n",
    "#         column5   A    B     A    B\n",
    "#column3  column4\n",
    "#F         one     1.0  NaN   10.0  NaN \n",
    "#          two     2.0  NaN   20.0  NaN\n",
    "#M         two     NaN  3.0   NaN   30.0\n",
    "df.pivot_table(['column1', 'column2'], index=['column3', 'column4'], columns=['column5'], aggfunc=np.sum, \n",
    "               margins=True, fill_value=0)\n",
    "\n",
    "#aggfunc=len , each cell is how many values containing in this cell \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "from dateutil.parser import parse\n",
    "from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "import pytz #for timezone setting\n",
    "\n",
    "datetime.now()           #datetime.datetime(2017, 8, 28, 11, 22, 34, 66000)\n",
    "datetime.now().year      #2017\n",
    "datetime.now().month\n",
    "datetime.now().day       #28\n",
    "datetime.now().minute\n",
    "datetime.now().second\n",
    "datetime.now().date()    #datetime.date(2017, 8, 28)\n",
    "datetime.now().time()    #datetime.time(11, 22, 34, 66000)\n",
    "datetime.now().strftime('%a, %d %b(%m) %Y %H:%M:%S') #'Mon, 28 Aug(08) 2017 11:34:33'\n",
    "datetime.now().replace(minute=0, second=0)           #datetime.datetime(2017, 8, 28, 11, 0, 0, 346000\n",
    "\n",
    "T1='2011-01-03';T2='1/3/2011'\n",
    "datetime.strptime(T1,'%Y-%m-%d')#datetime.datetime(2011, 1, 3, 0, 0)\n",
    "datetime.strptime(T2,'%m/%d/%Y')#datetime.datetime(2011, 1, 3, 0, 0)\n",
    "parse(T1)# datetime.datetime(2011, 1, 3, 0, 0)\n",
    "pd.to_datetime(T2)#Timestamp('2011-01-03 00:00:00')\n",
    "MonthBegin().rollforward(T1)#Timestamp('2011-02-01 00:00:00')\n",
    "MonthEnd().rollforward(T1)#Timestamp('2011-01-31 00:00:00')\n",
    "MonthBegin().rollback(T1)#Timestamp('2011-01-01 00:00:00')\n",
    "MonthEnd().rollback(T1)#Timestamp('2010-12-31 00:00:00')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "p=pd.Period('2007', freq='A-JAN');\n",
    "p.asfreq('M', how='start')#Period('2006-02', 'M')\n",
    "p.asfreq('M', how='end')#Period('2007-01', 'M')\n",
    "p.asfreq('B', how='start')#Period('2006-02-01', 'B')\n",
    "p.asfreq('B', how='end')#Period('2007-01-31', 'B')\n",
    "p.to_timestamp()#)#Period('2006-02-01', 'B')\n",
    "\n",
    "p=pd.period_range('2006','2008', freq='A-JAN')\n",
    "#PeriodIndex(['2006', '2007', '2008'], dtype='period[A-JAN]', freq='A-JAN')\n",
    "p.asfreq('M', how='end')\n",
    "#PeriodIndex(['2006-01', '2007-01', '2008-01'], dtype='period[M]', freq='M')\n",
    "p.asfreq('B', how='end')\n",
    "#PeriodIndex(['2006-01-31', '2007-01-31', '2008-01-31'], dtype='period[B]', freq='B')\n",
    "p.to_timestamp(how='end')\n",
    "#DatetimeIndex(['2006-01-31', '2007-01-31', '2008-01-31'], dtype='datetime64[ns]', freq='A-JAN')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "datetime(2011,1,2) - pd.offsets.BDay()#Timestamp('2010-12-31 00:00:00')\n",
    "datetime(2011,1,2) - pd.offsets.Day()#Timestamp('2011-01-01 00:00:00')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "A=pd.date_range('1/1/2000', periods=3, freq='M')#'2000-01-31', '2000-02-29', '2000-03-31'\n",
    "A.to_period()#PeriodIndex(['2000-01', '2000-02', '2000-03'], dtype='period[M]', freq='M')\n",
    "\n",
    "B=pd.period_range('1/1/2000', periods=3, freq='M')\n",
    "#PeriodIndex(['2000-01', '2000-02', '2000-03'], dtype='period[M]', freq='M')\n",
    "B.to_timestamp(how='end')\n",
    "#DatetimeIndex(['2000-01-31', '2000-02-29', '2000-03-31'], dtype='datetime64[ns]', freq='M')\n",
    "\n",
    "A=pd.date_range('1/1/2000', periods=3, freq='D')\n",
    "#'DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03'], dtype='datetime64[ns]', freq='D')\n",
    "A.to_period()#PeriodIndex(['2000-01-01', '2000-01-02', '2000-01-03'], dtype='period[D]', freq='D')\n",
    "A.to_period('M')#PeriodIndex(['2000-01', '2000-01', '2000-01'], dtype='period[M]', freq='M')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "[datetime(2011,1,2),datetime(2011,1,3),datetime(2011,1,4)]\n",
    "#[datetime.datetime(2011, 1, 2, 0, 0),\n",
    "# datetime.datetime(2011, 1, 3, 0, 0),\n",
    "# datetime.datetime(2011, 1, 4, 0, 0)]\n",
    "pd.DatetimeIndex(['1/2/2011','1/3/2011','1/4/2011'])\n",
    "#DatetimeIndex(['2011-01-02', '2011-01-03', '2011-01-04'], dtype='datetime64[ns]', freq=None)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#pd.date_range(start, end, periods, freq, normalize, tz)\n",
    "pd.date_range('1/2/2011', periods=3)\n",
    "pd.date_range('1/2/2011', periods=3,normalize=True) #if 1/1/2000 6:30:00, set to 1/1/2000 00:00:00, normalized to midnight\n",
    "pd.date_range('1/2/2011', periods=3,freq='D')\n",
    "pd.date_range('1/2/2011', '1/4/2011')\n",
    "pd.date_range(start='1/2/2011', periods=3)\n",
    "pd.date_range(end='1/4/2011', periods=3)\n",
    "#DatetimeIndex(['2011-01-02', '2011-01-03', '2011-01-04'], dtype='datetime64[ns]', freq='D')\n",
    "#freq='8h', '2D', '60T'\n",
    "#'D' calender day, 'B' business day, 'H' hourly, 'T' Minutely, 'S' secondly, 'L' msec, 'U': microsec\n",
    "# 'M'   last calendar day of each month, 'BM'   last buiness day of each month\n",
    "# 'MS' first calendar day of each month, 'BMS' first buiness day of each month\n",
    "#'W-MON', 'W-TUE', 'W-WED', 'W-THU', 'W-FRI', 'W-SAT', 'W-SUN' weekly on a given day\n",
    "#'WON-1MON', 'WON-2MON', 'WON-2MON'(the third Monday of each month)\n",
    "#'Q-JAN: on the dates of 01-31, 04-30, 07-31, 10-31\n",
    "#'Q-JAN',    'Q-FEB   :quartly dates on the last calendar day of every three month\n",
    "#'BQ-JAN',  'BQ-FEB   :quartly dates on the last business day of every three month\n",
    "#'QS-JAN',  'QD-FEB   :quartly dates on the first calendar day of every three month\n",
    "#'BQS-JAN','BQD-FEB   :quartly dates on the first business day of every three month\n",
    "#'A-JAN',    'A-FEB   :annual dates on the last calendar day of a given month ex: 'A-JAN' 01-31 every year\n",
    "#'BA-JAN',  'BA-FEB   :annual dates on the last business day of a given month\n",
    "#'AS-JAN',  'AS-FEB   :annual dates on the first calendar day of a given month\n",
    "#'BAS-JAN', 'BAS-FEB   :annual dates on the first business day of a given month\n",
    "#tz='UTC' (or 'US/Eastern') set time zone\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "ts=Series(np.arange(1,5), index=pd.date_range('1/2/2011', periods=4, freq='D'))\n",
    "#2011-01-02    1\n",
    "#2011-01-03    2\n",
    "#2011-01-04    3\n",
    "#2011-01-05    4\n",
    "#Freq: D, dtype: int32\n",
    "\n",
    "ts.truncate(after='1/3/2011')\n",
    "#2011-01-02    1\n",
    "#2011-01-03    2\n",
    "#Freq: D, dtype: int32\n",
    "ts['2011']#show all data in '2011'\n",
    "ts['2011-01']\n",
    "#2011-01-02    1\n",
    "#2011-01-03    2\n",
    "#2011-01-04    3\n",
    "#2011-01-05    4\n",
    "#Freq: D, dtype: int32\n",
    "ts.shift(2, freq='D')# data shift down by 2 days\n",
    "#2011-01-04    1\n",
    "#2011-01-05    2\n",
    "#2011-01-06    3\n",
    "#2011-01-07    4\n",
    "ts.groupby(MonthEnd().rollforward).mean()\n",
    "#2011-01-31    2.5\n",
    "ts.resample('2D', how='mean') #how='sum'\n",
    "ts.resample('2D', how='mean',closed='left')#closed='left'(default)\n",
    "ts.resample('2D', how='mean',label='left')#label='left'(default)\n",
    "#2011-01-02    1.5\n",
    "#2011-01-04    3.5\n",
    "#Freq: 2D, dtype: float64\n",
    "#how='sum'\n",
    "# how='ohlc' show open(first value), high(max value), low(min value), close(last value)\n",
    "#how='prod' all values are multiplied together\n",
    "ts.resample('2D', how='mean',closed='right')\n",
    "#2010-12-31    1.0\n",
    "#2011-01-02    2.5\n",
    "#2011-01-04    4.0\n",
    "#Freq: 2D, dtype: float64\n",
    "ts.resample('2D', how='mean',label='right')\n",
    "#2011-01-04    1.5\n",
    "#2011-01-06    3.5\n",
    "#Freq: 2D, dtype: float64\n",
    "ts.resample('2D', how='mean',loffset='-1D')#all index minus 1 Day\n",
    "#2011-01-01    1.5\n",
    "#2011-01-03    3.5\n",
    "#dtype: float64\n",
    "ts.groupby(lambda x: x.month).mean()\n",
    "#1    2.5\n",
    "#dtype: float64\n",
    "ts.groupby(lambda x: x.weekday).mean()  #index is Monday, Tus, Wed ...Sunday\n",
    "#0    2\n",
    "#1    3\n",
    "#2    4\n",
    "#6    1    \n",
    "#dtype: int32\n",
    "ts=ts.tz_localize('Europe/London') #set up timezone\n",
    "#2011-01-02 00:00:00+00:00    1\n",
    "#2011-01-03 00:00:00+00:00    2\n",
    "#2011-01-04 00:00:00+00:00    3\n",
    "#2011-01-05 00:00:00+00:00    4\n",
    "#Freq: D, dtype: int32\n",
    "ts=ts.tz_convert('Europe/Moscow')  # convert timezone\n",
    "#2011-01-02 03:00:00+03:00    1\n",
    "#2011-01-03 03:00:00+03:00    2\n",
    "#2011-01-04 03:00:00+03:00    3\n",
    "#2011-01-05 03:00:00+03:00    4\n",
    "#Freq: D, dtype: int32\n",
    "S=pd.date_range('1/2/2011', periods=4, freq='2D') \n",
    "#DatetimeIndex(['2011-01-02', '2011-01-04', '2011-01-06', '2011-01-08'], dtype='datetime64[ns]', freq='2D')\n",
    "ts.asof(S)\n",
    "#2011-01-02    1.0\n",
    "#2011-01-04    3.0\n",
    "#2011-01-06    4.0\n",
    "#2011-01-08    4.0\n",
    "#Freq: 2D, dtype: float64\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "ts=Series(np.arange(1,3), index=pd.date_range('1/2/2011', periods=2, freq='M'))\n",
    "#2011-01-31    1\n",
    "#2011-02-28    2\n",
    "#Freq: M, dtype: int32\n",
    "#when large date period resample to small date period\n",
    "ts.resample('W', fill_method='ffill', limit=2) #only fill the first two missing values, the rest of missing values are NaN\n",
    "#2011-02-06    1.0\n",
    "#2011-02-13    1.0\n",
    "#2011-02-20    NaN\n",
    "#2011-02-27    NaN\n",
    "#2011-03-06    2.0\n",
    "#Freq: W-SUN, dtype: float64\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "A=Series(np.arange(5), index=pd.date_range('1/1/2000', periods=5, freq='h'))\n",
    "#2000-01-01 00:00:00    0\n",
    "#2000-01-01 01:00:00    1\n",
    "#2000-01-01 02:00:00    2\n",
    "#2000-01-01 03:00:00    3\n",
    "#2000-01-01 04:00:00    4\n",
    "#Freq: H, dtype: int32\n",
    "A.between_time('01:00', '03:00')\n",
    "A.between_time(time(1,0), time(3,0))\n",
    "#2000-01-01 01:00:00    1\n",
    "#2000-01-01 02:00:00    2\n",
    "#2000-01-01 03:00:00    3\n",
    "#Freq: H, dtype: int32\n",
    "A.at_time(time(3,0))\n",
    "A[time(3,0)]\n",
    "#2000-01-01 03:00:00    3\n",
    "#Freq: H, dtype: int32\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df=DataFrame({'time':pd.date_range('2/1/2011', periods=2, freq='M')})\n",
    "#    time\n",
    "#0  2011-02-28\n",
    "#1  2011-03-31\n",
    "pd.DatetimeIndex(df['time']).month  #array([2, 3])\n",
    "\n",
    "df.time.dt.month\n",
    "#0    2\n",
    "#1    3\n",
    "#Name: time, dtype: int64\n",
    "df.time.dt.year\n",
    "#0    2011\n",
    "#1    2011\n",
    "#Name: time, dtype: int64\n",
    "df.time.dt.weekofyear\n",
    "#0     9\n",
    "#1    13\n",
    "#Name: time, dtype: int64\n",
    "df.time.dt.dayofweek\n",
    "#0    0\n",
    "#1    3\n",
    "#Name: time, dtype: int64\n",
    "month_year = (df.time.dt.month + df.time.dt.year * 100)\n",
    "#0    201102\n",
    "#1    201103\n",
    "#Name: time, dtype: int64\n",
    "month_year_cnt_map = month_year.value_counts().to_dict() #{201102: 1, 201103: 1}\n",
    "month_year.map(month_year_cnt_map)\n",
    "#0    1\n",
    "#1    1\n",
    "#Name: time, dtype: int64\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#time unit: 'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns'\n",
    "#64-bit integer in nanosecond\n",
    "np.datetime64('2009-01-01') - np.datetime64('2008-01-01')  #numpy.timedelta64(366,'D')\n",
    "np.datetime64('2009') + np.timedelta64(20, 'D')            #numpy.datetime64('2009-01-21')\n",
    "np.timedelta64(1,'W') / np.timedelta64(1,'D')   #7\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import time\n",
    "%time [i for i in range(5)]#will show processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################  Identifying and removing the seasonal component from the time series #####\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "timeseries=Series(np.arange(1,1001), index=pd.date_range('1/2/2011', periods=1000, freq='D'))\n",
    "#Check the stationarity of a time series:\n",
    "#Method1:\n",
    "#Before time series modelling, we need to check the stationarity of a time series. \n",
    "#We can plot the moving average or moving variance and see if it varies with time\n",
    "rolmean = timeseries.rolling(window=52,center=False).mean() \n",
    "rolstd = timeseries.rolling(window=52,center=False).std()\n",
    "#Plot rolling statistics:\n",
    "orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Method2:\n",
    "#Perform Dickey-Fuller test: a statistical test with the null hypothesis that the time series is non-stationary. \n",
    "#If the p value is larger than the 0.05 critical value, the null hypothesis of the Dickey-Fuller test cannot be rejected.\n",
    "#that means the moving average is not constant over time.\n",
    "print 'Results of Dickey-Fuller Test:'\n",
    "dftest = adfuller(timeseries, autolag='AIC')\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)'%key] = value\n",
    "print dfoutput\n",
    "#Results of Dickey-Fuller Test:\n",
    "#Test Statistic                -9.524178e+00\n",
    "#p-value                        3.008676e-16\n",
    "##Lags Used                     4.000000e+00\n",
    "#Number of Observations Used    9.950000e+02\n",
    "#Critical Value (5%)           -2.864449e+00\n",
    "#Critical Value (1%)           -3.436939e+00\n",
    "#Critical Value (10%)          -2.568319e+00\n",
    "\n",
    "###############################################\n",
    "#Removing trend and seasonality:\n",
    "#If the types of seasonality is yearly, we can subtract the data from the same day last year to correct for seasonality.\n",
    "timeseries_diff = timeseries - timeseries.shift()\n",
    "timeseries_diff.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import quandl\n",
    "quandl.ApiConfig.api_key = 'LqAzd9wm8ZWXxLDNtc_1'\n",
    "ticker='AAPL' \n",
    "date='2010-02-01'\n",
    "sqllist=['SF1/{}_ASSETS_ARQ'.format(ticker),    # Total Assets                           \n",
    "         'SF1/{}_REVENUE_ARQ'.format(ticker)]   # Revenue \n",
    "data=quandl.get(sqllist,start_date= date)\n",
    "#           SF1/AAPL_ASSETS_ARQ - Value  SF1/AAPL_REVENUE_ARQ - Value\n",
    "#Date               \n",
    "#2010-04-21       5.705700e+10                 1.349900e+10\n",
    "#2010-07-21       6.472500e+10                 1.570000e+10\n",
    "#.... until today\n",
    "\n",
    "data=quandl.get('WIKI/AAPL', start_date= '2010-02-01')\n",
    "selectedcol=[\"Adj. Close\",'Adj. Open','Adj. High','Adj. Low', 'Adj. Volume']\n",
    "data[selectedcol]\n",
    "#               Adj. Close   Adj. Open   Adj. High   Adj. Low    Adj. Volume\n",
    "#Date\n",
    "#2010-02-01     25.025519   24.722226    25.188732   24.584716   187469100.0\n",
    "#... until today\n",
    "\n",
    "from stockstats import StockDataFrame\n",
    "stock = StockDataFrame.retype(data)\n",
    "stock['rsi_2']\n",
    "#Date\n",
    "#2010-02-01           NaN\n",
    "#2010-02-02    100.000000\n",
    "#2010-02-03    100.000000\n",
    "#.... until today\n",
    "\n",
    "##############################################################\n",
    "import pandas_datareader.data as web\n",
    "data=web.get_data_yahoo('AAPL', '2017-01-02')\n",
    "ratio=data['Adj Close']/data['Close']\n",
    "data['Adj. Open']=data['Open']*ratio\n",
    "data['Adj. High']=data['High']*ratio\n",
    "data['Adj. Low']=data['Low']*ratio\n",
    "data=technical_data.rename(columns={'Volume':'Adj. Volume', 'Adj Close': 'Adj. Close' })\n",
    "data=data.drop(['Open','High','Low','Close'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import polyfit\n",
    "x=np.array([0,2,3])\n",
    "y=np.array([0,4,6])\n",
    "polyfit(x,y,1) #array([  2.00000000e+00,  -7.69185075e-16])\n",
    "polyfit(x,y,2) #array([  6.15259634e-16,   2.00000000e+00,   1.02558010e-15])  #degree=2 poly\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy import poly1d\n",
    "#if f(x)=3x^2 + x + 5\n",
    "p=poly1d([3,1,5])\n",
    "x=[1,2]\n",
    "p(x) \n",
    "#array([ 9, 19])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import gaussian_kde\n",
    "xn = np.random.randn(10)#generate a sample of a normal distribution\n",
    "gkde=gaussian_kde(xn)#we create an instance of the gaussian_kde class and feed our sample to it\n",
    "ind = np.linspace(-7,7,101)\n",
    "kdepdf = gkde.evaluate(ind)#We need some points at which we evaluate the density funtion for the estimated density function\n",
    "gkde.evaluate(0) #interpolate the density for values that don’t appear in the sample.\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy import linalg\n",
    "x = np.random.randn(3, 2) \n",
    "Umatrix, sigma, Vmatrix= linalg.svd(x, full_matrices=True)\n",
    "Umatrix\n",
    "#array([[-0.05896357,  0.99737542, -0.0420186 ],\n",
    "#       [-0.87946859, -0.03198673,  0.47488088],\n",
    "#       [ 0.47229048,  0.06495471,  0.87904641]])\n",
    "np.diag(sigma)\n",
    "#array([[ 1.46405123,  0.        ],\n",
    "#       [ 0.        ,  0.54081416]])\n",
    "Vmatrix\n",
    "#array([[ 0.99999213, -0.00396835],\n",
    "#       [-0.00396835, -0.99999213]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.misc import comb\n",
    "x = np.array([1, 2])\n",
    "y = np.array([3, 4])\n",
    "comb(y, x)#y coose x\n",
    "#array([ 3.,  6.])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "X = np.array([1,2,3,4])\n",
    "Y = np.array([1,2,3,38])\n",
    "from scipy.stats import pearsonr #return pearson r correlation coefficient and p value\n",
    "#Calculates a Pearson correlation coefficient and the p-value for testing non-correlation.\n",
    "#The Pearson correlation coefficient measures the linear relationship between two datasets.\n",
    "Pearson_correlation_coefficient, two_tailed_p_value = pearsonr(X, Y)\n",
    "\n",
    "from scipy.stats import spearmanr #return a Spearman rank-order correlation coefficient and p value\n",
    "#The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated\n",
    "correlation, pvalue = spearmanr(X, Y)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy import interp\n",
    "X = [1, 2, 3]\n",
    "Y = [2, 4, 6]\n",
    "interp(2.5, X, Y) #5.0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.sparse import csr_matrix # create matrix in Compressed Sparse Row format\n",
    "row = np.array([0, 1, 2])\n",
    "col = np.array([2, 1, 0])\n",
    "data = np.array([4, 5, 6])\n",
    "csr_matrix((data, (row, col)), shape=(3, 3)).toarray()\n",
    "#(row[0], col[0])=data[0]\n",
    "#array([[0, 0, 4],\n",
    "#       [0, 5, 0],\n",
    "#       [6, 0, 0]])\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "A = csr_matrix([[1, 2], [3, 4]])\n",
    "B = csr_matrix([[5, 6], [6, 8]])\n",
    "hstack([A,B]).toarray()\n",
    "#array([[1, 2, 5, 6],\n",
    "#       [3, 4, 7, 8]])\n",
    "vstack([A,B]).toarray()\n",
    "#array([[1, 2],\n",
    "#       [3, 4],\n",
    "#       [5, 6],\n",
    "#       [6, 8]])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "ffrom scipy.signal import argrelextrema # search local peak location \n",
    "#                   peak                  peak\n",
    "x = np.array([1, 3,  5,  3, 1, 0, 1, 3,7,  8,  5,4,3,2])\n",
    "argrelextrema(x, np.greater) # show the indice of the local maximum peaks\n",
    "#(array([2, 9], dtype=int64),)\n",
    "\n",
    "#                   peak                  peak\n",
    "x = np.array([5, 4,  3,  4, 5, 5, 5, 3,2,  0,  3,4,5,5])\n",
    "argrelextrema(x, np.less) # show the indice of the local minimum peaks\n",
    "#(array([2, 9], dtype=int64),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#caculate central moment\n",
    "from scipy.stats import moment\n",
    "#Calculates the nth moment about the mean for a sample\n",
    "x=[1,2,3]\n",
    "moment(x, moment=2)  #0.66666666666\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ normal distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import norm\n",
    "norm.rvs(size=3,loc = 0, scale=1)\n",
    "x=[-0.5,0.0,0.5]\n",
    "norm.pdf(x) #exp(-x**2/2)/sqrt(2*pi) standardized probability density function \n",
    "norm.pdf(x, loc=0, scale=1) #loc: mean, scale:standard deviation \n",
    "#array([ 0.35206533,  0.39894228,  0.35206533])\n",
    "\n",
    "x=[-0.5,0.0,0.5]\n",
    "norm.cdf(x, loc=0, scale=1) #array([ 0.30853754,  0.5       ,  0.69146246]) Cumulative distribution function\n",
    "#Survival function (1 - cdf) is CCDF.\n",
    "norm.sf(x, loc=0, scale=1) #array([ 0.69146246,  0.5       ,  0.30853754])\n",
    "\n",
    "x=[  0.30853754,  0.5       ,  0.69146246] # from norm.cdf([-0.5,0.0,0.5], loc=0, scale=1)\n",
    "#Percent point function \n",
    "norm.ppf(x,loc = 0, scale=1)  #array([-0.5,  0. ,  0.5])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ uniform distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import uniform\n",
    "uniform.rvs(size=3,loc = 0, scale=10)#Generate random numbers of uniform distribution between loc and loc + scale. \n",
    "#array([ 0.83144246,  4.25187789,  2.37639833])\n",
    "uniform.cdf(x=0.25,loc = 0, scale=1) #0.25  #means there is a 25% chance that an observation will be in the range 0 to x=0.25 \n",
    "#Percent point function \n",
    "uniform.ppf(q=0.5,loc = 0, scale=1) #0.5     #means the value of 0.5 has a 50% probability of drawing an observation below that value\n",
    "x=[0.1,0.2,0.3]\n",
    "uniform.pdf(x, loc=0, scale=1)    #array([ 1,  1,  1])   #gives you the probability density at a given x value\n",
    "uniform.cdf(x,loc = 0, scale=1)   #array([ 0.1,  0.2,  0.3])\n",
    "uniform.sf(x, loc=0, scale=1)     #array([ 0.9,  0.8,  0.7])   #Survival function (1 - cdf) is CCDF.\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ binomial distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import binom\n",
    "binom.rvs(n=10, p=0.5, size=3)  #n:Number trials, p=Success probability, size=Number of samples \n",
    "# array([7, 6, 3]) # show count number of success for each sample\n",
    "#choose(n, k) * p**k * (1-p)**(n-k) for k in {0, 1,..., n}.\n",
    "binom.pmf(k=5, n=10, p=0.5) #0.24609375000000025 # the probability of k = 5 successes with 10 trails and 0.5 success probability\n",
    "binom.cdf(k=5,n=10, p=0.5) #0.62304687499999989 # the probability of k = 5 or less successes \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ poisson distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import poisson\n",
    "poisson.rvs(mu = 2, size=10) #mu is lamda\n",
    "#pmf(k) = exp(-mu) * mu**k / k! #for a single observation, the probability of observing x=k in a fixed time or area\n",
    "poisson.pmf(k=2, mu=2)       #0.2706705664732254  # The maximum of pmf is mu\n",
    "poisson.cdf(k=2, mu=2)       #0.67667641618306362 \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~exponential distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# exponential distribution\n",
    "from scipy.stats import expon\n",
    "#Generate random numbers that have exponential distribution\n",
    "expon.rvs(scale=5, size=3) #array([ 0.86527021,  5.2753096 ,  6.25730534]) \n",
    "x=[1,2,4]\n",
    "#pdf = lambda * exp(-lambda * x). scale=1/lambda\n",
    "expon.pdf(x,scale =5) #array([ 0.16374615,  0.13406401,  0.08986579])\n",
    "expon.cdf(x,scale =5) #array([ 0.18126925,  0.32967995,  0.55067104])         \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ beta distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# beta distribution\n",
    "from scipy.stats import beta\n",
    "#for 0 < x < 1, a > 0, b > 0\n",
    "#                    gamma(a+b) * x**(a-1) * (1-x)**(b-1)\n",
    "#beta.pdf(x, a, b) = ------------------------------------\n",
    "#                             gamma(a)*gamma(b)\n",
    "beta.rvs(a=2.3, b=0.62, size=3) #array([ 0.9333461 ,  0.83408968,  0.99956068])\n",
    "x=[0.1,0.2,0.3]\n",
    "beta.pdf(x, a=2.3, b=0.62) #array([ 0.0575531 ,  0.14819914,  0.26411999])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Gamma distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import gamma \n",
    "#            beta**alpha  *  x**(alpha -1) * exp(-x*beta)  / gamma(alpha)\n",
    "#pdf(x, a) =                 x**(a-1)      * exp(-x)      / gamma(a) #standard gamma distribution\n",
    "# x can be replaced by (x - loc) / scale.\n",
    "#a=alpha,scale=1/beta\n",
    "gamma.rvs(a=2, scale=3, size=3) #array([ 7.51085973,  6.99763131,  1.47107471])\n",
    "x=[1,2,3]\n",
    "gamma.pdf(x, a=2, scale=3)      #array([ 0.07961459,  0.11409269,  0.12262648])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ lognormal distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import lognorm\n",
    "#pdf(x, s) = 1 / (s*x*sqrt(2*pi)) * exp(-1/2*(log(x)/s)**2)  \n",
    "# s is a standard seviation\n",
    "lognorm.rvs(s=1, size=3)  #array([ 1.53793771,  4.76268567,  3.81257908])\n",
    "x=[1,2,3]\n",
    "lognorm.pdf(x, s=1)       #array([ 0.39894228,  0.15687402,  0.07272826])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Pareto distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import pareto\n",
    "#pdf(x, b) = alpha*xm**alpha / x**(alpha+1) for x >= 1, alpha > 0.\n",
    "#b=alpha, scale=xm\n",
    "pareto.rvs(b=1, scale=2, size=3)\n",
    "pareto.pdf(x, b=1, scale=2)\n",
    "pareto.cdf(x, b=1, scale=2)\n",
    "#cdf(x, b) =1- (x/xm)**(-alpha) for x >= xm; otherwise = 0\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  chi square distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import chi2\n",
    "#chi2.pdf(x, df) = 1 / (2*gamma(df/2)) * (x/2)**(df/2-1) * exp(-x/2),df is degree of freedom\n",
    "chi2.rvs(df=3, size=3)\n",
    "chi2.pdf(x, df)\n",
    "chi2.cdf(x, df)\n",
    "chi2.ppf(q, df)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ T distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import t\n",
    "t.rvs(df, size=1)# df is degree of freedom\n",
    "t.pdf(x, df)\n",
    "t.cdf(x, df)\n",
    "t.ppf(q, df)\n",
    "#                              gamma((df+1)/2)\n",
    "#t.pdf(x, df) = ---------------------------------------------------\n",
    "#               sqrt(pi*df) * gamma(df/2) * (1+x**2/df)**((df+1)/2)   \n",
    "          \n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ F distribution~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from scipy.stats import f\n",
    "f.rvs(dfn, dfd, size=1)# df is degree of freedom\n",
    "f.pdf(x, dfn, dfd)\n",
    "f.cdf(x, dfn, dfd)\n",
    "f.ppf(q, dfn, dfd)\n",
    "#                              df2**(df2/2) * df1**(df1/2) * x**(df1/2-1)\n",
    "#F.pdf(x, df1, df2) = --------------------------------------------\n",
    "#                     (df2+df1*x)**((df1+df2)/2) * B(df1/2, df2/2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z score, F score, P score, confidence interval, Chi-squared test, ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_1samp # one sample\n",
    "from scipy.stats import t\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import zmap\n",
    "from scipy.stats import ttest_ind #TWO INDEPENDENT samples\n",
    "from scipy.stats import ttest_rel # TWO Relative samples\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "################# Z90%  ########################################################################\n",
    "# the z score corresponding to 95% probability - one tail\n",
    "#norm.ppf(q, loc=0, scale=1) Percent point function \n",
    "norm.ppf(0.95)# one tail test, show z score 1.65 \n",
    "norm.ppf((1+0.95)/2)# two tail test, show z score is 1.96\n",
    "# if z score is 1.64, show the probability -- one tail\n",
    "norm.cdf(1.65)#0.95\n",
    "################# Calculate the z score by a given sample data #################################\n",
    "x=[-1,-2,0,1,2]\n",
    "zscore(x) #Calculates the z score of each value in the sample, relative to the sample mean and standard deviation\n",
    "#array([-0.70710678, -1.41421356,  0.        ,  0.70710678,  1.41421356])\n",
    "zmap(scores=[0.5, 2.0, 2.5, 3], compare= [0, 1, 2, 3, 4])  #array([-1.06066017,  0.        ,  0.35355339,  0.70710678])\n",
    "#Returns an array of z-scores scores : array_like\n",
    "#scores: The input for which z-scores are calculated by using mean and standard deviation which are obtained from compare array.\n",
    "\n",
    "################# show T score and its corresponding p value for a population mean###############\n",
    "x=np.random.normal(loc=0.5, scale=1, size=10)\n",
    "ttest_1samp(x, popmean=0) #popmean is population mean \n",
    "# Ttest_1sampResult(Tstatistic=2.9917864975600903, pvalue=0.015156928916249181)\n",
    "# statistic is t score\n",
    "\n",
    "################# show T score and its corresponding p value for the means of TWO INDEPENDENT samples of scores.\n",
    "#This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. \n",
    "#This test assumes that the populations have identical variances by default.\n",
    "sample1 = norm.rvs(loc=5,scale=10,size=500)\n",
    "sample2 = norm.rvs(loc=10,scale=10,size=500)\n",
    "ttest_ind(a= sample1, b= sample2,equal_var=True) #Ttest_indResult(statistic=-7.3285318388586793, pvalue=4.7978272767238161e-13)\n",
    "#equal_var=True (default), perform a standard independent 2 sample test that assumes equal population variances \n",
    "#If the p-value is smaller than the threshold, e.g. 1%, 5% or 10%, then we reject the null hypothesis of equal averages.\n",
    "#Small p-values are associated with large t-statistics.\n",
    "ttest_ind_from_stats(mean1=5, std1=10, nobs1=500, mean2=10, std2=10, nobs2=500, equal_var=True) \n",
    "#Ttest_indResult(statistic=-7.9056941504209481, pvalue=7.0261518075272276e-15)\n",
    "#mean1 : The mean of sample 1 ; std1 : The standard deviation(s) of sample 1 ; \n",
    "#nobs1 : The number(s) of observations of sample 1.\n",
    "\n",
    "################# show T score and its corresponding p value for the means of TWO paired means at the same group \n",
    "#This is a two-sided test for the null hypothesis that 2 related or repeated samples have identical average (expected) values\n",
    "rvs1 = norm.rvs(loc=5,scale=10,size=500)\n",
    "rvs2 = (norm.rvs(loc=5,scale=10,size=500) + norm.rvs(scale=0.2,size=500))\n",
    "ttest_rel(rvs1,rvs2)#Ttest_relResult(statistic=0.195374474762399, pvalue=0.84517932489683667)\n",
    "\n",
    "################# type 2 error  #############################################################\n",
    "# Suppose group mean =0\n",
    "# H0: mu=0\n",
    "# Ha: mu=3\n",
    "# Type II error is that Ha is true, but  the null distribution are within the confidence level\n",
    "#when we fail to reject a false null hypothesis\n",
    "lower_quantile = norm.ppf(0.025)  # -1.9599639845400545  #Lower cutoff value of the null distribution \n",
    "upper_quantile = norm.ppf(0.975)  # 1.959963984540054    #Upper cutoff value of the null distribution \n",
    "# Area under alternative, to the left the lower cutoff value\n",
    "low = norm.cdf(lower_quantile, loc=3, scale=1)  #0.0065694509049582846     #loc=(sample mean), scale=(sample std)\n",
    "# Area under alternative, to the left the upper cutoff value\n",
    "high = norm.cdf(upper_quantile, loc=3, scale=1) #0.30152551201728128         \n",
    "# Area under the alternative, between the cutoffs (Type II error) # we reject Ha\n",
    "high-low   #0.14916123167294387\n",
    "\n",
    "################# show critical t value #######################################################\n",
    "#95% confidence, df=n-1=999\n",
    "t.ppf([(0.025),(1-0.025)], 999)# 2-tail test show critical t scores \n",
    "#array([-1.96234146,  1.96234146])\n",
    "t.ppf(1-0.05, 999)# one-tail test show critical t score \n",
    "#1.646380345427535\n",
    "\n",
    "################# given t score, show p value #################################################\n",
    "#x is t score, df=n-1\n",
    "p_value =1-t.cdf(x=1.5, df=999)# show p value is 0.067 # one tail \n",
    "p_value =(1-t.cdf(x=1.5, df=999))*2 # show p value is 0.13 # two tail\n",
    "\n",
    "################# confidence interval for population mean #######################################\n",
    "t.interval(alpha=0.95,df= 24,loc =0, scale = 1) #(-2.0638985616280205, 2.0638985616280205\n",
    "#alpha=Confidence level,df= Degrees of freedom,loc = Sample mean, scale = sample std/sqrt(sample_size)\n",
    "#it will show( sample_mean-t95%*sample_std/sqrt(sample_size), sample_mean+t95%*sample_std/sqrt(sample_size)  )\n",
    "\n",
    "################# confidence interval for population proportion  ###############################\n",
    "norm.interval(alpha = 0.95, loc = 0.19, scale=1) #(-1.76996399845400541, 2.149963984540054)\n",
    "#alpha =Confidence level, loc =Point estimate of proportion(p) , scale =sqrt((p*(1-p))/n)), n = Sample size\n",
    "\n",
    "################# chi-squared test #############################################################\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "##### Find the critical value for 95% confidence\n",
    "chi2.ppf(q = 0.95, df = 4)   # Df = number of variable categories - 1 #9.487729036781154\n",
    "\n",
    "##### fing p value from a chi2 score (chi_squared_stat)\n",
    "p_value = 1 - chi2.cdf(x=9.487, df=4) #0.050015054973933215\n",
    "\n",
    "##### The chi square test tests the null hypothesis that the categorical data has the given frequencies (counts)\n",
    "chisquare(f_obs= [10, 10, 20],   # Array of observed counts in each category.\n",
    "          f_exp= [10, 10, 20])   # Array of expected counts in each category.\n",
    "#show (The chi-squared test statistic, The p-value of the test)\n",
    "#Power_divergenceResult(statistic=0.18181818181818182, pvalue=0.9131007162822623)\n",
    "\n",
    "##### chi-square independent test (a test for the independence of different categories of a population)\n",
    "#df   size1 size2 size3 \n",
    "#   A  10    10    20\n",
    "#   B  10    10   22\n",
    "# each cell value is the frequency counts\n",
    "obs = np.array([[10, 10, 20], [10, 10, 22]])\n",
    "chi2_contingency(observed= obs)#Chi-square test of independence of variables in a contingency table\n",
    "# return (chi2 value, p value, degree of freedom, the expected frequncies for all cells)\n",
    "#(0.046485260770975138,\n",
    "# 0.97702539896326912,\n",
    "# 2L,\n",
    "# array([[  9.75609756,   9.75609756,  20.48780488],\n",
    "#        [ 10.24390244,  10.24390244,  21.51219512]]))\n",
    "\n",
    "################# ANOVA ##################################################################\n",
    "from scipy.stats import f_oneway\n",
    "##The one-way ANOVA tests the null hypothesis that two or more groups have the same population mean. \n",
    "sample1=[-2,-1,0,1,2];sample2=[-20,-10,0,10,20]; sample3=[-0.2,-0.1,0,0.1,0.2]\n",
    "f_oneway(sample1, sample2, sample3)# F_onewayResult(statistic=6.1013521651709296e-37, pvalue=1.0)\n",
    "#\n",
    "## fing p value from a F score \n",
    "p_value = 1 - f.cdf(x=F_score, dfn, dfd)\n",
    "\n",
    "################# T statistics for linear regression#######################################\n",
    "from scipy.stats import linregress\n",
    "x=[1,2,3,4];y=[2,4,6,8]\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "#LinregressResult(slope=2.0, intercept=0.0, rvalue=1.0, pvalue=9.9999999999999979e-21, stderr=0.0) \n",
    "#r-value : correlation coefficient ; p-value : for a hypothesis test whose null hypothesis is that the slope is zero.\n",
    "#stderr : the standard error of the slope coefficient\n",
    "n=4\n",
    "confidence_interval =[slope - t.ppf(1-0.05, n-2)*std_err,slope +t.ppf(0.95, n-2)*std_err]\n",
    "r_squared=r_value**2\n",
    "################# multivaluable linear regression ########################################## \n",
    "import statsmodels.api as sm\n",
    "x=np.array([0,2,3,3.8])\n",
    "y=np.array([0,4,6,8])\n",
    "A=sm.OLS(y,x).fit()     #fit a line\n",
    "#print A.params          #[ 2.05539359]    #show the fitting slope\n",
    "A.summary()# show R-squared, Adj. R-squared, F-statistic .. , t-statistic for each variable, slope interval\n",
    "#std(ys) is the standard deviation of the dependent variable\n",
    "#std(res) is the standard deviation of the residuals\n",
    "#if the slope and the intercept are statistically significant, it means that they were unlikely to occur by chance\n",
    "#if the slope of a variable is smaller, it means the effect on y is smaller\n",
    "#if the R2 value for the model is small, it means that these predictors doesn’t account for a substantial part \n",
    "#of the variation in y. That means these variables have little predictive power\n",
    "\n",
    "x_new=[5,6]\n",
    "A.predict(x_new) #array([ 10.27696793,  12.33236152])  # predict y series based on the fitting parameters\n",
    "A.fittedvalues #array([ 0., 4.11078717, 6.16618076, 7.81049563]) #The predicted values for the original design.\n",
    "\n",
    "#we can check whether the estimated slope is statistically significant\n",
    "A.pvalues # array([  7.09097382e-06]) #The two-tailed p values for the t-stats of the params  \n",
    "A.rsquared #0.99934653664421436       # show R^2\n",
    "A.rsquared_adj #0.9991287155256191\n",
    "A.f_pvalue # 7.0909738222397439e-06   #p-value of the F-statistic\n",
    "A.resid    # array([ 0.        , -0.11078717, -0.16618076,  0.18950437]) #The residuals of the model\n",
    "\n",
    "A.mse_model #115.92419825072886   #MSR, This is the sum of squared  divided by the model degrees of freedom.\n",
    "A.df_model  #1.0 #Model degress of freedom. The number of regressors p.\n",
    "\n",
    "A.mse_resid #0.025267249757045741 #MSE, Mean squared error of the residuals. \n",
    "A.ssr       #0.075801749271137225 #Sum of squared  residuals\n",
    "#The sum of squared residuals divided by the residual degrees of freedom.\n",
    "A.df_resid #3.0 #Residual degrees of freedom. n - p - 1\n",
    "\n",
    "A.mse_total #29.0 #MST,  Total mean squared error. \n",
    "#the uncentered total sum of squares divided by n the number of observations.\n",
    "A.nobs      #4    #Number of observations \n",
    "\n",
    "### Fitting models using R-style formulas\n",
    "#df.columns is [Lottery, Literacy, Wealth, Region]\n",
    "#   Lottery  Literacy  Wealth Region\n",
    "#0       41        37      73      E\n",
    "#1       38        51      22      N\n",
    "#2       66        13      61      C\n",
    "\n",
    "#dependent variable is Lottery\n",
    "mod = sm.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)\n",
    "res = mod.fit()\n",
    "res.summary()#it treated Region as a categorical variable\n",
    "#                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
    "#-------------------------------------------------------------------------------\n",
    "#Intercept      38.6517      9.456      4.087      0.000        19.826    57.478\n",
    "#Region[T.E]   -15.4278      9.727     -1.586      0.117       -34.793     3.938\n",
    "#Region[T.N]   -10.0170      9.260     -1.082      0.283       -28.453     8.419\n",
    "#Region[T.S]    -4.5483      7.279     -0.625      0.534       -19.039     9.943\n",
    "#Region[T.W]   -10.0913      7.196     -1.402      0.165       -24.418     4.235\n",
    "#Literacy       -0.1858      0.210     -0.886      0.378        -0.603     0.232\n",
    "#Wealth          0.4515      0.103      4.390      0.000         0.247     \n",
    "\n",
    "#The slope coef of Region[T.E] is the difference on the lottery between Region is E or not E  \n",
    "\n",
    "#If Region had been an integer variable, we can treat it as categorical by using the C() operator:\n",
    "formula='Lottery ~ Literacy + Wealth + C(Region)'  \n",
    "\n",
    "#remove the intercept from a model \n",
    "formula='Lottery ~ Literacy + Wealth + C(Region) -1\n",
    "\n",
    "#if we add some columns in df: df['sex']= 1 1 1 2 2 1 1 (1 is male and 2 is female)\n",
    "#df[housenumber]=3 2 4 1 1 2 1 ..\n",
    "formula='Lottery ~ Literacy + Wealth + C(Region)+ sex==1 + housenumber>1'  \n",
    "#sex==1 convert sex to boolean: 1 is True, 2 is False\n",
    "#housenumber>1 means 0 is false and >1 is True\n",
    "\n",
    "################# Autocorrelation in StatsModel #####################################\n",
    "from statsmodels.tsa.stattools import acf\n",
    "x=[1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7,1,2,3,4,5,6,7]\n",
    "acf=acf(x, unbiased=True, nlags=30)# compute serial correlation from lag=0 to lag=30\n",
    "#x is array (Time series data) ; unbiased : If True, then denominators for autocovariance are n-k, otherwise n\n",
    "#nlags: Number of lags to return autocorrelation for.\n",
    "print  acf[1], acf[7], acf[30]  #0.302083333333 1.0 -0.118421052632 # show the daily, weekly, monthly correlation\n",
    "#acf[365]  yearly correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
