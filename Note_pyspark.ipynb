{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## skip this if use Databrick ####################################\n",
    "# The first step of any Spark driver application is to create a SparkContext. \n",
    "# The SparkContext allows Spark driver application to access the cluster through a resource manager. \n",
    "# In order to create a SparkContext, we need first create a SparkConf. \n",
    "# The SparkConf stores configuration parameters that Spark driver application will pass to SparkContext. \n",
    "# setAppName() gives your Spark driver application a name so you can identify it in the Spark \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"appName\")\n",
    "#conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Naive_Bayes\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "print \"Running Spark Version %s\" % (sc.version)\n",
    "\n",
    "#SparkSQL is used to process structured data, SparkSQL implements dataframes and a SQL query engine\n",
    "#SparkSQL has a SQLContext and a HiveContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.options.display.mpl_style = 'default'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.clustering import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, MulticlassMetrics\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import rand, log, sqrt, pow, size, exp, mean, sum, avg \n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create a RDD from the file in HDFS\n",
    "#The textFile() method reads the file into a Resilient Distributed Dataset (RDD) with each line in the file \n",
    "rdd1 = sc.textFile(\"amazon_cells_labelled.txt\")\n",
    "rdd1.take(5)\n",
    "#[u'So there is no way for me to plug it in here in the US unless I go by a converter.\\t0',\n",
    "# u'Good case, Excellent value.\\t1',\n",
    "# u'Great for the jawbone.\\t1',\n",
    "# u'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\\t0',\n",
    "# u'The mic is great.\\t1']\n",
    "rdd1.count() #1000   #prints the number of elements (lines) \n",
    "rdd1.getNumPartitions() #2\n",
    "##########################################################################################\n",
    "## create a RDD from the train file in HDFS\n",
    "rdd1 = sc.textFile(\"allstate_train.csv\")\n",
    "rdd1.take(3)\n",
    "#[u'id,cat1,cat2,\n",
    "# u'1,A,B,\n",
    "# u'2,A,B]\n",
    "##########################################################################################\n",
    "#convert a list into a RDD\n",
    "lines = sc.parallelize([1,2,3])\n",
    "numbers = sc.parallelize(range(10),3)#parallize the range output [0,1,2,..,9] into 3 partitions \n",
    "#it will become 3 partitions : [0,1,2],[3,4,5],[6,7,8,9]\n",
    "numbers.collect() #gather 3 partitions into one partition [1,2,3,...9]\n",
    "numbers.getNumPartitions() #3 show oartition number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"amazon_cells_labelled.txt\")\n",
    "#Split a review sentence and a corresponding label by finding a tab (\\t )\n",
    "rdd2 = rdd1.map(lambda x: x.split(\"\\t\"))\n",
    "#[[u'So there is no way for me to plug it in here in the US unless I go by a converter.', u'0'],\n",
    "# [u'Good case, Excellent value.', u'1'],\n",
    "# [u'Great for the jawbone.', u'1'],\n",
    "# [u'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!', u'0'],\n",
    "# [u'The mic is great.', u'1']]\n",
    "\n",
    "#Create a RDD of Rows with the field names : label and review\n",
    "rdd3 = rdd2.map(lambda x: Row(review=x[0],label=x[1]))\n",
    "#[Row(label=u'0', review=u'So there is no way for me to plug it in here in the US unless I go by a converter.'),\n",
    "# Row(label=u'1', review=u'Good case, Excellent value.'),\n",
    "# Row(label=u'1', review=u'Great for the jawbone.'),\n",
    "# Row(label=u'0', review=u'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!'),\n",
    "# Row(label=u'1', review=u'The mic is great.')]\n",
    "\n",
    "rdd3 = rdd1.map(lambda x: x.split(\"\\t\")).map(lambda x: Row(x[0],x[1])).toDF([\"review\",\"label\"])\n",
    "#[Row(review=u'So there is no way for me to plug it in here in the US unless I go by a converter.', label=u'0'),\n",
    "# Row(review=u'Good case, Excellent value.', label=u'1'),\n",
    "# Row(review=u'Great for the jawbone.', label=u'1')]\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "rdd1 = sc.textFile(\"/FileStore/tables/c4pzqsme1501527259399/amazon_cells_labelled.txt\")\n",
    "words = rdd1.flatMap(lambda line: line.split(\" \"))\n",
    "words.take(4) #[u'So', u'there', u'is', u'no']\n",
    "tuples = words.map(lambda word: (word, 1))\n",
    "tuples.take(4) #[(u'So', 1), (u'there', 1), (u'is', 1), (u'no', 1)]\n",
    "#sum all the counts in the tuples for each word into a new RDD counts:\n",
    "counts = tuples.reduceByKey(lambda a, b:(a+b))\n",
    "counts.take(4)\n",
    "#[(u'magnetic', 1),\n",
    "# (u'unsatisfactory\\t0', 1),\n",
    "# (u'four', 1),\n",
    "# (u'earpiece.\\t0', 2)]\n",
    "counts.coalesce(1).saveAsTextFile('....')\n",
    "########################################################\n",
    "rdd1 = sc.textFile(\"allstate_train.csv\")\n",
    "#[u'id,cat1,cat2,\n",
    "# u'1,A,B,\n",
    "# u'2,A,B]\n",
    "rdd2 = rdd1.map(lambda x: x.split(\",\"))\n",
    "########################################################\n",
    "map(lambda c: c + \"_Index\", ['column1','column2']) #['column1_Index', 'column2_Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################### filter ##########################\n",
    "# find out which review has 'Good'\n",
    "#rdd2 is like\n",
    "#[Row(review=u'So there is no way for me to plug it in here in the US unless I go by a converter.', label=u'0'),\n",
    "# Row(review=u'Good case, Excellent value.', label=u'1'),\n",
    "rdd2.filter(col(\"review\").like(\"%Good%\")).take(2)\n",
    "#[Row(review=u'Good case, Excellent value.', label=u'1'),\n",
    "# Row(review=u'So Far So Good!.', label=u'1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"amazon_cells_labelled.txt\")\n",
    "rdd2 = rdd1.map(lambda x: x.split(\"\\t\"))\n",
    "rdd3 = rdd2.map(lambda x: Row(review=x[0],label=x[1]))\n",
    "df=sqlContext.createDataFrame(rdd3)\n",
    "# convert the RDD of Rows to a DataFrame via createDataFrame\n",
    "# register this DataFrame as a Temporary Table (named as “df”) in the SQLContext\n",
    "df.registerTempTable(\"df\")\n",
    "df.show()\n",
    "#+-----+--------------------+\n",
    "#|label|              review|\n",
    "#+-----+--------------------+\n",
    "#|    0|So there is no wa...|\n",
    "#|    1|Good case, Excell...|\n",
    "#|    1|Great for the jaw...|\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## load table in DataBrick to create DataFrame\n",
    "df = sqlContext.table(\"allstate_train_csv\")\n",
    "print '(Sample number, column number) in the train dataset =({},{})'.format(df.count(), len(df.columns))\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "## create a RDD from the train file in HDFS\n",
    "rdd1 = sc.textFile(\"allstate_train.csv\")\n",
    "rdd2 = rdd1.map(lambda x: x.split(\",\"))\n",
    "header = rdd2.first() #extract header\n",
    "data = rdd2.zipWithIndex().filter(lambda (row,index): index > 0).keys()\n",
    "# Converting an RDD to DataFrame\n",
    "df=sqlContext.createDataFrame(data,header)\n",
    "## Registers this DataFrame as a temporary table using the given name\n",
    "df.registerTempTable(\"df\")\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from pyspark.sql.types import * \n",
    "l = [('Alice',2),('May',5) ]\n",
    "rdd = sc.parallelize(l)\n",
    "schema =  StructType([StructField (\"name\" , StringType(), True) , \n",
    "                      StructField(\"age\" , IntegerType(), True)]) \n",
    "df = sqlContext.createDataFrame(rdd, schema) \n",
    "df.show()\n",
    "#+-----+---+\n",
    "#| name|age|\n",
    "#+-----+---+\n",
    "#|Alice|  2|\n",
    "#|  May|  5|\n",
    "#+-----+---+\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark I.\"),\n",
    "  (1, \"Logistic regression models are neat.\")\n",
    "], [\"label\", \"sentence\"])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "df = spark.createDataFrame([(1.0, 2.0, 3.0), (None, 4.0, 6.0)], (\"column1\", \"column2\", \"column3\"))\n",
    "df.show()\n",
    "#+-------+-------+-------+\n",
    "#|column1|column2|column3|\n",
    "#+-------+-------+-------+\n",
    "#|    1.0|    2.0|    3.0|\n",
    "#|   null|    4.0|    6.0|\n",
    "#+-------+-------+-------+\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n",
    "        (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n",
    "df = sqlContext.createDataFrame(data, [\"features\"])\n",
    "df.show()\n",
    "#+---------+\n",
    "#| features|\n",
    "#+---------+\n",
    "#|[0.0,0.0]|\n",
    "#|[1.0,1.0]|\n",
    "#|[9.0,8.0]|\n",
    "#|[8.0,9.0]|\n",
    "#+---------+\n",
    "##########################################################################\n",
    "pd.DataFrame(df.take(5), columns=df.columns)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "#root\n",
    "# |-- label: double (nullable = true)\n",
    "# |-- review: string (nullable = true)\n",
    "print '(Sample number, column number) in the train dataset =({},{})'.format(df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert DataFrame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0.0, 2.0), (1.0, 3.0), (2.0, 4.0)], (\"column1\", \"column2\"))\n",
    "rdd1=df.rdd.map(tuple)\n",
    "rdd1.collect() #[(0.0, 2.0), (1.0, 3.0), (2.0, 4.0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert String labels to Double type\n",
    "df = df.withColumn(\"label\", df.label.cast(DoubleType()))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# convert all the 'cont' variables to the 'Double' type\n",
    "def convertfloatColumn(dataframe, name):\n",
    "    return dataframe.withColumn(name, dataframe[name].cast(DoubleType()))\n",
    "float_columns = [feat for feat in df.columns if 'cont' in feat]\n",
    "for f in float_columns:\n",
    "    df=convertfloatColumn(df,f)\n",
    "    \n",
    "# convert the 'loss' variable to the 'Double' type   \n",
    "df=convertfloatColumn(df,'loss')\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# convert the 'id' column to 'Integer' type\n",
    "def convertIntColumn(dataframe, name):\n",
    "    return dataframe.withColumn(name, dataframe[name].cast(IntegerType()))\n",
    "df=convertIntColumn(df,'id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and load a DataFrame file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.toPandas().to_csv('mycsv.csv')\n",
    "df.write.csv('mycsv.csv') # doesn't have header\n",
    "df.write.csv('mycsv1.csv',header='true') # keep the header\n",
    "df.write.save(\"df_amazon.parquet\", format=\"parquet\")\n",
    "\n",
    "df = sqlContext.read.load(\"df_amazon.parquet\")\n",
    "rdd = sc.textFile(\"mycsv1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=df.select(['column2', 'column2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['label'] ==0).show()\n",
    "#+-----+--------------------+\n",
    "#|label|              review|\n",
    "#+-----+--------------------+\n",
    "#|  0.0|So there is no wa...|\n",
    "#|  0.0|Tied to charger f...|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"label\").count().show()\n",
    "#+-----+-----+\n",
    "#|label|count|\n",
    "#+-----+-----+\n",
    "##|  1.0|  500|\n",
    "#|  0.0|  500|\n",
    "#+-----+-----+\n",
    "df.groupby('label').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe union and join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark I.\"),\n",
    "  (1, \"Logistic regression models are neat.\")\n",
    "], [\"label\", \"sentence\"])\n",
    "df.unionAll(df).show()\n",
    "#+-----+--------------------+\n",
    "#|label|            sentence|\n",
    "#+-----+--------------------+\n",
    "#|    0|Hi I heard about ...|\n",
    "#|    1|Logistic regressi...|\n",
    "#|    0|Hi I heard about ...|\n",
    "#|    1|Logistic regressi...|\n",
    "#+-----+--------------------+\n",
    "############################################\n",
    "# Combine the training data and the test data as “train_test” DataFrame\n",
    "train_test=df.unionAll(df_test)\n",
    "print '(Sample number, column number) in the train_test dataset =({},{})'.format(train_test.count(),len(train_test.columns))\n",
    "#############################################\n",
    "df1 = spark.createDataFrame([(1.0, 10.0), (2.0, 20.0), (3.0, 30.0)], (\"column1\", \"column2\"))\n",
    "#+-------+-------+\n",
    "#|column1|column2|\n",
    "#+-------+-------+\n",
    "#|    1.0|   10.0|\n",
    "#|    2.0|   20.0|\n",
    "#|    3.0|   30.0|\n",
    "#+-------+-------+\n",
    "df2 = spark.createDataFrame([(1.0, 100.0), (2.0, 200.0), (2.0, 300.0)], (\"column1\", \"column3\"))\n",
    "#+-------+-------+\n",
    "#|column1|column3|\n",
    "#+-------+-------+\n",
    "#|    1.0|  100.0|\n",
    "#|    2.0|  200.0|\n",
    "#|    2.0|  300.0|\n",
    "#+-------+-------+\n",
    "df1.join(df2, \"column1\").show()               \n",
    "#+-------+-------+-------+\n",
    "#|column1|column2|column3|\n",
    "#+-------+-------+-------+\n",
    "#|    1.0|   10.0|  100.0|\n",
    "#|    2.0|   20.0|  200.0|\n",
    "#|    2.0|   20.0|  300.0|\n",
    "#+-------+-------+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a new column in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column of log(loss +200) named as ‘label’, and a new column of all zeros named 'all_zero'\n",
    "df=df.withColumn('label', log(df.loss+200)).withColumn('new_id', df.loss *0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other functions in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1.0, 2.0, 3.0), (None, 4.0, 6.0)], (\"column1\", \"column2\", \"column3\"))\n",
    "df.show()\n",
    "#+-------+-------+-------+\n",
    "#|column1|column2|column3|\n",
    "#+-------+-------+-------+\n",
    "#|    1.0|    2.0|    3.0|\n",
    "#|   null|    4.0|    6.0|\n",
    "#+-------+-------+-------+\n",
    "df.drop('column1')\n",
    "df.describe().toPandas().transpose() \n",
    "#             0     1                   2    3    4\n",
    "#summary  count  mean              stddev  min  max\n",
    "#column1      1   1.0                 NaN  1.0  1.0\n",
    "#column2      2   3.0  1.4142135623730951  2.0  4.0\n",
    "#column3      2   4.5  2.1213203435596424  3.0  6.0\n",
    "df.na.drop().show() #drop all the rows missing a value in any column\n",
    "df.na.drop(subset=['column1']).show()  #drop the rows with missing values in the teamlevel column\n",
    "#+-------+-------+-------+\n",
    "#|column1|column2|column3|\n",
    "#+-------+-------+-------+\n",
    "#|    1.0|    2.0|    3.0|\n",
    "#+-------+-------+-------+\n",
    "df.na.fill(10, ['column1']).show()        #replace the missing values with 10 in column 1 \n",
    "#+-------+-------+-------+\n",
    "#|column1|column2|column3|\n",
    "#+-------+-------+-------+\n",
    "#|    1.0|    2.0|    3.0|\n",
    "#|   10.0|    4.0|    6.0|\n",
    "#+-------+-------+-------+\n",
    "df.stat.corr('column2', 'column3') #1.0  #Compute correlation between two columns\n",
    "df.select(mean('column2'), sum('column2')).show()   ##Calculate average and sum\n",
    "#+------------+------------+\n",
    "#|avg(column2)|sum(column2)|\n",
    "#+------------+------------+\n",
    "#|         3.0|         6.0|\n",
    "#+------------+------------+\n",
    "df.agg(avg('column2')).show()\n",
    "#+------------+\n",
    "#|avg(column2)|\n",
    "#+------------+\n",
    "#|         3.0|\n",
    "#+------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new column of log(loss +200) named as ‘label’.\n",
    "shift=200\n",
    "df=df.withColumn('label', log(df.loss+shift))\n",
    "\n",
    "# Calculate skewness before data transform\n",
    "float_columns = [feat for feat in df.columns if 'cont' in feat]\n",
    "df_new=df.select(float_columns).toPandas()\n",
    "skew_values_before=[]\n",
    "for f in float_columns:\n",
    "    skew = df_new[f].skew()\n",
    "    skew_values_before.append(skew)\n",
    "############# Data transformation ######################\n",
    "# 'cont1' : Square root transformation             \n",
    "# 'cont2' : Unchanged                             \n",
    "# 'cont3' : X^ 1/3 transformation\n",
    "# 'cont4' : Log transformation                    \n",
    "# 'cont5' : Square root transformation\n",
    "\n",
    "df = df.withColumn(\"cont1_trans\", sqrt(df.cont1)).\\\n",
    "withColumn(\"cont2_trans\", df.cont2).\\\n",
    "withColumn(\"cont3_trans\", pow(df.cont4,0.3333333)).\\\n",
    "withColumn(\"cont4_trans\", log(df.cont5 + 0.000000001)).\\\n",
    "withColumn(\"cont5_trans\", sqrt(df.cont6))\n",
    "# Calculate skewness after data transform\n",
    "float_trans = [feat for feat in df.columns if 'trans' in feat]\n",
    "df_new=df.select(float_trans).toPandas()\n",
    "skew_values_after=[]\n",
    "for f in float_trans:\n",
    "    skew = df_new[f].skew()\n",
    "    skew_values_after.append(skew)\n",
    "\n",
    "# Plot skewness distribution of continuous features before/after data transformation\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,5), facecolor='white')\n",
    "ax.plot(np.arange(0, len(float_columns)),skew_values_before,marker='D',linewidth=3,label='Before data transformation')\n",
    "ax.plot(np.arange(0, len(float_columns)),skew_values_after,marker='D',linewidth=3, color='r',label='After data transformation')\n",
    "ax.set_xticks(np.arange(0, len(float_columns)))\n",
    "ax.set_xticklabels(float_columns)\n",
    "ax.grid(color='black', linestyle='--', linewidth=1)\n",
    "ax.set_title('Skewness of continuous features before/after data transformation')\n",
    "ax.set_ylabel('Skewness')\n",
    "ax.set_ylim(-0.5,1.2)\n",
    "ax.legend(loc='best')\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  convert the distinct labels in the input dataset to index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenizer \n",
    "# the RegexTokenizer function to break a review sentence into individual words. \n",
    "#The parameter “pattern” (\"\\W\") is used to remove all non-word characters, such as , !. .\n",
    "tokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\W\")##'\\w' remove none-word letters\n",
    "df_tokenized = tokenizer.transform(df)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df)\n",
    "#+-----+--------------------+--------------------+\n",
    "#|label|              review|               words|\n",
    "#+-----+--------------------+--------------------+\n",
    "#|  0.0|So there is no wa...|[so, there, is, n...|\n",
    "#|  1.0|Good case, Excell...|[good, case,, exc...|\n",
    "#################################################################################\n",
    "# remove stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"can\"] # standard stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "df_removed = remover.transform(df_tokenized)\n",
    "\n",
    "#################################################################################\n",
    "# Convert to TF words vector\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
    "df_TF = hashingTF.transform(df_removed)\n",
    "# Convert to TF*IDF words vector\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(df_TF)\n",
    "df_idf = idfModel.transform(df_TF)\n",
    "for features_label in df_idf.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)\n",
    "#Row(features=SparseVector(262144, {21872: 5.8101, 52801: 6.2156, 61625: 5.5225, 113100: 4.4238, 172477: 4.9628, 199255: 4.8293}), label=0.0)\n",
    "#Row(features=SparseVector(262144, {113432: 2.5913, 117481: 3.5766, 192310: 3.5076, 206496: 5.117}), label=1.0)\n",
    "#Row(features=SparseVector(262144, {138356: 2.3238, 138642: 5.5225}), label=1.0)\n",
    "######################################################################################################\n",
    "df = spark.createDataFrame([(0.0, 2.0), (1.0, 3.0), (2.0, 4.0)], (\"column1\", \"column2\"))\n",
    "#+-------+-------+\n",
    "#|column1|column2|\n",
    "#+-------+-------+\n",
    "#|    0.0|    2.0|\n",
    "#|    1.0|    3.0|\n",
    "#|    2.0|    4.0|\n",
    "#+-------+-------+\n",
    "bi=Binarizer(threshold=0.0, inputCol=\"column1\", outputCol=\"features\")\n",
    "bi.transform(df).show()\n",
    "#If the value is more than 0, the categorical value to be 1, otherwise the categorical value should be 0\n",
    "+-------+-------+--------+\n",
    "|column1|column2|features|\n",
    "+-------+-------+--------+\n",
    "|    0.0|    2.0|     0.0|\n",
    "|    1.0|    3.0|     1.0|\n",
    "|    2.0|    4.0|     1.0|\n",
    "+-------+-------+--------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler and PCA for numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Cumulative explained variance Vs. Number of PCA components for numerical features\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "float_columns = [feat for feat in df.columns if 'cont' in feat]\n",
    "data=df.select(float_columns).toPandas()\n",
    "z_scaler = StandardScaler()\n",
    "z_data = z_scaler.fit_transform(data)\n",
    "pca_trafo = PCA().fit(z_data);\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(12,5), facecolor='white')\n",
    "ax.plot(pca_trafo.explained_variance_ratio_.cumsum(),'--o',linewidth=3)\n",
    "ax.hlines(y=0.99, xmin=0, xmax=14, lw=2,color='r')\n",
    "ax.set_xlabel('Number of PCA components')\n",
    "ax.set_ylabel('Cumulative explained variance')\n",
    "ax.grid(color='black', linestyle='--', linewidth=1)\n",
    "display(fig)\n",
    "########################################################################\n",
    "# Create a vector column of transformed numerical variables as the input of a standard scaler\n",
    "# combine all the continuous features into a single vector column \n",
    "vecAssembler = VectorAssembler(inputCols=float_columns, outputCol=\"numerical_features\")\n",
    "#df=vecAssembler.transform(df)\n",
    "\n",
    "# normalizes each feature in “numerical_features” to have unit standard deviation and zero mean\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"numerical_scaledfeatures\",withStd=True, withMean=True)\n",
    "#scalerModel = scaler.fit(df)\n",
    "#df = scalerModel.transform(df)\n",
    "\n",
    "# The scaled features are feed into PCA \n",
    "#The new features obtained fron PCA is called as “pcaFeatures_numerical\".\n",
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=9, inputCol=\"numerical_scaledfeatures\", outputCol=\"pcaFeatures_numerical\")\n",
    "#model = pca.fit(df)\n",
    "#df = model.transform(df)\n",
    "#df.select('pcaFeatures').take(2)\n",
    "\n",
    "#  Wrap VectorAssembler, StandardScaler, and PCA using pipelines\n",
    "pipeline = Pipeline(stages=[vecAssembler,scaler,pca])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)\n",
    "df.select('pcaFeatures_numerical').take(2)\n",
    "#[Row(pcaFeatures=DenseVector([-2.5706, 1.9145, -1.7275, 0.5826, 0.5813, -0.4001, -0.9643, -1.0756, -1.3186])),\n",
    "# Row(pcaFeatures=DenseVector([0.875, 0.4521, 2.2523, -0.7282, -0.0405, -0.6469, 0.9521, -0.3714, 0.2652]))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encoding and PCA for categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################### 2-label categorical variables ####################################\n",
    "categorical_columns = [feat for feat in df.columns if 'cat' in feat]\n",
    "data=df.select(categorical_columns).toPandas()\n",
    "for f in categorical_columns:\n",
    "    print '{} unique number: {}'.format(f, data[f].nunique())\n",
    "    \n",
    "#  Transform the labels A and B into 0 and 1 numbers for all 2-labels categorical variables ('cat1' ~ 'cat72')\n",
    "for f in categorical_columns[:72]:\n",
    "    stringIndexer = StringIndexer(inputCol=f, outputCol=f+\"_Index\")\n",
    "    model = stringIndexer.fit(df)\n",
    "    df = model.transform(df)\n",
    "encoded_2labels_features=map(lambda c: c + \"_Index\", categorical_columns[:72])\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply PCA using Python Sklearn to find suitable PCA n_components\n",
    "from sklearn.decomposition import PCA\n",
    "data=train_test.select(encoded_2labels_features).toPandas()\n",
    "pca_trafo = PCA().fit(data);\n",
    "ax.plot(pca_trafo.explained_variance_ratio_.cumsum(),'--o',linewidth=3)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Create a vector column of encoded 2-lables categorical variables as the input of PCA \n",
    "vecAssembler_2labels = VectorAssembler(inputCols=encoded_2labels_features, outputCol=\"vec_encoded_2labels_features\")\n",
    "\n",
    "# Apply PCA \n",
    "from pyspark.ml.feature import PCA\n",
    "pca_2labels = PCA(k=45, inputCol=\"vec_encoded_2labels_features\", outputCol=\"pcaFeatures_2labels\")\n",
    "\n",
    "#  Wrap VectorAssembler, and PCA using pipelines\n",
    "pipeline = Pipeline(stages=[vecAssembler_2labels,pca_2labels])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)\n",
    "df.select('pcaFeatures_2labels').take(2)\n",
    "\n",
    "######################### muilti-label categorical variables ####################################\n",
    "data=df.select(categorical_columns[72:]).toPandas()\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "# Use one-hot encoding to transform their categorical labels into binary vectors for multi-label \n",
    "# categorical variables (‘cat73’ ~’cat116) \n",
    "one_hot_features = []\n",
    "for f in categorical_columns[72:]:\n",
    "    dummy = pd.get_dummies(data[f].astype('category'))\n",
    "    dummy = csr_matrix(dummy)\n",
    "    one_hot_features.append(dummy)\n",
    "one_hot_features=hstack(one_hot_features, format = 'csr').toarray()\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply PCA using Python Sklearn to find suitable PCA n_components\n",
    "from sklearn.decomposition import PCA\n",
    "pca_trafo = PCA().fit(one_hot_features)\n",
    "# Plot Cumulative explained variance Vs. Number of PCA components for multi-labels categorical variable\n",
    "ax.plot(pca_trafo.explained_variance_ratio_.cumsum(),'--o',linewidth=3)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "for f in categorical_columns[72:]:\n",
    "    # Encode the string labels into numerical labels\n",
    "    stringIndexer = StringIndexer(inputCol=f, outputCol=f+\"_Index\")\n",
    "    model = stringIndexer.fit(df)\n",
    "    df = model.transform(df)\n",
    "    # Maps a column of numerical labels to a column of binary vectors \n",
    "    encoder = OneHotEncoder(dropLast=False, inputCol=f+\"_Index\", outputCol=f+\"_onehot\")\n",
    "    df = encoder.transform(df)\n",
    "# Assemble all the encoded vector columns of the multi-label categorical variables into a single vector columns \n",
    "onehot_features=map(lambda c: c + \"_onehot\", categorical_columns[72:])\n",
    "vecAssembler_onehot = VectorAssembler(inputCols=onehot_features, outputCol=\"vec_onehot_features\")\n",
    "df=vecAssembler_onehot.transform(df)\n",
    "# Apply PCA \n",
    "from pyspark.ml.feature import PCA\n",
    "pca_multilabels = PCA(k=300, inputCol=\"vec_onehot_features\", outputCol=\"pcaFeatures_multilabels\")\n",
    "model = pca_multilabels.fit(df)\n",
    "df = model.transform(df)\n",
    "df.select('pcaFeatures_multilabels').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the features created by PCA on the continuous input variables, 2-label and multi-label categorical variables \n",
    "#are combined together to become one vector column by ‘VectorAssembler’\n",
    "vecAssembler_total = VectorAssembler(inputCols=['pcaFeatures_numerical','pcaFeatures_2labels','pcaFeatures_multilabels'], \n",
    "                                     outputCol=\"features\")\n",
    "df=vecAssembler_total.transform(df)\n",
    "print 'The feature number in \"features\" column of train_test DataFrame = {}'.format(df.select('features').\n",
    "                                                                                    rdd.map(lambda raw : len(raw[0])).take(1))\n",
    "df('features').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split, cache data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data aproximately into training (80%) and test (20%)\n",
    "(train, test)=df.randomSplit([0.8,0.2], seed = 0)\n",
    "# Cache the train and test data in-memory because they will be called more than once during model building. \n",
    "train = train.cache()\n",
    "test = test.cache()\n",
    "print 'Sample number in the train set : {}'.format(train.count())\n",
    "print 'Sample number in the test set : {}'.format(test.count())\n",
    "train.groupby('label').count().toPandas()\n",
    "##################################################################\n",
    "# Split the “train_test” dataframe into the ‘X’ dataframe (referred to the training set) and \n",
    "#the “X_test” dataframe (refered to the test set),\n",
    "X=train_test.filter(train_test.loss!=0).select('features', 'label')\n",
    "X_test=train_test.filter(train_test.loss==0).select('features')\n",
    "print 'Sample number in X = {}'.format(X.count())\n",
    "print 'Sample number in X_test = {}'.format(X_test.count())\n",
    "# Split the ‘X’ dataframe into the ‘train’ (80%) and ‘validation’ (20%) sets \n",
    "(train, validation)=X.randomSplit([0.8,0.2], seed = 0)\n",
    "# Cache the train and validation data in-memory \n",
    "train = train.cache() \n",
    "validation = validation.cache()\n",
    "print 'Sample number in the \"train\" set : {}'.format(train.count())\n",
    "print 'Sample number in the \"validation\" set : {}'.format(validation.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store RDD in memory\n",
    "df = df.cache()\n",
    "df = df.persist() \n",
    "#delete caching, release memory\n",
    "df.unpersist() \n",
    "#Removes all cached tables from the in-memory cache.\n",
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################### classification ##################################\n",
    "def grid_search(p1,p2,p3,p4):\n",
    "    lr = LogisticRegression()\n",
    "    pipeline = Pipeline(stages=[labelIndexer,tokenizer, remover, hashingTF, idfModel, lr])\n",
    "  \n",
    "    #Create ParamGrid for Cross Validation\n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(hashingTF.numFeatures, [p1])\n",
    "                 .addGrid(lr.regParam, [p2])\n",
    "                 .addGrid(lr.elasticNetParam, [p3])\n",
    "                 .addGrid(lr.maxIter, [p4])\n",
    "                 .build())\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=4)\n",
    "    \n",
    "    ########  Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(train)\n",
    "    # average cross-validation accuracy metric/s on all folds\n",
    "    average_score = cvModel.avgMetrics\n",
    "    print 'average cross-validation accuracy = {}'.format(average_score[0])\n",
    "    return average_score[0]\n",
    "\n",
    "########################### regression ##################################\n",
    "def cross_valid(estimator, paramGrid, train, validation):\n",
    "    evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "    #\"rmse\" (default): root mean squared error - \"mse\": mean squared error - \"r2\": R^2^ metric - \"mae\": mean absolute error \n",
    "    crossval = CrossValidator(estimator=estimator,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=4)\n",
    "    ########  Run cross-validation, and choose the best set of parameters.\n",
    "    cvModel = crossval.fit(train)\n",
    "    average_score = cvModel.avgMetrics\n",
    "    print 'average cross-validation MAE = {}'.format(average_score[0])\n",
    "    ########  Make predictions on on the validation data\n",
    "    prediction = cvModel.transform(validation)\n",
    "    # since lable is log(loss+200), we need to transform back\n",
    "    prediction=prediction.withColumn(\"loss\", exp(prediction.label) - 200).\n",
    "    withColumn(\"loss_prediction\", exp(prediction.prediction) - 200)\n",
    "    ########  Model evaluation\n",
    "    scores=RegressionMetrics(prediction.select(\"label\", \"prediction\").rdd)\n",
    "    print 'Mean absolute error in the validation data = {}'.format(scores.meanAbsoluteError)\n",
    "    scores=RegressionMetrics(prediction.select(\"loss\", \"loss_prediction\").rdd)\n",
    "    print 'Mean absolute error in the validation data after inverse log transformation = {}'.format(scores.meanAbsoluteError)\n",
    "    return average_score[0], prediction\n",
    "def grid_search(p1,p2,p3,p4):\n",
    "    lr = LinearRegression()\n",
    "    # Create ParamGrid for Cross Validation \n",
    "    paramGrid = (ParamGridBuilder()\n",
    "                 .addGrid(lr.regParam, [p1])#(default: 0.0)\n",
    "                 .addGrid(lr.elasticNetParam, [p2]) \n",
    "                 .addGrid(lr.maxIter, [p3]) #(default: 100)\n",
    "                 .addGrid(lr.fitIntercept, [p4])#False, True\n",
    "                 .build())\n",
    "    average_score, _ = cross_valid(lr, paramGrid, train, validation)\n",
    "    return average_score\n",
    "##################################loop over all parameter combinations ####################\n",
    "score=0.0\n",
    "for p1 in [45000,50000,55000]:\n",
    "    for p2 in [0.09,0.10,0.11]:\n",
    "        for p3 in [0.09,0.10,0.11]:\n",
    "            for p4 in [9,10,11]:\n",
    "                t0 = time()\n",
    "                print '(numFeatures,regParam,elasticNetParam,maxIter)=({},{},{},{})'.format(p1,p2,p3,p4)\n",
    "                average_score=grid_search(p1,p2,p3,p4)\n",
    "                tt = time() - t0\n",
    "                print \"Classifier trained in {} seconds\".format(round(tt,3))\n",
    "                if average_score > score:\n",
    "                    print '################ Best score ######################'\n",
    "                    params=(p1,p2,p3,p4)\n",
    "                    score=average_score\n",
    "print 'Best score is {} at params ={}'.format(score, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for classification #######################\n",
    "# trained by a logistic regression \n",
    "lr = LogisticRegression()\n",
    "print lr.explainParams()\n",
    "# Build a pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer,tokenizer, remover, hashingTF, idfModel, lr])\n",
    "\n",
    "# Create ParamGrid for Cross Validation \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(hashingTF.numFeatures, [50000])\n",
    "             .addGrid(lr.regParam, [0.10])\n",
    "             .addGrid(lr.elasticNetParam, [0.10])\n",
    "             .addGrid(lr.maxIter, [10])\n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,#BinaryClassificationEvaluator(), the default metric is areaUnderROC\n",
    "                          numFolds=4)\n",
    "    \n",
    "########  Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "average_score = cvModel.avgMetrics\n",
    "print 'best average cross-validation accuracy = {}'.format(average_score[0])\n",
    "\n",
    "########  Make predictions on on the test data\n",
    "prediction = cvModel.transform(test)\n",
    "#The prediction DataFrame will be created with the new output columns of \n",
    "#\"rawPrediction\", \"probability\", and \"prediction\". \n",
    "#The “RawPrediction” means raw prediction probability for each possible label. \n",
    "#This is a measure of confidence in each possible label. A larger value indicates more confidence for that label. \n",
    "#The “Probability” estimate the probability of each class, based on a given raw prediction. \n",
    "#The \"prediction\" gives a prediction label for each data instance\n",
    "prediction.printSchema()\n",
    "#root\n",
    "# |-- label: double (nullable = true)\n",
    "# |-- review: string (nullable = true)\n",
    "# |-- indexedLabel: double (nullable = true)\n",
    "# |-- words: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- filtered: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- rawFeatures: vector (nullable = true)\n",
    "# |-- features: vector (nullable = true)\n",
    "# |-- rawPrediction: vector (nullable = true)\n",
    "# |-- probability: vector (nullable = true)\n",
    "# |-- prediction: double (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for classification #######################\n",
    "# trained by a logistic regression \n",
    "lr = LogisticRegression()\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.10]) \n",
    "             .addGrid(lr.elasticNetParam, [0.10])\n",
    "             .addGrid(lr.maxIter, [10])\n",
    "             .build())\n",
    "#regParam (regularization parameter), \n",
    "#elasticNetParam (the ElasticNet mixing parameter), \n",
    "#maxIter (max number of iterations).\n",
    "\n",
    "##############################################\n",
    "# trained by a Naïve Bayes \n",
    "nb = NaiveBayes()\n",
    "# Create ParamGrid for Cross Validation \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [1.0])\n",
    "             .build())\n",
    "#smoothing (smoothing parameter) 0 ~ 1 \n",
    "\n",
    "##############################################\n",
    "# trained by a Decision Tree \n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\",impurity=\"entropy\")\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(dt.maxDepth, [25])\n",
    "             .addGrid(dt.minInstancesPerNode, [4])\n",
    "             .build())\n",
    "#maxDepth (maximum depth of the tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Decision Tree classifier). \n",
    "\n",
    "##############################################\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\",impurity=\"entropy\", seed=5043)\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [31])\n",
    "             .addGrid(rf.maxDepth, [29])\n",
    "             .addGrid(rf.minInstancesPerNode, [1])\n",
    "             .build())\n",
    "#numTrees (number of trees to train), \n",
    "#maxDepth (maximum depth of the tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Random Forest classifier). \n",
    "\n",
    "##############################################\n",
    "# trained by a Gradient Boosted Tree \n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\")\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxIter, [25]) #(default: 20)\n",
    "             .addGrid(gbt.maxDepth, [19])\n",
    "             .addGrid(gbt.minInstancesPerNode, [2])\n",
    "             .build())\n",
    "#maxIter (max number of iterations), \n",
    "#maxDepth (maximum depth of the tree for a Gradient Boosted Tree classifier), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split for a Gradient Boosted Tree classifier). \n",
    "\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for regression #######################\n",
    "lr = LinearRegression()\n",
    "#lr.explainParams()\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0001])#(default: 0.0)\n",
    "             .addGrid(lr.elasticNetParam, [1.0]) \n",
    "             .addGrid(lr.maxIter, [1000]) #(default: 100)\n",
    "             .build())\n",
    "average_score, prediction = cross_valid(lr, paramGrid, train, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for regression #######################\n",
    "lr = LinearRegression()\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0001])#(default: 0.0)\n",
    "             .addGrid(lr.elasticNetParam, [1.0]) \n",
    "             .addGrid(lr.maxIter, [1000]) #(default: 100)\n",
    "             .build())\n",
    "#regParam (regularization parameter), \n",
    "#elasticNetParam (ElasticNet mixing parameter in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. \n",
    "#For alpha = 1, it is an L1 penalty.),\n",
    "#maxIter (max number of iterationsfor \n",
    "##############################################\n",
    "rf = RandomForestRegressor(seed=5043)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [2])\n",
    "             .addGrid(rf.maxDepth, [2])\n",
    "             .addGrid(rf.minInstancesPerNode, [2]).build())\n",
    "#numTrees (number of trees to train), \n",
    "#maxDepth (maximum depth of a tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split). \n",
    "##############################################\n",
    "gbt = GBTRegressor()\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxIter, [2]) \n",
    "             .addGrid(gbt.maxDepth, [2])\n",
    "             .addGrid(gbt.minInstancesPerNode, [1])\n",
    "             .build())\n",
    "#maxIter (max number of iterations), \n",
    "#maxDepth (maximum depth of a tree), \n",
    "#minInstancesPerNode (minimum number of instances each child must have after split). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for clustering #######################\n",
    "kmeans = KMeans(k=2,seed=1)\n",
    "model = kmeans.fit(train) # train DataFrame has \"features\" column\n",
    "centers = model.clusterCenters()\n",
    "len(centers) #2 # cluster center number\n",
    "prediction = cvModel.transform(test)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(train_new)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for classification #######################\n",
    "######## Calculate accuracy of the prediction of the test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_score=evaluator.evaluate(prediction)\n",
    "# another way to calculate accuracy \n",
    "correct=prediction.filter(prediction['label']== prediction['prediction']).select(\"label\",\"prediction\")\n",
    "accuracy_score = correct.count() / float(test.count())  \n",
    "print 'Accuracy in the test data = {}'.format(accuracy_score)\n",
    "   \n",
    "######## calculate F1 score of the prediction of the test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score=evaluator.evaluate(prediction)\n",
    "print 'F1 score in the test data = {}'.format(f1_score) \n",
    "\n",
    "######## Calculate area under ROC for the prediction of the test data\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "ROC_score=evaluator.evaluate(prediction)\n",
    "print 'areaUnderROC in the test data = {}'.format(ROC_score)\n",
    "#print \"areaUnderROC in the train data = %g\" % evaluator.evaluate(model.transform(train))  \n",
    "#print \"areaUnderROC in the test data = %g\" % evaluator.evaluate(model.transform(test))\n",
    "\n",
    "######## Print classification_report\n",
    "prediction_and_labels=prediction.select(\"label\",\"prediction\")\n",
    "prediction_and_labels.collect()\n",
    "#[Row(label=0.0, prediction=0.0),\n",
    "# Row(label=0.0, prediction=0.0),....]\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for x in prediction_and_labels.collect():\n",
    "    #ex:x is Row(label=0.0, prediction=0.0), xx=[0.0, 0.0]\n",
    "    xx = list(x)\n",
    "    try:\n",
    "        tt = int(xx[1])\n",
    "        pp = int(xx[0])\n",
    "        y_true.append(tt)\n",
    "        y_pred.append(pp)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "target_names = ['neg 0', 'pos 1']\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(y_true, y_pred, target_names=target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### for regression #######################\n",
    "scores=RegressionMetrics(prediction.select(\"label\", \"prediction\").rdd)\n",
    "print 'Mean absolute error in the validation data = {}'.format(scores.meanAbsoluteError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvModel.write.overwrite.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "classifier_names=['Logistic_Regression', 'Naive_Bayes', 'Decision_Tree', 'Random_Forest', 'Gradient_Boosted_Tree']\n",
    "time=[6.41,2.893,141,133,3179]\n",
    "accuracy=[0.8385,0.8177,0.78125,0.8125,0.8229]\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(5,5), facecolor='white')\n",
    "ax.barh(np.arange(0, 5),time)\n",
    "ax.set_yticks(np.arange(0.5, 5.5))\n",
    "ax.set_yticklabels(classifier_names)\n",
    "ax.grid(color='b', linestyle='--', linewidth=1)\n",
    "ax.set_title('Model training time')\n",
    "ax.set_xlabel('Time (sec)')\n",
    "#ax.set_xscale('log')\n",
    "display(fig)\n",
    "##########################################################################\n",
    "# Plot loss distribution, log(loss) distribution\n",
    "df_loss=df.select('loss').toPandas()\n",
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,5))\n",
    "F=sns.distplot(df_loss.loss.values, kde=False, bins=1000, ax=ax[0])\n",
    "F.set(title='Loss distribution. Skewness={:.2f}'.format(df_loss.loss.skew()))\n",
    "F.set(xlim=(0,20000))\n",
    "F1=sns.distplot(np.log(df_loss.loss.values), kde=False, bins=100,ax=ax[1])\n",
    "F1.set(title='log(loss) distribution. Skewness={:.2f}'.format(np.log(df_loss.loss).skew()))\n",
    "F1.set(xlim=(4,12))\n",
    "display(fig)\n",
    "##########################################################################\n",
    "## plot a correlation heatmap of the target valuable and the continuous input variables\n",
    "float_columns = [feat for feat in df.columns if 'cont' in feat]\n",
    "data=df.select(float_columns+['label']).toPandas()\n",
    "df_corr = data.corr(method='pearson', min_periods=100)\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "k=14 # the number of the continuous input variables\n",
    "df_corr = np.abs(df_corr)\n",
    "cols = df_corr.nlargest(k, 'label')['label'].index\n",
    "cm=data[cols].corr(method='pearson', min_periods=100)\n",
    "cm = np.abs(cm)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, \\\n",
    "                 yticklabels=cols.values, xticklabels=cols.values)\n",
    "display(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
